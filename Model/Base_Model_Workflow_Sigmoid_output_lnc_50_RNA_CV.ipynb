{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils \n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('./Platin_Data/Semi_Final/common_patient.csv', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLATINUM_STATUS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1331</th>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1332</th>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <td>Resistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1364</th>\n",
       "      <td>Resistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2098</th>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <td>Resistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2113</th>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLATINUM_STATUS\n",
       "PATIENT_ID                  \n",
       "TCGA-04-1331       Sensitive\n",
       "TCGA-04-1332       Sensitive\n",
       "TCGA-04-1347       Sensitive\n",
       "TCGA-04-1362       Resistant\n",
       "TCGA-04-1364       Resistant\n",
       "...                      ...\n",
       "TCGA-61-2098       Sensitive\n",
       "TCGA-61-2109       Sensitive\n",
       "TCGA-61-2110       Resistant\n",
       "TCGA-61-2111       Sensitive\n",
       "TCGA-61-2113       Sensitive\n",
       "\n",
       "[210 rows x 1 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.loc[df_label['PLATINUM_STATUS'] == 'Sensitive', 'label'] = 0\n",
    "df_label.loc[df_label['PLATINUM_STATUS'] == 'Resistant', 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLATINUM_STATUS</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1331</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1332</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1364</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2098</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2113</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLATINUM_STATUS  label\n",
       "PATIENT_ID                         \n",
       "TCGA-04-1331       Sensitive    0.0\n",
       "TCGA-04-1332       Sensitive    0.0\n",
       "TCGA-04-1347       Sensitive    0.0\n",
       "TCGA-04-1362       Resistant    1.0\n",
       "TCGA-04-1364       Resistant    1.0\n",
       "...                      ...    ...\n",
       "TCGA-61-2098       Sensitive    0.0\n",
       "TCGA-61-2109       Sensitive    0.0\n",
       "TCGA-61-2110       Resistant    1.0\n",
       "TCGA-61-2111       Sensitive    0.0\n",
       "TCGA-61-2113       Sensitive    0.0\n",
       "\n",
       "[210 rows x 2 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('./Platin_Data/Semi_Final/PC_tpm_200_minmax.csv', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCGA-04-1331</th>\n",
       "      <th>TCGA-04-1332</th>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <th>TCGA-04-1364</th>\n",
       "      <th>TCGA-04-1365</th>\n",
       "      <th>TCGA-04-1514</th>\n",
       "      <th>TCGA-04-1530</th>\n",
       "      <th>TCGA-04-1536</th>\n",
       "      <th>TCGA-04-1542</th>\n",
       "      <th>...</th>\n",
       "      <th>TCGA-61-2000</th>\n",
       "      <th>TCGA-61-2008</th>\n",
       "      <th>TCGA-61-2009</th>\n",
       "      <th>TCGA-61-2092</th>\n",
       "      <th>TCGA-61-2097</th>\n",
       "      <th>TCGA-61-2098</th>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <th>TCGA-61-2113</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000131096</th>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.200155</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.023144</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000187581</th>\n",
       "      <td>0.074279</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015051</td>\n",
       "      <td>0.017779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.018865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015682</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.222856</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.154442</td>\n",
       "      <td>0.112554</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.096016</td>\n",
       "      <td>0.010557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000047936</th>\n",
       "      <td>0.149561</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000186198</th>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.020709</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005363</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.027494</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.002652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000179914</th>\n",
       "      <td>0.008411</td>\n",
       "      <td>0.056473</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032674</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.258720</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.000410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000153822</th>\n",
       "      <td>0.202502</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.014825</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.001336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000197616</th>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.022576</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.026711</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.007845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000112499</th>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000111249</th>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000203909</th>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TCGA-04-1331  TCGA-04-1332  TCGA-04-1347  TCGA-04-1362  \\\n",
       "ID                                                                        \n",
       "ENSG00000131096      0.001124      0.000566      0.000096      0.002925   \n",
       "ENSG00000187581      0.074279      0.001385      0.002826      0.016522   \n",
       "ENSG00000047936      0.149561      0.002034      0.000060      0.002256   \n",
       "ENSG00000186198      0.020867      0.004467      0.004142      0.005812   \n",
       "ENSG00000179914      0.008411      0.056473      0.001344      0.001347   \n",
       "...                       ...           ...           ...           ...   \n",
       "ENSG00000153822      0.202502      0.002711      0.004158      0.001322   \n",
       "ENSG00000197616      0.009398      0.000928      0.000315      0.022576   \n",
       "ENSG00000112499      0.000136      0.000092      0.000016      0.000036   \n",
       "ENSG00000111249      0.000482      0.000072      0.000010      0.000539   \n",
       "ENSG00000203909      0.005991      0.000000      0.224272      0.000000   \n",
       "\n",
       "                 TCGA-04-1364  TCGA-04-1365  TCGA-04-1514  TCGA-04-1530  \\\n",
       "ID                                                                        \n",
       "ENSG00000131096      0.001883      0.000146      0.000412      0.200155   \n",
       "ENSG00000187581      1.000000      0.015051      0.017779      0.000000   \n",
       "ENSG00000047936      0.000016      0.007940      0.000027      0.000705   \n",
       "ENSG00000186198      0.020709      0.005042      0.001216      0.001000   \n",
       "ENSG00000179914      0.000372      0.001071      0.001127      0.000742   \n",
       "...                       ...           ...           ...           ...   \n",
       "ENSG00000153822      0.000091      0.014324      0.000079      0.004260   \n",
       "ENSG00000197616      0.002524      0.000320      0.026711      0.000975   \n",
       "ENSG00000112499      0.000020      0.000008      0.000070      0.000000   \n",
       "ENSG00000111249      0.000000      0.000404      0.001925      0.000013   \n",
       "ENSG00000203909      0.000000      0.000000      0.000000      0.001321   \n",
       "\n",
       "                 TCGA-04-1536  TCGA-04-1542  ...  TCGA-61-2000  TCGA-61-2008  \\\n",
       "ID                                           ...                               \n",
       "ENSG00000131096      0.000272      0.000401  ...      0.000119      0.000052   \n",
       "ENSG00000187581      0.001618      0.018865  ...      0.015682      0.001707   \n",
       "ENSG00000047936      0.000014      0.000171  ...      0.000045      0.000044   \n",
       "ENSG00000186198      0.000285      0.001728  ...      0.005363      0.001585   \n",
       "ENSG00000179914      0.004618      0.000000  ...      0.032674      0.001082   \n",
       "...                       ...           ...  ...           ...           ...   \n",
       "ENSG00000153822      0.003334      0.000000  ...      0.000672      0.000008   \n",
       "ENSG00000197616      0.000014      0.007845  ...      0.000700      0.000025   \n",
       "ENSG00000112499      0.000004      0.000000  ...      0.000058      0.000006   \n",
       "ENSG00000111249      0.000013      0.000009  ...      0.000136      0.000061   \n",
       "ENSG00000203909      0.000000      0.000000  ...      0.005059      0.000000   \n",
       "\n",
       "                 TCGA-61-2009  TCGA-61-2092  TCGA-61-2097  TCGA-61-2098  \\\n",
       "ID                                                                        \n",
       "ENSG00000131096      0.000138      0.000038      0.000651      0.000309   \n",
       "ENSG00000187581      0.222856      0.000508      0.005218      0.154442   \n",
       "ENSG00000047936      0.000303      0.000000      0.000134      0.000215   \n",
       "ENSG00000186198      0.002821      0.000372      0.003824      0.001345   \n",
       "ENSG00000179914      0.001927      0.000023      0.006381      0.001390   \n",
       "...                       ...           ...           ...           ...   \n",
       "ENSG00000153822      0.000420      0.006529      0.000149      0.000120   \n",
       "ENSG00000197616      0.000859      0.000000      0.012233      0.010409   \n",
       "ENSG00000112499      0.000011      0.000011      0.000029      0.000003   \n",
       "ENSG00000111249      0.000647      0.000003      0.000097      0.001646   \n",
       "ENSG00000203909      0.000000      0.000000      0.000000      0.001015   \n",
       "\n",
       "                 TCGA-61-2109  TCGA-61-2110  TCGA-61-2111  TCGA-61-2113  \n",
       "ID                                                                       \n",
       "ENSG00000131096      0.001277      0.000163      0.023144      0.000137  \n",
       "ENSG00000187581      0.112554      0.006289      0.096016      0.010557  \n",
       "ENSG00000047936      0.005764      0.000069      0.000012      0.000103  \n",
       "ENSG00000186198      0.027494      0.000263      0.001709      0.002652  \n",
       "ENSG00000179914      0.258720      0.030643      0.001957      0.000410  \n",
       "...                       ...           ...           ...           ...  \n",
       "ENSG00000153822      0.013265      0.014825      0.000069      0.001336  \n",
       "ENSG00000197616      0.000838      0.000120      0.002083      0.000067  \n",
       "ENSG00000112499      0.000723      0.000010      0.000008      0.000017  \n",
       "ENSG00000111249      0.000452      0.000007      0.000094      0.000123  \n",
       "ENSG00000203909      0.000000      0.002609      0.000000      0.001460  \n",
       "\n",
       "[200 rows x 210 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_mod = df_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ID</th>\n",
       "      <th>ENSG00000131096</th>\n",
       "      <th>ENSG00000187581</th>\n",
       "      <th>ENSG00000047936</th>\n",
       "      <th>ENSG00000186198</th>\n",
       "      <th>ENSG00000179914</th>\n",
       "      <th>ENSG00000186897</th>\n",
       "      <th>ENSG00000138136</th>\n",
       "      <th>ENSG00000139219</th>\n",
       "      <th>ENSG00000136944</th>\n",
       "      <th>ENSG00000182870</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000048545</th>\n",
       "      <th>ENSG00000188076</th>\n",
       "      <th>ENSG00000048462</th>\n",
       "      <th>ENSG00000187094</th>\n",
       "      <th>ENSG00000106038</th>\n",
       "      <th>ENSG00000153822</th>\n",
       "      <th>ENSG00000197616</th>\n",
       "      <th>ENSG00000112499</th>\n",
       "      <th>ENSG00000111249</th>\n",
       "      <th>ENSG00000203909</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1331</th>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.074279</td>\n",
       "      <td>0.149561</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>0.063072</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012713</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.202502</td>\n",
       "      <td>0.009398</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.005991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1332</th>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.056473</td>\n",
       "      <td>0.046285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.029110</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.793124</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132969</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.224272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.054898</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019489</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.022576</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1364</th>\n",
       "      <td>0.001883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.020709</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2098</th>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.154442</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.233778</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.001015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.112554</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.027494</td>\n",
       "      <td>0.258720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.063795</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053940</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014825</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <td>0.023144</td>\n",
       "      <td>0.096016</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.003381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2113</th>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.138558</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.001460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ID            ENSG00000131096  ENSG00000187581  ENSG00000047936  \\\n",
       "TCGA-04-1331         0.001124         0.074279         0.149561   \n",
       "TCGA-04-1332         0.000566         0.001385         0.002034   \n",
       "TCGA-04-1347         0.000096         0.002826         0.000060   \n",
       "TCGA-04-1362         0.002925         0.016522         0.002256   \n",
       "TCGA-04-1364         0.001883         1.000000         0.000016   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-2098         0.000309         0.154442         0.000215   \n",
       "TCGA-61-2109         0.001277         0.112554         0.005764   \n",
       "TCGA-61-2110         0.000163         0.006289         0.000069   \n",
       "TCGA-61-2111         0.023144         0.096016         0.000012   \n",
       "TCGA-61-2113         0.000137         0.010557         0.000103   \n",
       "\n",
       "ID            ENSG00000186198  ENSG00000179914  ENSG00000186897  \\\n",
       "TCGA-04-1331         0.020867         0.008411         0.372549   \n",
       "TCGA-04-1332         0.004467         0.056473         0.046285   \n",
       "TCGA-04-1347         0.004142         0.001344         0.793124   \n",
       "TCGA-04-1362         0.005812         0.001347         0.015423   \n",
       "TCGA-04-1364         0.020709         0.000372         0.245882   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-2098         0.001345         0.001390         0.016812   \n",
       "TCGA-61-2109         0.027494         0.258720         1.000000   \n",
       "TCGA-61-2110         0.000263         0.030643         0.001324   \n",
       "TCGA-61-2111         0.001709         0.001957         0.002696   \n",
       "TCGA-61-2113         0.002652         0.000410         0.138558   \n",
       "\n",
       "ID            ENSG00000138136  ENSG00000139219  ENSG00000136944  \\\n",
       "TCGA-04-1331         0.000000         0.074310         0.063072   \n",
       "TCGA-04-1332         0.000000         0.005921         0.029110   \n",
       "TCGA-04-1347         0.000461         0.001680         0.000038   \n",
       "TCGA-04-1362         0.003234         0.054898         0.010333   \n",
       "TCGA-04-1364         0.000000         0.010063         0.000123   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-2098         0.000855         0.233778         0.002315   \n",
       "TCGA-61-2109         0.003060         0.063795         0.004804   \n",
       "TCGA-61-2110         0.000000         0.015528         0.001865   \n",
       "TCGA-61-2111         0.021030         0.000892         0.000157   \n",
       "TCGA-61-2113         0.004919         0.003812         0.000630   \n",
       "\n",
       "ID            ENSG00000182870  ...  ENSG00000048545  ENSG00000188076  \\\n",
       "TCGA-04-1331         0.002291  ...         0.001769         1.000000   \n",
       "TCGA-04-1332         0.000781  ...         0.000264         0.002173   \n",
       "TCGA-04-1347         0.000037  ...         0.000000         0.132969   \n",
       "TCGA-04-1362         0.001136  ...         0.000787         0.000000   \n",
       "TCGA-04-1364         0.000088  ...         0.011812         0.000000   \n",
       "...                       ...  ...              ...              ...   \n",
       "TCGA-61-2098         0.000060  ...         0.000050         0.001234   \n",
       "TCGA-61-2109         0.005207  ...         0.000447         0.000000   \n",
       "TCGA-61-2110         0.008710  ...         0.000000         0.000705   \n",
       "TCGA-61-2111         0.000115  ...         0.000033         0.000538   \n",
       "TCGA-61-2113         0.000292  ...         0.002155         0.000000   \n",
       "\n",
       "ID            ENSG00000048462  ENSG00000187094  ENSG00000106038  \\\n",
       "TCGA-04-1331         0.012713         0.018254         0.000933   \n",
       "TCGA-04-1332         0.000000         0.000000         0.000209   \n",
       "TCGA-04-1347         0.002902         0.000000         0.000000   \n",
       "TCGA-04-1362         0.000000         0.019489         0.000996   \n",
       "TCGA-04-1364         0.001123         0.001152         0.000275   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-2098         0.000108         0.000442         0.000040   \n",
       "TCGA-61-2109         0.053940         0.001317         0.000000   \n",
       "TCGA-61-2110         0.001476         0.002523         0.000000   \n",
       "TCGA-61-2111         0.003381         0.000000         0.000000   \n",
       "TCGA-61-2113         0.034072         0.001694         0.000000   \n",
       "\n",
       "ID            ENSG00000153822  ENSG00000197616  ENSG00000112499  \\\n",
       "TCGA-04-1331         0.202502         0.009398         0.000136   \n",
       "TCGA-04-1332         0.002711         0.000928         0.000092   \n",
       "TCGA-04-1347         0.004158         0.000315         0.000016   \n",
       "TCGA-04-1362         0.001322         0.022576         0.000036   \n",
       "TCGA-04-1364         0.000091         0.002524         0.000020   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-2098         0.000120         0.010409         0.000003   \n",
       "TCGA-61-2109         0.013265         0.000838         0.000723   \n",
       "TCGA-61-2110         0.014825         0.000120         0.000010   \n",
       "TCGA-61-2111         0.000069         0.002083         0.000008   \n",
       "TCGA-61-2113         0.001336         0.000067         0.000017   \n",
       "\n",
       "ID            ENSG00000111249  ENSG00000203909  \n",
       "TCGA-04-1331         0.000482         0.005991  \n",
       "TCGA-04-1332         0.000072         0.000000  \n",
       "TCGA-04-1347         0.000010         0.224272  \n",
       "TCGA-04-1362         0.000539         0.000000  \n",
       "TCGA-04-1364         0.000000         0.000000  \n",
       "...                       ...              ...  \n",
       "TCGA-61-2098         0.001646         0.001015  \n",
       "TCGA-61-2109         0.000452         0.000000  \n",
       "TCGA-61-2110         0.000007         0.002609  \n",
       "TCGA-61-2111         0.000094         0.000000  \n",
       "TCGA-61-2113         0.000123         0.001460  \n",
       "\n",
       "[210 rows x 200 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_label_mi = pd.read_csv('./Platin_Data/miRNA_patient.csv', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_mi = pd.read_csv('./Platin_Data/Semi_Final/MIR_rpm_100_minmax.csv', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_mi = df_data_mi.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_lnc = pd.read_csv('./Platin_Data/Semi_Final/LNC_tpm_50_minmax.csv', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TCGA-04-1331</th>\n",
       "      <th>TCGA-04-1332</th>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <th>TCGA-04-1364</th>\n",
       "      <th>TCGA-04-1365</th>\n",
       "      <th>TCGA-04-1514</th>\n",
       "      <th>TCGA-04-1530</th>\n",
       "      <th>TCGA-04-1536</th>\n",
       "      <th>TCGA-04-1542</th>\n",
       "      <th>...</th>\n",
       "      <th>TCGA-61-2000</th>\n",
       "      <th>TCGA-61-2008</th>\n",
       "      <th>TCGA-61-2009</th>\n",
       "      <th>TCGA-61-2092</th>\n",
       "      <th>TCGA-61-2097</th>\n",
       "      <th>TCGA-61-2098</th>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <th>TCGA-61-2113</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000233048</th>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000083622</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.007278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000269994</th>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.029038</td>\n",
       "      <td>0.004058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000258752</th>\n",
       "      <td>0.222690</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.124308</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0.529993</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053335</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.040338</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.008254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000249790</th>\n",
       "      <td>0.003010</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.021516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017448</td>\n",
       "      <td>0.011822</td>\n",
       "      <td>0.028378</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000261399</th>\n",
       "      <td>0.566213</td>\n",
       "      <td>0.065518</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040054</td>\n",
       "      <td>0.156729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048270</td>\n",
       "      <td>0.327059</td>\n",
       "      <td>0.104678</td>\n",
       "      <td>0.280784</td>\n",
       "      <td>0.053024</td>\n",
       "      <td>0.033341</td>\n",
       "      <td>0.045478</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>0.159373</td>\n",
       "      <td>0.455519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000232721</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.079292</td>\n",
       "      <td>0.076157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047717</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.246036</td>\n",
       "      <td>0.279755</td>\n",
       "      <td>0.190579</td>\n",
       "      <td>0.090973</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.014341</td>\n",
       "      <td>0.020039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000243479</th>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>0.087608</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.964925</td>\n",
       "      <td>0.063399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.049149</td>\n",
       "      <td>0.042924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000248554</th>\n",
       "      <td>0.088593</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>0.017416</td>\n",
       "      <td>0.021562</td>\n",
       "      <td>0.032485</td>\n",
       "      <td>0.239507</td>\n",
       "      <td>0.126662</td>\n",
       "      <td>0.030493</td>\n",
       "      <td>0.034347</td>\n",
       "      <td>0.040995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042798</td>\n",
       "      <td>0.020713</td>\n",
       "      <td>0.026518</td>\n",
       "      <td>0.011855</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>0.099242</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.105778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000250432</th>\n",
       "      <td>0.014433</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.024167</td>\n",
       "      <td>0.047731</td>\n",
       "      <td>0.020660</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.022846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035765</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.003374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000226956</th>\n",
       "      <td>0.798367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092193</td>\n",
       "      <td>0.726227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281424</td>\n",
       "      <td>0.160843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010863</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000228058</th>\n",
       "      <td>0.004703</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>0.020171</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>0.374379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175485</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000226203</th>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.038454</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008414</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.015327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000233705</th>\n",
       "      <td>0.011394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.062446</td>\n",
       "      <td>0.019005</td>\n",
       "      <td>0.137431</td>\n",
       "      <td>0.004342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007459</td>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.002886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000249096</th>\n",
       "      <td>0.013160</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.034482</td>\n",
       "      <td>0.026340</td>\n",
       "      <td>0.036584</td>\n",
       "      <td>0.019047</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>0.090455</td>\n",
       "      <td>0.022747</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.119985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000174171</th>\n",
       "      <td>0.030238</td>\n",
       "      <td>0.013218</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.016162</td>\n",
       "      <td>0.004312</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.062446</td>\n",
       "      <td>0.044833</td>\n",
       "      <td>0.072944</td>\n",
       "      <td>0.042251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029215</td>\n",
       "      <td>0.059385</td>\n",
       "      <td>0.232304</td>\n",
       "      <td>0.124085</td>\n",
       "      <td>0.053487</td>\n",
       "      <td>0.085763</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.280806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000240990</th>\n",
       "      <td>0.009469</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>0.067517</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.551924</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000255910</th>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.001164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000241657</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.852956</td>\n",
       "      <td>0.109325</td>\n",
       "      <td>0.252783</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>0.974807</td>\n",
       "      <td>0.657092</td>\n",
       "      <td>0.082239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131751</td>\n",
       "      <td>0.357078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113754</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.138410</td>\n",
       "      <td>0.621661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000265179</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036236</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.035342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000228723</th>\n",
       "      <td>0.324026</td>\n",
       "      <td>0.055013</td>\n",
       "      <td>0.052117</td>\n",
       "      <td>0.173958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107393</td>\n",
       "      <td>0.463265</td>\n",
       "      <td>0.193027</td>\n",
       "      <td>0.041875</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016771</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.236407</td>\n",
       "      <td>0.199762</td>\n",
       "      <td>0.030705</td>\n",
       "      <td>0.295401</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.052757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000259070</th>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.093597</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.030701</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.038693</td>\n",
       "      <td>0.002625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000235269</th>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.011288</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000258837</th>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.052465</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026233</td>\n",
       "      <td>0.312294</td>\n",
       "      <td>0.029147</td>\n",
       "      <td>0.071135</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073657</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.041545</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.017103</td>\n",
       "      <td>0.019916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000278698</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015041</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000237928</th>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.076906</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000272797</th>\n",
       "      <td>0.153256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043707</td>\n",
       "      <td>0.055774</td>\n",
       "      <td>0.146075</td>\n",
       "      <td>0.113614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025569</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000260676</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.014333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.174273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000229155</th>\n",
       "      <td>0.015493</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>0.013255</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062036</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.458288</td>\n",
       "      <td>0.235435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000198788</th>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.305758</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.281829</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.103492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000230133</th>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013592</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000272988</th>\n",
       "      <td>0.213691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000261520</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000229228</th>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.021544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000259974</th>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.063566</td>\n",
       "      <td>0.403119</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.017877</td>\n",
       "      <td>0.805694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.086653</td>\n",
       "      <td>0.211451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000251185</th>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000254101</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000223502</th>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017899</td>\n",
       "      <td>0.024387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000230392</th>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000250584</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.031466</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.170735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000231621</th>\n",
       "      <td>0.300773</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.079680</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.302745</td>\n",
       "      <td>0.154531</td>\n",
       "      <td>0.303544</td>\n",
       "      <td>0.727942</td>\n",
       "      <td>0.192061</td>\n",
       "      <td>0.343855</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.243228</td>\n",
       "      <td>0.389236</td>\n",
       "      <td>0.092333</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.442771</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.023089</td>\n",
       "      <td>0.080657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000225472</th>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.007047</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.012881</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000231764</th>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.055033</td>\n",
       "      <td>0.223903</td>\n",
       "      <td>0.013692</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.201863</td>\n",
       "      <td>0.266049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000241388</th>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.051242</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>0.011505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.015335</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.014934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000233536</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011602</td>\n",
       "      <td>0.009024</td>\n",
       "      <td>0.014682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016289</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000234692</th>\n",
       "      <td>0.079446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015923</td>\n",
       "      <td>0.135944</td>\n",
       "      <td>0.065053</td>\n",
       "      <td>0.246101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132547</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000224717</th>\n",
       "      <td>0.207852</td>\n",
       "      <td>0.005395</td>\n",
       "      <td>0.061105</td>\n",
       "      <td>0.072904</td>\n",
       "      <td>0.006669</td>\n",
       "      <td>0.127648</td>\n",
       "      <td>0.527475</td>\n",
       "      <td>0.015409</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.136069</td>\n",
       "      <td>0.462718</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.029413</td>\n",
       "      <td>0.050283</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.015793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000262117</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.002876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.011469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000248646</th>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000258170</th>\n",
       "      <td>0.012416</td>\n",
       "      <td>0.017186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055726</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039985</td>\n",
       "      <td>0.008128</td>\n",
       "      <td>0.010839</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TCGA-04-1331  TCGA-04-1332  TCGA-04-1347  TCGA-04-1362  \\\n",
       "ID                                                                        \n",
       "ENSG00000233048      0.001818      0.000000      0.000000      0.000000   \n",
       "ENSG00000083622      0.000000      0.001047      0.000000      0.001600   \n",
       "ENSG00000269994      0.000668      0.000547      0.002004      0.002408   \n",
       "ENSG00000258752      0.222690      0.002003      0.003058      0.003266   \n",
       "ENSG00000249790      0.003010      0.002467      0.009037      0.001810   \n",
       "ENSG00000261399      0.566213      0.065518      1.000000      0.040054   \n",
       "ENSG00000232721      0.000000      0.014051      0.004949      0.079292   \n",
       "ENSG00000243479      0.004035      0.000661      0.000000      0.046100   \n",
       "ENSG00000248554      0.088593      0.006224      0.017416      0.021562   \n",
       "ENSG00000250432      0.014433      0.005733      0.024167      0.047731   \n",
       "ENSG00000226956      0.798367      1.000000      0.092193      0.726227   \n",
       "ENSG00000228058      0.004703      0.006167      0.000000      0.084833   \n",
       "ENSG00000226203      0.005043      0.001378      0.020188      0.002022   \n",
       "ENSG00000233705      0.011394      0.000000      0.004276      0.002284   \n",
       "ENSG00000249096      0.013160      0.000863      0.003951      0.006330   \n",
       "ENSG00000174171      0.030238      0.013218      0.005044      0.016162   \n",
       "ENSG00000240990      0.009469      0.003880      0.067517      0.011387   \n",
       "ENSG00000255910      0.000383      0.000141      0.002085      0.000518   \n",
       "ENSG00000241657      1.000000      0.059609      0.852956      0.109325   \n",
       "ENSG00000265179      0.000000      0.004766      0.000000      0.000000   \n",
       "ENSG00000228723      0.324026      0.055013      0.052117      0.173958   \n",
       "ENSG00000259070      0.012522      0.001044      0.000243      0.093597   \n",
       "ENSG00000235269      0.000137      0.000045      0.000000      0.000411   \n",
       "ENSG00000258837      0.021843      0.012890      0.052465      0.178622   \n",
       "ENSG00000278698      0.000000      0.011336      0.000000      0.000000   \n",
       "ENSG00000237928      0.001101      0.000155      0.000708      0.000284   \n",
       "ENSG00000272797      0.153256      0.000000      0.153378      0.000000   \n",
       "ENSG00000260676      0.000000      0.000000      0.000289      0.014333   \n",
       "ENSG00000229155      0.015493      0.002540      0.000000      0.009316   \n",
       "ENSG00000198788      0.003182      0.000000      0.000000      0.003827   \n",
       "ENSG00000230133      0.000418      0.000114      0.000105      0.000000   \n",
       "ENSG00000272988      0.213691      0.000000      0.000000      0.036711   \n",
       "ENSG00000261520      0.000000      0.000724      0.000553      0.000000   \n",
       "ENSG00000229228      0.002181      0.000358      0.000819      0.001967   \n",
       "ENSG00000259974      0.005126      0.000720      0.000000      0.000440   \n",
       "ENSG00000251185      0.002709      0.000081      0.000739      0.003405   \n",
       "ENSG00000254101      0.000000      0.000000      0.000000      0.000000   \n",
       "ENSG00000223502      0.003797      0.001245      0.008550      0.000000   \n",
       "ENSG00000230392      0.001029      0.000000      0.000000      0.000000   \n",
       "ENSG00000250584      0.000000      0.000000      0.000650      0.003642   \n",
       "ENSG00000231621      0.300773      0.011601      0.079680      1.000000   \n",
       "ENSG00000225472      0.008762      0.000988      0.006577      0.000000   \n",
       "ENSG00000231764      0.009620      0.000252      0.000578      0.027764   \n",
       "ENSG00000241388      0.002890      0.000663      0.002170      0.002781   \n",
       "ENSG00000233536      0.000000      0.001330      0.000000      0.019518   \n",
       "ENSG00000234692      0.079446      0.000000      0.000000      0.015923   \n",
       "ENSG00000224717      0.207852      0.005395      0.061105      0.072904   \n",
       "ENSG00000262117      0.000000      0.002474      0.003777      0.001513   \n",
       "ENSG00000248646      0.001548      0.000102      0.000000      0.000186   \n",
       "ENSG00000258170      0.012416      0.017186      0.000000      0.002488   \n",
       "\n",
       "                 TCGA-04-1364  TCGA-04-1365  TCGA-04-1514  TCGA-04-1530  \\\n",
       "ID                                                                        \n",
       "ENSG00000233048      0.008223      0.000284      0.000371      0.000000   \n",
       "ENSG00000083622      0.000000      0.000581      0.000000      0.003550   \n",
       "ENSG00000269994      0.000000      0.001458      0.000954      0.003711   \n",
       "ENSG00000258752      0.124308      0.005930      0.529993      0.013590   \n",
       "ENSG00000249790      0.005150      0.003286      0.021516      0.000000   \n",
       "ENSG00000261399      0.156729      1.000000      1.000000      1.000000   \n",
       "ENSG00000232721      0.076157      0.000000      0.377076      0.000000   \n",
       "ENSG00000243479      0.001726      0.002203      0.011538      0.008974   \n",
       "ENSG00000248554      0.032485      0.239507      0.126662      0.030493   \n",
       "ENSG00000250432      0.020660      0.017879      0.007937      0.019753   \n",
       "ENSG00000226956      0.000000      0.122924      0.000000      0.182110   \n",
       "ENSG00000228058      0.000000      0.002567      0.020171      0.005230   \n",
       "ENSG00000226203      0.008629      0.002753      0.038454      0.001869   \n",
       "ENSG00000233705      0.000812      0.001037      0.062446      0.019005   \n",
       "ENSG00000249096      0.001689      0.034482      0.026340      0.036584   \n",
       "ENSG00000174171      0.004312      0.031179      0.062446      0.044833   \n",
       "ENSG00000240990      0.000000      0.005169      0.006769      0.000000   \n",
       "ENSG00000255910      0.000041      0.000209      0.000000      0.000320   \n",
       "ENSG00000241657      0.252783      0.099253      0.974807      0.657092   \n",
       "ENSG00000265179      0.002487      0.000000      0.000000      0.000000   \n",
       "ENSG00000228723      1.000000      0.107393      0.463265      0.193027   \n",
       "ENSG00000259070      0.000647      0.030701      0.006636      0.000960   \n",
       "ENSG00000235269      0.000000      0.000075      0.000783      0.000000   \n",
       "ENSG00000258837      0.000000      0.026233      0.312294      0.029147   \n",
       "ENSG00000278698      0.000000      0.094374      0.000000      0.000000   \n",
       "ENSG00000237928      0.000202      0.000086      0.076906      0.001049   \n",
       "ENSG00000272797      0.043707      0.055774      0.146075      0.113614   \n",
       "ENSG00000260676      0.000000      0.000000      0.000000      0.000000   \n",
       "ENSG00000229155      0.013255      0.008458      0.000000      0.017228   \n",
       "ENSG00000198788      0.000545      0.000695      0.305758      0.001416   \n",
       "ENSG00000230133      0.013592      0.000076      0.000398      0.000000   \n",
       "ENSG00000272988      0.000000      0.000000      0.043645      0.000000   \n",
       "ENSG00000261520      0.000000      0.000402      0.001053      0.000000   \n",
       "ENSG00000229228      0.000467      0.001191      0.000000      0.007276   \n",
       "ENSG00000259974      0.000313      0.063566      0.403119      0.000814   \n",
       "ENSG00000251185      0.002001      0.003495      0.010561      0.001917   \n",
       "ENSG00000254101      0.003070      0.000010      0.000210      0.000020   \n",
       "ENSG00000223502      0.001624      0.000000      0.000000      0.025332   \n",
       "ENSG00000230392      0.000440      0.000843      0.000736      0.000000   \n",
       "ENSG00000250584      0.031466      0.000472      0.170735      0.000000   \n",
       "ENSG00000231621      0.302745      0.154531      0.303544      0.727942   \n",
       "ENSG00000225472      0.000469      0.004185      0.007047      0.003045   \n",
       "ENSG00000231764      0.000658      0.055033      0.223903      0.013692   \n",
       "ENSG00000241388      0.000495      0.000947      0.051242      0.006428   \n",
       "ENSG00000233536      0.003472      0.000000      0.011602      0.009024   \n",
       "ENSG00000234692      0.135944      0.065053      0.246101      0.000000   \n",
       "ENSG00000224717      0.006669      0.127648      0.527475      0.015409   \n",
       "ENSG00000262117      0.001076      0.002747      0.003597      0.002797   \n",
       "ENSG00000248646      0.000000      0.000338      0.000000      0.000000   \n",
       "ENSG00000258170      0.000000      0.055726      0.001972      0.000000   \n",
       "\n",
       "                 TCGA-04-1536  TCGA-04-1542  ...  TCGA-61-2000  TCGA-61-2008  \\\n",
       "ID                                           ...                               \n",
       "ENSG00000233048      0.000000      0.000297  ...      0.000000      0.000510   \n",
       "ENSG00000083622      0.000000      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000269994      0.002415      0.000000  ...      0.005804      0.001311   \n",
       "ENSG00000258752      0.004913      0.000000  ...      0.000000      0.053335   \n",
       "ENSG00000249790      0.000000      0.000000  ...      0.017448      0.011822   \n",
       "ENSG00000261399      0.000000      0.000000  ...      0.048270      0.327059   \n",
       "ENSG00000232721      0.047717      0.007538  ...      0.009556      0.246036   \n",
       "ENSG00000243479      0.087608      0.027679  ...      0.964925      0.063399   \n",
       "ENSG00000248554      0.034347      0.040995  ...      0.042798      0.020713   \n",
       "ENSG00000250432      0.007030      0.022846  ...      0.012067      0.000000   \n",
       "ENSG00000226956      1.000000      0.046806  ...      0.000000      0.281424   \n",
       "ENSG00000228058      0.374379      0.000000  ...      0.000000      0.175485   \n",
       "ENSG00000226203      0.000000      0.019218  ...      0.068214      0.000000   \n",
       "ENSG00000233705      0.137431      0.004342  ...      0.000000      0.007459   \n",
       "ENSG00000249096      0.019047      0.007522  ...      0.005721      0.090455   \n",
       "ENSG00000174171      0.072944      0.042251  ...      0.029215      0.059385   \n",
       "ENSG00000240990      0.000000      0.010825  ...      0.006861      0.000000   \n",
       "ENSG00000255910      0.001213      0.000547  ...      0.000139      0.000000   \n",
       "ENSG00000241657      0.082239      0.000000  ...      0.131751      0.357078   \n",
       "ENSG00000265179      0.000000      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000228723      0.041875      0.013230  ...      0.016771      1.000000   \n",
       "ENSG00000259070      0.002539      0.000494  ...      0.001252      0.006996   \n",
       "ENSG00000235269      0.000248      0.000156  ...      0.000397      0.011288   \n",
       "ENSG00000258837      0.071135      0.004994  ...      0.000000      0.008580   \n",
       "ENSG00000278698      0.000000      0.000000  ...      0.050109      0.000000   \n",
       "ENSG00000237928      0.000285      0.000000  ...      0.000000      0.000618   \n",
       "ENSG00000272797      0.000000      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000260676      0.000000      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000229155      0.000000      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000198788      0.003455      0.001455  ...      0.005535      0.000000   \n",
       "ENSG00000230133      0.000504      0.000000  ...      0.000808      0.000274   \n",
       "ENSG00000272988      0.000000      0.000000  ...      0.132726      0.000000   \n",
       "ENSG00000261520      0.001332      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000229228      0.000000      0.000000  ...      0.000000      0.012851   \n",
       "ENSG00000259974      0.001325      0.000000  ...      0.007430      0.001438   \n",
       "ENSG00000251185      0.000000      0.001126  ...      0.000714      0.000000   \n",
       "ENSG00000254101      0.000000      0.000000  ...      0.000000      0.006468   \n",
       "ENSG00000223502      0.000000      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000230392      0.003724      0.000000  ...      0.000746      0.000000   \n",
       "ENSG00000250584      0.000000      0.000000  ...      0.001254      0.000000   \n",
       "ENSG00000231621      0.192061      0.343855  ...      1.000000      0.243228   \n",
       "ENSG00000225472      0.012881      0.001878  ...      0.005556      0.000000   \n",
       "ENSG00000231764      0.001392      0.003519  ...      0.006134      0.005290   \n",
       "ENSG00000241388      0.011505      0.000000  ...      0.000000      0.001135   \n",
       "ENSG00000233536      0.014682      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000234692      0.000000      0.000000  ...      0.000000      0.026004   \n",
       "ENSG00000224717      0.006268      1.000000  ...      0.015062      0.136069   \n",
       "ENSG00000262117      0.009103      0.002876  ...      0.007292      0.000000   \n",
       "ENSG00000248646      0.000560      0.000000  ...      0.000000      0.000000   \n",
       "ENSG00000258170      0.002496      0.000000  ...      0.039985      0.008128   \n",
       "\n",
       "                 TCGA-61-2009  TCGA-61-2092  TCGA-61-2097  TCGA-61-2098  \\\n",
       "ID                                                                        \n",
       "ENSG00000233048      0.000000      0.000000      0.000000      0.000780   \n",
       "ENSG00000083622      0.000000      0.000000      0.000000      0.000000   \n",
       "ENSG00000269994      0.001049      0.000268      0.000000      0.001336   \n",
       "ENSG00000258752      0.006401      0.040338      0.001441      0.072042   \n",
       "ENSG00000249790      0.028378      0.006041      0.014375      0.016571   \n",
       "ENSG00000261399      0.104678      0.280784      0.053024      0.033341   \n",
       "ENSG00000232721      0.279755      0.190579      0.090973      0.009901   \n",
       "ENSG00000243479      0.000000      0.377435      1.000000      0.000000   \n",
       "ENSG00000248554      0.026518      0.011855      0.010074      0.099242   \n",
       "ENSG00000250432      0.035765      0.004680      0.000295      0.015559   \n",
       "ENSG00000226956      0.160843      0.000000      0.010863      0.010246   \n",
       "ENSG00000228058      0.022171      0.000000      0.004991      0.000000   \n",
       "ENSG00000226203      0.026416      0.000000      0.000000      0.008414   \n",
       "ENSG00000233705      0.002984      0.009910      0.003023      0.003802   \n",
       "ENSG00000249096      0.022747      0.009509      0.007682      0.001976   \n",
       "ENSG00000174171      0.232304      0.124085      0.053487      0.085763   \n",
       "ENSG00000240990      0.193428      0.000000      0.273840      0.000000   \n",
       "ENSG00000255910      0.001204      0.001000      0.000000      0.000288   \n",
       "ENSG00000241657      1.000000      0.036495      0.000000      0.113754   \n",
       "ENSG00000265179      0.009137      0.000000      0.006171      0.000000   \n",
       "ENSG00000228723      0.236407      0.199762      0.030705      0.295401   \n",
       "ENSG00000259070      0.002884      0.000260      0.000286      0.000594   \n",
       "ENSG00000235269      0.002366      0.000000      0.000000      0.000205   \n",
       "ENSG00000258837      0.000000      0.073657      0.006955      0.041545   \n",
       "ENSG00000278698      0.000000      0.000000      0.000000      0.000000   \n",
       "ENSG00000237928      0.001730      0.000000      0.000083      0.000000   \n",
       "ENSG00000272797      0.000000      0.102538      0.000000      0.025569   \n",
       "ENSG00000260676      0.002417      1.000000      0.006121      0.000000   \n",
       "ENSG00000229155      0.000000      0.373173      0.000000      0.062036   \n",
       "ENSG00000198788      0.001000      0.281829      0.001013      1.000000   \n",
       "ENSG00000230133      0.000657      0.000000      0.000000      0.000000   \n",
       "ENSG00000272988      0.047972      0.000000      0.000000      0.000000   \n",
       "ENSG00000261520      0.000000      0.000296      0.000000      0.000000   \n",
       "ENSG00000229228      0.005141      0.000000      0.000000      0.003275   \n",
       "ENSG00000259974      0.000000      0.005880      0.017877      0.805694   \n",
       "ENSG00000251185      0.001161      0.000099      0.000131      0.000123   \n",
       "ENSG00000254101      0.000000      0.000000      0.000000      0.003472   \n",
       "ENSG00000223502      0.017899      0.024387      0.000000      0.005701   \n",
       "ENSG00000230392      0.000000      0.000413      0.000000      0.000258   \n",
       "ENSG00000250584      0.104708      0.000000      0.000000      0.000866   \n",
       "ENSG00000231621      0.389236      0.092333      0.131443      0.442771   \n",
       "ENSG00000225472      0.001721      0.000220      0.000291      0.000274   \n",
       "ENSG00000231764      0.000000      0.008032      0.000204      0.002311   \n",
       "ENSG00000241388      0.004542      0.185185      0.022394      0.015335   \n",
       "ENSG00000233536      0.000000      0.016289      0.004306      0.004062   \n",
       "ENSG00000234692      0.000000      0.010631      0.000000      0.132547   \n",
       "ENSG00000224717      0.462718      0.002086      0.029413      0.050283   \n",
       "ENSG00000262117      0.000000      0.367610      0.000000      0.010073   \n",
       "ENSG00000248646      0.000000      0.000000      0.000000      0.000310   \n",
       "ENSG00000258170      0.010839      0.000554      0.000000      0.000000   \n",
       "\n",
       "                 TCGA-61-2109  TCGA-61-2110  TCGA-61-2111  TCGA-61-2113  \n",
       "ID                                                                       \n",
       "ENSG00000233048      0.000031      0.000000      0.000452      0.000000  \n",
       "ENSG00000083622      0.000038      0.000021      0.003009      0.007278  \n",
       "ENSG00000269994      0.000047      0.000036      0.029038      0.004058  \n",
       "ENSG00000258752      0.000032      0.000164      0.001181      0.008254  \n",
       "ENSG00000249790      0.000071      0.000000      0.000000      0.018295  \n",
       "ENSG00000261399      0.045478      0.004026      0.159373      0.455519  \n",
       "ENSG00000232721      0.000621      0.000443      0.014341      0.020039  \n",
       "ENSG00000243479      0.001045      0.000108      0.049149      0.042924  \n",
       "ENSG00000248554      0.002682      0.000312      0.036703      0.105778  \n",
       "ENSG00000250432      0.000575      0.000261      0.001449      0.003374  \n",
       "ENSG00000226956      1.000000      1.000000      0.000000      0.000000  \n",
       "ENSG00000228058      0.000664      0.000063      0.002046      0.000000  \n",
       "ENSG00000226203      0.000119      0.001603      0.000731      0.015327  \n",
       "ENSG00000233705      0.000224      0.000332      0.001652      0.002886  \n",
       "ENSG00000249096      0.000217      0.000141      0.005152      0.119985  \n",
       "ENSG00000174171      0.005220      0.000767      0.005846      0.280806  \n",
       "ENSG00000240990      0.000223      0.000382      0.551924      1.000000  \n",
       "ENSG00000255910      0.000018      0.000003      0.000208      0.001164  \n",
       "ENSG00000241657      0.005350      0.000611      0.138410      0.621661  \n",
       "ENSG00000265179      0.000000      0.036236      0.002529      0.035342  \n",
       "ENSG00000228723      0.000545      0.000933      0.010068      0.052757  \n",
       "ENSG00000259070      0.000028      0.000006      0.038693      0.002625  \n",
       "ENSG00000235269      0.000077      0.000397      0.000000      0.001664  \n",
       "ENSG00000258837      0.002263      0.000821      0.017103      0.019916  \n",
       "ENSG00000278698      0.006512      0.000000      0.015041      0.000000  \n",
       "ENSG00000237928      0.000004      0.000002      0.000000      0.000239  \n",
       "ENSG00000272797      0.001203      0.000000      1.000000      0.000000  \n",
       "ENSG00000260676      0.000000      0.000026      0.174273      0.000000  \n",
       "ENSG00000229155      0.002918      0.000624      0.458288      0.235435  \n",
       "ENSG00000198788      0.000150      0.000000      0.005261      0.103492  \n",
       "ENSG00000230133      0.000016      0.000017      0.000000      0.000424  \n",
       "ENSG00000272988      0.003593      0.000000      0.000000      0.000000  \n",
       "ENSG00000261520      0.000000      0.000000      0.000961      0.000000  \n",
       "ENSG00000229228      0.000051      0.000044      0.001423      0.021544  \n",
       "ENSG00000259974      0.000000      0.000039      0.086653      0.211451  \n",
       "ENSG00000251185      0.000017      0.000000      0.000107      0.000000  \n",
       "ENSG00000254101      0.000000      0.000000      0.000008      0.000112  \n",
       "ENSG00000223502      0.000179      0.000000      0.000000      0.005770  \n",
       "ENSG00000230392      0.000036      0.000000      0.000000      0.000782  \n",
       "ENSG00000250584      0.000122      0.000000      0.000000      0.000000  \n",
       "ENSG00000231621      0.013329      0.010931      0.023089      0.080657  \n",
       "ENSG00000225472      0.000052      0.000022      0.000000      0.000000  \n",
       "ENSG00000231764      0.000127      0.000010      0.201863      0.266049  \n",
       "ENSG00000241388      0.000014      0.000280      0.202428      0.014934  \n",
       "ENSG00000233536      0.021015      0.000545      0.000000      0.000000  \n",
       "ENSG00000234692      0.000623      0.000000      0.000000      0.020121  \n",
       "ENSG00000224717      0.000489      0.001024      0.006781      0.015793  \n",
       "ENSG00000262117      0.000000      0.000000      0.003283      0.011469  \n",
       "ENSG00000248646      0.000000      0.000000      0.000000      0.000000  \n",
       "ENSG00000258170      0.000000      0.000000      0.000000      0.004193  \n",
       "\n",
       "[50 rows x 210 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_lnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_lnc = df_data_lnc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mi = pd.concat([df_label_mi,df_data_mi], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_label, df_data_lnc], axis = 1)\n",
    "#df = pd.concat([df_label,df_data_mod,df_data_mi,df_data_lnc], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLATINUM_STATUS</th>\n",
       "      <th>label</th>\n",
       "      <th>ENSG00000233048</th>\n",
       "      <th>ENSG00000083622</th>\n",
       "      <th>ENSG00000269994</th>\n",
       "      <th>ENSG00000258752</th>\n",
       "      <th>ENSG00000249790</th>\n",
       "      <th>ENSG00000261399</th>\n",
       "      <th>ENSG00000232721</th>\n",
       "      <th>ENSG00000243479</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000231621</th>\n",
       "      <th>ENSG00000225472</th>\n",
       "      <th>ENSG00000231764</th>\n",
       "      <th>ENSG00000241388</th>\n",
       "      <th>ENSG00000233536</th>\n",
       "      <th>ENSG00000234692</th>\n",
       "      <th>ENSG00000224717</th>\n",
       "      <th>ENSG00000262117</th>\n",
       "      <th>ENSG00000248646</th>\n",
       "      <th>ENSG00000258170</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0727</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358713</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040832</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015279</td>\n",
       "      <td>0.071954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0885</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>0.019528</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.027577</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035835</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.003427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-25-1626</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.009001</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.165459</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301344</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.042849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-29-1762</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.062580</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.093347</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079338</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.144516</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121605</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0726</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129586</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008586</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-1914</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146755</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-25-1320</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.075832</td>\n",
       "      <td>0.020374</td>\n",
       "      <td>0.281825</td>\n",
       "      <td>0.033475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.105490</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064487</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.044356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.040054</td>\n",
       "      <td>0.079292</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.015923</td>\n",
       "      <td>0.072904</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-29-1768</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.010945</td>\n",
       "      <td>0.015167</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-09-1670</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.003454</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014156</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLATINUM_STATUS  label  ENSG00000233048  ENSG00000083622  \\\n",
       "PATIENT_ID                                                              \n",
       "TCGA-13-0727       Resistant    1.0         0.000300         0.000000   \n",
       "TCGA-13-0885       Sensitive    0.0         0.000143         0.007784   \n",
       "TCGA-25-1626       Resistant    1.0         0.000000         0.000755   \n",
       "TCGA-29-1762       Sensitive    0.0         0.000036         0.000149   \n",
       "TCGA-13-0726       Sensitive    0.0         0.000112         0.000000   \n",
       "...                      ...    ...              ...              ...   \n",
       "TCGA-61-1914       Sensitive    0.0         0.000000         0.005811   \n",
       "TCGA-25-1320       Sensitive    0.0         0.000000         0.002702   \n",
       "TCGA-04-1362       Resistant    1.0         0.000000         0.001600   \n",
       "TCGA-29-1768       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-09-1670       Sensitive    0.0         0.000015         0.000229   \n",
       "\n",
       "              ENSG00000269994  ENSG00000258752  ENSG00000249790  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0727         0.001284         0.000000         0.000000   \n",
       "TCGA-13-0885         0.019528         0.000750         0.000831   \n",
       "TCGA-25-1626         0.009001         0.009636         0.002136   \n",
       "TCGA-29-1762         0.062580         0.000190         0.002531   \n",
       "TCGA-13-0726         0.000866         0.002348         0.000000   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-1914         0.000070         0.000535         0.001027   \n",
       "TCGA-25-1320         0.007908         0.075832         0.020374   \n",
       "TCGA-04-1362         0.002408         0.003266         0.001810   \n",
       "TCGA-29-1768         0.001755         0.002231         0.000989   \n",
       "TCGA-09-1670         0.000020         0.001831         0.000357   \n",
       "\n",
       "              ENSG00000261399  ENSG00000232721  ENSG00000243479  ...  \\\n",
       "PATIENT_ID                                                       ...   \n",
       "TCGA-13-0727         0.358713         0.005072         0.000000  ...   \n",
       "TCGA-13-0885         0.027577         0.001820         0.000000  ...   \n",
       "TCGA-25-1626         0.165459         0.009359         0.000000  ...   \n",
       "TCGA-29-1762         0.093347         0.000924         0.003958  ...   \n",
       "TCGA-13-0726         0.129586         0.005701         0.000000  ...   \n",
       "...                       ...              ...              ...  ...   \n",
       "TCGA-61-1914         0.004371         0.004500         0.010274  ...   \n",
       "TCGA-25-1320         0.281825         0.033475         0.000000  ...   \n",
       "TCGA-04-1362         0.040054         0.079292         0.046100  ...   \n",
       "TCGA-29-1768         0.010945         0.015167         0.001326  ...   \n",
       "TCGA-09-1670         0.003454         0.000195         0.000598  ...   \n",
       "\n",
       "              ENSG00000231621  ENSG00000225472  ENSG00000231764  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0727         0.040832         0.001053         0.000444   \n",
       "TCGA-13-0885         0.029297         0.000302         0.000425   \n",
       "TCGA-25-1626         0.301344         0.002721         0.021573   \n",
       "TCGA-29-1762         0.079338         0.003070         0.000377   \n",
       "TCGA-13-0726         0.107078         0.000000         0.000166   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-1914         0.146755         0.000029         0.000172   \n",
       "TCGA-25-1320         0.509000         0.009268         0.105490   \n",
       "TCGA-04-1362         1.000000         0.000000         0.027764   \n",
       "TCGA-29-1768         1.000000         0.000000         0.000253   \n",
       "TCGA-09-1670         0.014156         0.000008         0.000034   \n",
       "\n",
       "              ENSG00000241388  ENSG00000233536  ENSG00000234692  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0727         0.010228         0.000000         0.015279   \n",
       "TCGA-13-0885         0.000000         0.035835         0.003654   \n",
       "TCGA-25-1626         0.001231         0.000000         0.000000   \n",
       "TCGA-29-1762         0.144516         0.002274         0.000000   \n",
       "TCGA-13-0726         0.001000         0.000000         0.008586   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-1914         0.000379         0.000000         0.006604   \n",
       "TCGA-25-1320         0.001957         0.000000         0.000000   \n",
       "TCGA-04-1362         0.002781         0.019518         0.015923   \n",
       "TCGA-29-1768         0.000000         0.000000         0.000000   \n",
       "TCGA-09-1670         0.000034         0.000000         0.000981   \n",
       "\n",
       "              ENSG00000224717  ENSG00000262117  ENSG00000248646  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0727         0.071954         0.000000         0.000000   \n",
       "TCGA-13-0885         0.000478         0.000000         0.000085   \n",
       "TCGA-25-1626         0.012292         0.042849         0.000000   \n",
       "TCGA-29-1762         0.121605         0.000705         0.000000   \n",
       "TCGA-13-0726         0.020217         0.003263         0.000000   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-1914         0.000591         0.000000         0.000000   \n",
       "TCGA-25-1320         0.064487         0.012772         0.000524   \n",
       "TCGA-04-1362         0.072904         0.001513         0.000186   \n",
       "TCGA-29-1768         0.000000         0.000827         0.000000   \n",
       "TCGA-09-1670         0.000128         0.000000         0.000009   \n",
       "\n",
       "              ENSG00000258170  \n",
       "PATIENT_ID                     \n",
       "TCGA-13-0727         0.000000  \n",
       "TCGA-13-0885         0.003427  \n",
       "TCGA-25-1626         0.007832  \n",
       "TCGA-29-1762         0.000967  \n",
       "TCGA-13-0726         0.000298  \n",
       "...                       ...  \n",
       "TCGA-61-1914         0.000000  \n",
       "TCGA-25-1320         0.044356  \n",
       "TCGA-04-1362         0.002488  \n",
       "TCGA-29-1768         0.000000  \n",
       "TCGA-09-1670         0.000102  \n",
       "\n",
       "[168 rows x 52 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_platin_total_lnc__50_only.csv\", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLATINUM_STATUS</th>\n",
       "      <th>label</th>\n",
       "      <th>ENSG00000233048</th>\n",
       "      <th>ENSG00000083622</th>\n",
       "      <th>ENSG00000269994</th>\n",
       "      <th>ENSG00000258752</th>\n",
       "      <th>ENSG00000249790</th>\n",
       "      <th>ENSG00000261399</th>\n",
       "      <th>ENSG00000232721</th>\n",
       "      <th>ENSG00000243479</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000231621</th>\n",
       "      <th>ENSG00000225472</th>\n",
       "      <th>ENSG00000231764</th>\n",
       "      <th>ENSG00000241388</th>\n",
       "      <th>ENSG00000233536</th>\n",
       "      <th>ENSG00000234692</th>\n",
       "      <th>ENSG00000224717</th>\n",
       "      <th>ENSG00000262117</th>\n",
       "      <th>ENSG00000248646</th>\n",
       "      <th>ENSG00000258170</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0765</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011328</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068857</td>\n",
       "      <td>0.352923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369531</td>\n",
       "      <td>0.021447</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207416</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.001801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-1918</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.068171</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026556</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.002635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-30-1860</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009180</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.160155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726590</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-23-2078</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.029288</td>\n",
       "      <td>0.112730</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.013658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284441</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.044768</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033612</td>\n",
       "      <td>0.394985</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0893</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.052118</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.087162</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.010560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246936</td>\n",
       "      <td>0.013376</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.016847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1648</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.402176</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.102054</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.045478</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-36-1570</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.069012</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.036243</td>\n",
       "      <td>0.005065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-25-1635</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040385</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.016554</td>\n",
       "      <td>0.305311</td>\n",
       "      <td>0.048353</td>\n",
       "      <td>0.110968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259490</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.104405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242752</td>\n",
       "      <td>0.050808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-23-1027</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.005361</td>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-1544</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>0.005221</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046694</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.025794</td>\n",
       "      <td>0.042566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076881</td>\n",
       "      <td>0.033828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-36-1568</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.006499</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.027568</td>\n",
       "      <td>0.610132</td>\n",
       "      <td>0.201308</td>\n",
       "      <td>0.246397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.702223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-2033</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395988</td>\n",
       "      <td>0.013516</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.015775</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-1467</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.253887</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.089576</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757190</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.015483</td>\n",
       "      <td>0.013570</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-29-1705</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032869</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.053759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052380</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019601</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2000</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017448</td>\n",
       "      <td>0.048270</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.964925</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-1103</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010493</td>\n",
       "      <td>0.126861</td>\n",
       "      <td>0.105289</td>\n",
       "      <td>0.016212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640084</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.239434</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>0.017035</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-29-2414</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.070074</td>\n",
       "      <td>0.011095</td>\n",
       "      <td>0.450169</td>\n",
       "      <td>0.129626</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069799</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.346906</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-2289</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.414851</td>\n",
       "      <td>0.079229</td>\n",
       "      <td>0.044925</td>\n",
       "      <td>0.903899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360114</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.036027</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.143737</td>\n",
       "      <td>0.023503</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-1557</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>0.251003</td>\n",
       "      <td>0.049690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.028273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>0.049893</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-23-2084</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.045183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061864</td>\n",
       "      <td>0.363458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.005424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065006</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-0979</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.017965</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.058526</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-1488</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.033890</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008049</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-1104</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.892039</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>0.007719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033847</td>\n",
       "      <td>0.040335</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-09-1667</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.021090</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.006242</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.026034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089689</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-1924</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.031531</td>\n",
       "      <td>0.418712</td>\n",
       "      <td>0.020723</td>\n",
       "      <td>0.088774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908216</td>\n",
       "      <td>0.022949</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.059511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2097</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.014375</td>\n",
       "      <td>0.053024</td>\n",
       "      <td>0.090973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.029038</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159373</td>\n",
       "      <td>0.014341</td>\n",
       "      <td>0.049149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201863</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-23-1022</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.100057</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009203</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-23-1114</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006147</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.102244</td>\n",
       "      <td>0.010120</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036208</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.068509</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.013549</td>\n",
       "      <td>0.187872</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.002823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1651</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.009743</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.024152</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079878</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-10-0937</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022187</td>\n",
       "      <td>0.055634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271068</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-25-2409</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.014217</td>\n",
       "      <td>0.046995</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.088671</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>0.004297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499280</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007050</td>\n",
       "      <td>0.094992</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0795</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.926647</td>\n",
       "      <td>0.202163</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-20-1683</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541393</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.015648</td>\n",
       "      <td>0.519485</td>\n",
       "      <td>0.034280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505381</td>\n",
       "      <td>0.728846</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0900</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.082162</td>\n",
       "      <td>0.143561</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.057976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076260</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.278631</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0.093276</td>\n",
       "      <td>0.019024</td>\n",
       "      <td>0.330984</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-0970</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.006622</td>\n",
       "      <td>0.219824</td>\n",
       "      <td>0.014506</td>\n",
       "      <td>0.026632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350312</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029130</td>\n",
       "      <td>0.575403</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-10-0927</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008771</td>\n",
       "      <td>0.024302</td>\n",
       "      <td>0.161359</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-29-1693</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.228374</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>0.083004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436725</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>0.013719</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0724</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151772</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011887</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0905</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.024784</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLATINUM_STATUS  label  ENSG00000233048  ENSG00000083622  \\\n",
       "PATIENT_ID                                                              \n",
       "TCGA-13-0765       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-61-1918       Sensitive    0.0         0.000035         0.000000   \n",
       "TCGA-30-1860       Sensitive    0.0         0.000000         0.000733   \n",
       "TCGA-23-2078       Sensitive    0.0         0.000110         0.001576   \n",
       "TCGA-13-0893       Resistant    1.0         0.000000         0.000000   \n",
       "TCGA-04-1648       Sensitive    0.0         0.000467         0.008750   \n",
       "TCGA-61-2109       Sensitive    0.0         0.000031         0.000038   \n",
       "TCGA-36-1570       Sensitive    0.0         0.000150         0.000000   \n",
       "TCGA-25-1635       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-23-1027       Resistant    1.0         0.000018         0.000000   \n",
       "TCGA-24-1544       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-36-1568       Sensitive    0.0         0.001586         0.006499   \n",
       "TCGA-24-2033       Resistant    1.0         0.000958         0.000218   \n",
       "TCGA-24-1467       Sensitive    0.0         0.000091         0.000000   \n",
       "TCGA-29-1705       Resistant    1.0         0.000000         0.000000   \n",
       "TCGA-61-2000       Resistant    1.0         0.000000         0.000000   \n",
       "TCGA-24-1103       Sensitive    0.0         0.000182         0.000000   \n",
       "TCGA-29-2414       Sensitive    0.0         0.000638         0.001308   \n",
       "TCGA-24-2289       Sensitive    0.0         0.001762         0.001444   \n",
       "TCGA-24-1557       Resistant    1.0         0.000000         0.001504   \n",
       "TCGA-23-2084       Sensitive    0.0         0.000000         0.002996   \n",
       "TCGA-24-0979       Sensitive    0.0         0.000161         0.000000   \n",
       "TCGA-13-1488       Sensitive    0.0         0.000118         0.033890   \n",
       "TCGA-24-1104       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-09-1667       Sensitive    0.0         0.000060         0.021090   \n",
       "TCGA-24-1924       Resistant    1.0         0.000544         0.001672   \n",
       "TCGA-61-2097       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-61-2111       Sensitive    0.0         0.000452         0.003009   \n",
       "TCGA-23-1022       Sensitive    0.0         0.000000         0.000148   \n",
       "TCGA-61-2110       Resistant    1.0         0.000000         0.000021   \n",
       "TCGA-23-1114       Sensitive    0.0         0.000266         0.000000   \n",
       "TCGA-04-1651       Sensitive    0.0         0.000022         0.000227   \n",
       "TCGA-10-0937       Resistant    1.0         0.000000         0.000000   \n",
       "TCGA-25-2409       Resistant    1.0         0.000000         0.000567   \n",
       "TCGA-13-0795       Resistant    1.0         0.000000         0.000000   \n",
       "TCGA-20-1683       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-13-0900       Sensitive    0.0         0.000000         0.003823   \n",
       "TCGA-24-0970       Resistant    1.0         0.001143         0.003512   \n",
       "TCGA-10-0927       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-29-1693       Sensitive    0.0         0.000000         0.000000   \n",
       "TCGA-13-0724       Resistant    1.0         0.000000         0.005021   \n",
       "TCGA-13-0905       Sensitive    0.0         0.000004         0.000035   \n",
       "\n",
       "              ENSG00000269994  ENSG00000258752  ENSG00000249790  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0765         0.011328         0.010635         0.003929   \n",
       "TCGA-61-1918         0.001640         0.005929         0.000821   \n",
       "TCGA-30-1860         0.000000         0.000749         0.000000   \n",
       "TCGA-23-2078         0.001694         0.004596         0.029288   \n",
       "TCGA-13-0893         0.002911         0.052118         0.005251   \n",
       "TCGA-04-1648         0.000857         0.057910         0.000773   \n",
       "TCGA-61-2109         0.000047         0.000032         0.000071   \n",
       "TCGA-36-1570         0.000085         0.069012         0.000771   \n",
       "TCGA-25-1635         0.040385         0.007468         0.016554   \n",
       "TCGA-23-1027         0.000366         0.004652         0.005361   \n",
       "TCGA-24-1544         0.000176         0.000358         0.000000   \n",
       "TCGA-36-1568         0.004076         0.004146         0.027568   \n",
       "TCGA-24-2033         0.000547         0.000000         0.000000   \n",
       "TCGA-24-1467         0.000078         0.253887         0.001056   \n",
       "TCGA-29-1705         0.001647         0.000670         0.000000   \n",
       "TCGA-61-2000         0.005804         0.000000         0.017448   \n",
       "TCGA-24-1103         0.000000         0.010493         0.126861   \n",
       "TCGA-29-2414         0.005741         0.070074         0.011095   \n",
       "TCGA-24-2289         0.414851         0.079229         0.044925   \n",
       "TCGA-24-1557         0.001258         0.017908         0.005671   \n",
       "TCGA-23-2084         0.003758         0.007644         0.045183   \n",
       "TCGA-24-0979         0.000069         0.017965         0.000311   \n",
       "TCGA-13-1488         0.000911         0.000000         0.002738   \n",
       "TCGA-24-1104         0.001277         0.006494         0.008637   \n",
       "TCGA-09-1667         0.000154         0.000626         0.006242   \n",
       "TCGA-24-1924         0.000000         0.005690         0.031531   \n",
       "TCGA-61-2097         0.000000         0.001441         0.014375   \n",
       "TCGA-61-2111         0.029038         0.001181         0.000000   \n",
       "TCGA-23-1022         0.100057         0.004909         0.007533   \n",
       "TCGA-61-2110         0.000036         0.000164         0.000000   \n",
       "TCGA-23-1114         0.006147         0.011810         0.004620   \n",
       "TCGA-04-1651         0.000114         0.000925         0.009743   \n",
       "TCGA-10-0937         0.000000         0.013266         0.000000   \n",
       "TCGA-25-2409         0.014217         0.046995         0.025641   \n",
       "TCGA-13-0795         0.000000         0.000771         0.000854   \n",
       "TCGA-20-1683         0.541393         0.007060         0.015648   \n",
       "TCGA-13-0900         0.000959         0.015607         0.082162   \n",
       "TCGA-24-0970         0.001469         0.032860         0.006622   \n",
       "TCGA-10-0927         0.000000         0.008771         0.024302   \n",
       "TCGA-29-1693         0.016477         0.011173         0.004127   \n",
       "TCGA-13-0724         0.000573         0.019802         0.000000   \n",
       "TCGA-13-0905         0.000041         0.000030         0.000100   \n",
       "\n",
       "              ENSG00000261399  ENSG00000232721  ENSG00000243479  ...  \\\n",
       "PATIENT_ID                                                       ...   \n",
       "TCGA-13-0765         1.000000         0.068857         0.352923  ...   \n",
       "TCGA-61-1918         0.068171         0.006298         0.004955  ...   \n",
       "TCGA-30-1860         0.009180         0.005452         0.160155  ...   \n",
       "TCGA-23-2078         0.112730         0.005579         0.013658  ...   \n",
       "TCGA-13-0893         0.087162         0.011503         0.010560  ...   \n",
       "TCGA-04-1648         0.402176         0.018634         0.002073  ...   \n",
       "TCGA-61-2109         0.045478         0.000621         0.001045  ...   \n",
       "TCGA-36-1570         0.036243         0.005065         0.000000  ...   \n",
       "TCGA-25-1635         0.305311         0.048353         0.110968  ...   \n",
       "TCGA-23-1027         0.073022         0.002710         0.002488  ...   \n",
       "TCGA-24-1544         0.026371         0.005221         0.006390  ...   \n",
       "TCGA-36-1568         0.610132         0.201308         0.246397  ...   \n",
       "TCGA-24-2033         0.395988         0.013516         0.003309  ...   \n",
       "TCGA-24-1467         0.089576         0.000771         0.000000  ...   \n",
       "TCGA-29-1705         0.032869         0.003253         0.053759  ...   \n",
       "TCGA-61-2000         0.048270         0.009556         0.964925  ...   \n",
       "TCGA-24-1103         0.105289         0.016212         0.000000  ...   \n",
       "TCGA-29-2414         0.450169         0.129626         0.004958  ...   \n",
       "TCGA-24-2289         0.903899         0.000000         0.000000  ...   \n",
       "TCGA-24-1557         0.251003         0.049690         0.000000  ...   \n",
       "TCGA-23-2084         1.000000         0.061864         0.363458  ...   \n",
       "TCGA-24-0979         0.058526         0.005452         0.000417  ...   \n",
       "TCGA-13-1488         1.000000         0.000000         0.000000  ...   \n",
       "TCGA-24-1104         0.892039         0.006307         0.007719  ...   \n",
       "TCGA-09-1667         0.015349         0.006077         0.026034  ...   \n",
       "TCGA-24-1924         0.418712         0.020723         0.088774  ...   \n",
       "TCGA-61-2097         0.053024         0.090973         1.000000  ...   \n",
       "TCGA-61-2111         0.159373         0.014341         0.049149  ...   \n",
       "TCGA-23-1022         0.009262         0.001834         0.001122  ...   \n",
       "TCGA-61-2110         0.004026         0.000443         0.000108  ...   \n",
       "TCGA-23-1114         0.102244         0.010120         0.008258  ...   \n",
       "TCGA-04-1651         0.002837         0.024152         0.100027  ...   \n",
       "TCGA-10-0937         0.022187         0.055634         0.000000  ...   \n",
       "TCGA-25-2409         0.088671         0.007021         0.004297  ...   \n",
       "TCGA-13-0795         0.926647         0.202163         0.004582  ...   \n",
       "TCGA-20-1683         0.519485         0.034280         1.000000  ...   \n",
       "TCGA-13-0900         0.143561         0.018947         0.057976  ...   \n",
       "TCGA-24-0970         0.219824         0.014506         0.026632  ...   \n",
       "TCGA-10-0927         0.161359         0.031944         0.000000  ...   \n",
       "TCGA-29-1693         0.228374         0.027126         0.083004  ...   \n",
       "TCGA-13-0724         1.000000         0.079186         0.000000  ...   \n",
       "TCGA-13-0905         0.024784         0.000183         0.000515  ...   \n",
       "\n",
       "              ENSG00000231621  ENSG00000225472  ENSG00000231764  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0765         0.369531         0.021447         0.002511   \n",
       "TCGA-61-1918         0.026556         0.000299         0.000053   \n",
       "TCGA-30-1860         0.726590         0.001057         0.000106   \n",
       "TCGA-23-2078         0.284441         0.001622         0.044768   \n",
       "TCGA-13-0893         0.246936         0.013376         0.001678   \n",
       "TCGA-04-1648         1.000000         0.000281         0.007810   \n",
       "TCGA-61-2109         0.013329         0.000052         0.000127   \n",
       "TCGA-36-1570         1.000000         0.000105         0.000123   \n",
       "TCGA-25-1635         0.259490         0.027108         0.104405   \n",
       "TCGA-23-1027         0.126065         0.000000         0.000527   \n",
       "TCGA-24-1544         0.046694         0.000867         0.025794   \n",
       "TCGA-36-1568         0.702223         0.000000         0.003524   \n",
       "TCGA-24-2033         0.007253         0.000898         0.015775   \n",
       "TCGA-24-1467         0.757190         0.000064         0.000045   \n",
       "TCGA-29-1705         0.052380         0.000270         0.001709   \n",
       "TCGA-61-2000         1.000000         0.005556         0.006134   \n",
       "TCGA-24-1103         0.640084         0.000385         0.000000   \n",
       "TCGA-29-2414         1.000000         0.005383         0.001418   \n",
       "TCGA-24-2289         0.360114         0.000743         0.036027   \n",
       "TCGA-24-1557         1.000000         0.001548         0.028273   \n",
       "TCGA-23-2084         0.199200         0.020553         0.001444   \n",
       "TCGA-24-0979         0.480969         0.000000         0.000020   \n",
       "TCGA-13-1488         0.008049         0.002242         0.003501   \n",
       "TCGA-24-1104         0.033847         0.040335         0.002944   \n",
       "TCGA-09-1667         0.089689         0.000505         0.004788   \n",
       "TCGA-24-1924         0.908216         0.022949         0.002419   \n",
       "TCGA-61-2097         0.131443         0.000291         0.000204   \n",
       "TCGA-61-2111         0.023089         0.000000         0.201863   \n",
       "TCGA-23-1022         0.118085         0.000000         0.009203   \n",
       "TCGA-61-2110         0.010931         0.000022         0.000010   \n",
       "TCGA-23-1114         0.036208         0.001401         0.068509   \n",
       "TCGA-04-1651         0.079878         0.000467         0.003016   \n",
       "TCGA-10-0937         0.271068         0.000243         0.000000   \n",
       "TCGA-25-2409         0.499280         0.001166         0.000000   \n",
       "TCGA-13-0795         0.010046         0.001866         0.000546   \n",
       "TCGA-20-1683         0.000000         0.505381         0.728846   \n",
       "TCGA-13-0900         0.076260         0.000787         0.278631   \n",
       "TCGA-24-0970         0.350312         0.002410         0.002540   \n",
       "TCGA-10-0927         1.000000         0.000884         0.000000   \n",
       "TCGA-29-1693         0.436725         0.004506         0.013719   \n",
       "TCGA-13-0724         0.151772         0.000470         0.000330   \n",
       "TCGA-13-0905         0.006779         0.000636         0.000013   \n",
       "\n",
       "              ENSG00000241388  ENSG00000233536  ENSG00000234692  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0765         0.003773         0.000000         0.207416   \n",
       "TCGA-61-1918         0.000473         0.000000         0.001807   \n",
       "TCGA-30-1860         0.000159         0.000000         0.000000   \n",
       "TCGA-23-2078         0.001467         0.000000         0.033612   \n",
       "TCGA-13-0893         0.002521         0.000000         0.000000   \n",
       "TCGA-04-1648         0.002079         0.002085         0.102054   \n",
       "TCGA-61-2109         0.000014         0.021015         0.000623   \n",
       "TCGA-36-1570         0.000111         0.000000         0.038140   \n",
       "TCGA-25-1635         0.000000         0.000000         0.242752   \n",
       "TCGA-23-1027         0.000119         0.000000         0.000000   \n",
       "TCGA-24-1544         0.042566         0.000000         0.076881   \n",
       "TCGA-36-1568         0.003530         0.000000         0.000000   \n",
       "TCGA-24-2033         0.000237         0.000000         0.000000   \n",
       "TCGA-24-1467         0.000203         0.000949         0.015483   \n",
       "TCGA-29-1705         0.000000         0.000000         0.019601   \n",
       "TCGA-61-2000         0.000000         0.000000         0.000000   \n",
       "TCGA-24-1103         0.000406         0.239434         0.009302   \n",
       "TCGA-29-2414         0.000000         0.069799         0.016269   \n",
       "TCGA-24-2289         0.000784         0.044047         0.143737   \n",
       "TCGA-24-1557         0.000000         0.007645         0.049893   \n",
       "TCGA-23-2084         0.005424         0.000000         0.000000   \n",
       "TCGA-24-0979         0.000060         0.000000         0.004790   \n",
       "TCGA-13-1488         0.000789         0.000000         0.000000   \n",
       "TCGA-24-1104         0.001659         0.000000         0.000000   \n",
       "TCGA-09-1667         0.000133         0.000000         0.000000   \n",
       "TCGA-24-1924         0.002422         0.059511         0.000000   \n",
       "TCGA-61-2097         0.022394         0.004306         0.000000   \n",
       "TCGA-61-2111         0.202428         0.000000         0.000000   \n",
       "TCGA-23-1022         0.001768         0.000000         0.003682   \n",
       "TCGA-61-2110         0.000280         0.000545         0.000000   \n",
       "TCGA-23-1114         0.002070         0.004152         0.013549   \n",
       "TCGA-04-1651         0.000542         0.001383         0.000000   \n",
       "TCGA-10-0937         0.000385         0.000000         0.000000   \n",
       "TCGA-25-2409         0.000000         0.000000         0.007050   \n",
       "TCGA-13-0795         0.000985         0.000000         0.000000   \n",
       "TCGA-20-1683         0.002004         0.000000         0.011473   \n",
       "TCGA-13-0900         0.006644         0.093276         0.019024   \n",
       "TCGA-24-0970         0.002544         0.000000         0.029130   \n",
       "TCGA-10-0927         0.000000         0.000000         0.000000   \n",
       "TCGA-29-1693         0.000793         0.000000         0.000000   \n",
       "TCGA-13-0724         0.001488         0.000000         0.000000   \n",
       "TCGA-13-0905         0.000006         0.000090         0.000441   \n",
       "\n",
       "              ENSG00000224717  ENSG00000262117  ENSG00000248646  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-13-0765         0.004522         0.000000         0.000404   \n",
       "TCGA-61-1918         0.000473         0.000000         0.000085   \n",
       "TCGA-30-1860         0.000477         0.000000         0.000000   \n",
       "TCGA-23-2078         0.394985         0.003193         0.000000   \n",
       "TCGA-13-0893         0.000000         0.004389         0.000270   \n",
       "TCGA-04-1648         0.004005         0.000646         0.000000   \n",
       "TCGA-61-2109         0.000489         0.000000         0.000000   \n",
       "TCGA-36-1570         0.000443         0.000322         0.000000   \n",
       "TCGA-25-1635         0.050808         0.000000         0.000000   \n",
       "TCGA-23-1027         0.000712         0.000517         0.000000   \n",
       "TCGA-24-1544         0.033828         0.000000         0.000000   \n",
       "TCGA-36-1568         0.190378         0.000000         0.000000   \n",
       "TCGA-24-2033         0.000000         0.000000         0.000000   \n",
       "TCGA-24-1467         0.013570         0.000588         0.000072   \n",
       "TCGA-29-1705         0.007692         0.001241         0.000153   \n",
       "TCGA-61-2000         0.015062         0.007292         0.000000   \n",
       "TCGA-24-1103         0.017035         0.001767         0.000218   \n",
       "TCGA-29-2414         0.346906         0.009273         0.000000   \n",
       "TCGA-24-2289         0.023503         0.013655         0.000000   \n",
       "TCGA-24-1557         0.016317         0.000000         0.000000   \n",
       "TCGA-23-2084         0.065006         0.009442         0.000000   \n",
       "TCGA-24-0979         0.000269         0.000390         0.000064   \n",
       "TCGA-13-1488         0.015759         0.000000         0.000000   \n",
       "TCGA-24-1104         0.008284         0.000000         0.000000   \n",
       "TCGA-09-1667         0.039512         0.000000         0.000000   \n",
       "TCGA-24-1924         0.029033         0.000000         0.000000   \n",
       "TCGA-61-2097         0.029413         0.000000         0.000000   \n",
       "TCGA-61-2111         0.006781         0.003283         0.000000   \n",
       "TCGA-23-1022         0.000482         0.002099         0.000000   \n",
       "TCGA-61-2110         0.001024         0.000000         0.000000   \n",
       "TCGA-23-1114         0.187872         0.001287         0.000158   \n",
       "TCGA-04-1651         0.015050         0.002357         0.000000   \n",
       "TCGA-10-0937         0.001923         0.000000         0.000000   \n",
       "TCGA-25-2409         0.094992         0.006697         0.000000   \n",
       "TCGA-13-0795         0.000000         0.019998         0.000000   \n",
       "TCGA-20-1683         0.015009         0.008720         0.000000   \n",
       "TCGA-13-0900         0.330984         0.007229         0.000000   \n",
       "TCGA-24-0970         0.575403         0.005535         0.000681   \n",
       "TCGA-10-0927         0.274120         0.000000         0.000000   \n",
       "TCGA-29-1693         1.000000         0.010350         0.000000   \n",
       "TCGA-13-0724         0.011887         0.008632         0.000000   \n",
       "TCGA-13-0905         0.000693         0.000014         0.000000   \n",
       "\n",
       "              ENSG00000258170  \n",
       "PATIENT_ID                     \n",
       "TCGA-13-0765         0.001801  \n",
       "TCGA-61-1918         0.002635  \n",
       "TCGA-30-1860         0.077565  \n",
       "TCGA-23-2078         0.001167  \n",
       "TCGA-13-0893         0.016847  \n",
       "TCGA-04-1648         0.004962  \n",
       "TCGA-61-2109         0.000000  \n",
       "TCGA-36-1570         0.000177  \n",
       "TCGA-25-1635         0.037936  \n",
       "TCGA-23-1027         0.000000  \n",
       "TCGA-24-1544         0.000000  \n",
       "TCGA-36-1568         0.008424  \n",
       "TCGA-24-2033         0.001697  \n",
       "TCGA-24-1467         0.000000  \n",
       "TCGA-29-1705         0.000681  \n",
       "TCGA-61-2000         0.039985  \n",
       "TCGA-24-1103         0.000485  \n",
       "TCGA-29-2414         0.000000  \n",
       "TCGA-24-2289         0.000000  \n",
       "TCGA-24-1557         0.006498  \n",
       "TCGA-23-2084         0.062127  \n",
       "TCGA-24-0979         0.000285  \n",
       "TCGA-13-1488         0.033888  \n",
       "TCGA-24-1104         0.006598  \n",
       "TCGA-09-1667         0.000000  \n",
       "TCGA-24-1924         0.002890  \n",
       "TCGA-61-2097         0.000000  \n",
       "TCGA-61-2111         0.000000  \n",
       "TCGA-23-1022         0.000384  \n",
       "TCGA-61-2110         0.000000  \n",
       "TCGA-23-1114         0.002823  \n",
       "TCGA-04-1651         0.002233  \n",
       "TCGA-10-0937         0.000000  \n",
       "TCGA-25-2409         0.000000  \n",
       "TCGA-13-0795         0.001175  \n",
       "TCGA-20-1683         0.001195  \n",
       "TCGA-13-0900         0.011892  \n",
       "TCGA-24-0970         0.003035  \n",
       "TCGA-10-0927         0.000000  \n",
       "TCGA-29-1693         0.007567  \n",
       "TCGA-13-0724         0.001183  \n",
       "TCGA-13-0905         0.000153  \n",
       "\n",
       "[42 rows x 52 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.to_csv(\"val_platin_total_lnc__50_only.csv\", mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_platin_total_lnc__50_only.csv\")\n",
    "val = pd.read_csv(\"val_platin_total_lnc__50_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of Training Data\n",
      "Samples : 168\n",
      "Features : 50\n"
     ]
    }
   ],
   "source": [
    "trn_X_pd = train.drop([\"PATIENT_ID\",\"PLATINUM_STATUS\",\"label\"],axis=1).values\n",
    "trn_y_pd = train.label.values\n",
    "\n",
    "print(\"Information of Training Data\")\n",
    "print(\"Samples : {}\".format(trn_X_pd.shape[0]))\n",
    "print(\"Features : {}\".format(trn_X_pd.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of Training Data\n",
      "Samples : 42\n",
      "Features : 50\n"
     ]
    }
   ],
   "source": [
    "val_X_pd = val.drop([\"PATIENT_ID\",\"PLATINUM_STATUS\",\"label\"],axis=1).values\n",
    "val_y_pd = val.label.values\n",
    "\n",
    "print(\"Information of Training Data\")\n",
    "print(\"Samples : {}\".format(val_X_pd.shape[0]))\n",
    "print(\"Features : {}\".format(val_X_pd.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "29\n",
      "48\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "item = []\n",
    "item_2 = []\n",
    "\n",
    "print( (val_y_pd == 1.0).sum())\n",
    "print( (val_y_pd == 0.0).sum())\n",
    "\n",
    "print( (trn_y_pd == 1.0).sum())\n",
    "print( (trn_y_pd == 0.0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  0,   2,   3,   4,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n",
       "          16,  17,  18,  19,  20,  22,  23,  24,  25,  26,  27,  30,  31,\n",
       "          32,  33,  34,  35,  36,  37,  38,  39,  41,  42,  43,  44,  46,\n",
       "          47,  48,  49,  50,  51,  53,  56,  57,  58,  60,  62,  64,  65,\n",
       "          67,  68,  69,  70,  71,  72,  73,  76,  77,  78,  81,  82,  83,\n",
       "          84,  85,  87,  89,  90,  91,  92,  94,  95,  96,  97,  98, 100,\n",
       "         101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
       "         114, 115, 116, 117, 118, 120, 121, 123, 125, 128, 130, 131, 132,\n",
       "         133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "         147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 159, 160, 162,\n",
       "         163, 164, 165, 166]),\n",
       "  array([  1,   7,  13,  21,  28,  29,  40,  45,  52,  54,  55,  59,  61,\n",
       "          63,  66,  74,  75,  79,  80,  86,  88,  93,  99, 119, 122, 124,\n",
       "         126, 127, 129, 138, 155, 158, 161, 167])),\n",
       " (array([  0,   1,   2,   3,   4,   5,   7,   8,   9,  10,  12,  13,  14,\n",
       "          17,  18,  19,  20,  21,  22,  25,  26,  27,  28,  29,  30,  32,\n",
       "          33,  34,  37,  38,  39,  40,  43,  44,  45,  47,  48,  49,  50,\n",
       "          52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  63,  65,  66,\n",
       "          67,  68,  69,  70,  71,  72,  74,  75,  76,  77,  78,  79,  80,\n",
       "          82,  83,  84,  86,  87,  88,  91,  92,  93,  96,  97,  98,  99,\n",
       "         100, 101, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 115,\n",
       "         116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "         130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 143, 144, 146,\n",
       "         148, 149, 150, 151, 152, 153, 155, 157, 158, 159, 160, 161, 162,\n",
       "         163, 165, 166, 167]),\n",
       "  array([  6,  11,  15,  16,  23,  24,  31,  35,  36,  41,  42,  46,  51,\n",
       "          62,  64,  73,  81,  85,  89,  90,  94,  95, 102, 110, 114, 120,\n",
       "         134, 141, 142, 145, 147, 154, 156, 164])),\n",
       " (array([  0,   1,   2,   3,   4,   6,   7,   8,  10,  11,  12,  13,  14,\n",
       "          15,  16,  17,  18,  19,  21,  23,  24,  25,  26,  28,  29,  30,\n",
       "          31,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n",
       "          49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  61,  62,\n",
       "          63,  64,  66,  69,  70,  72,  73,  74,  75,  76,  77,  79,  80,\n",
       "          81,  83,  84,  85,  86,  88,  89,  90,  93,  94,  95,  96,  98,\n",
       "          99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 111, 113, 114,\n",
       "         118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 132, 133,\n",
       "         134, 135, 136, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148,\n",
       "         149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n",
       "         162, 163, 164, 167]),\n",
       "  array([  5,   9,  20,  22,  27,  32,  38,  43,  47,  48,  60,  65,  67,\n",
       "          68,  71,  78,  82,  87,  91,  92,  97, 105, 106, 112, 115, 116,\n",
       "         117, 123, 130, 131, 137, 143, 165, 166])),\n",
       " (array([  0,   1,   2,   3,   4,   5,   6,   7,   9,  10,  11,  12,  13,\n",
       "          15,  16,  18,  20,  21,  22,  23,  24,  25,  27,  28,  29,  31,\n",
       "          32,  33,  35,  36,  38,  40,  41,  42,  43,  44,  45,  46,  47,\n",
       "          48,  50,  51,  52,  54,  55,  59,  60,  61,  62,  63,  64,  65,\n",
       "          66,  67,  68,  71,  73,  74,  75,  77,  78,  79,  80,  81,  82,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,\n",
       "          97,  98,  99, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
       "         125, 126, 127, 128, 129, 130, 131, 133, 134, 136, 137, 138, 139,\n",
       "         140, 141, 142, 143, 145, 147, 149, 150, 154, 155, 156, 158, 161,\n",
       "         162, 164, 165, 166, 167]),\n",
       "  array([  8,  14,  17,  19,  26,  30,  34,  37,  39,  49,  53,  56,  57,\n",
       "          58,  69,  70,  72,  76,  83, 100, 104, 132, 135, 144, 146, 148,\n",
       "         151, 152, 153, 157, 159, 160, 163])),\n",
       " (array([  1,   5,   6,   7,   8,   9,  11,  13,  14,  15,  16,  17,  19,\n",
       "          20,  21,  22,  23,  24,  26,  27,  28,  29,  30,  31,  32,  34,\n",
       "          35,  36,  37,  38,  39,  40,  41,  42,  43,  45,  46,  47,  48,\n",
       "          49,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,\n",
       "          63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n",
       "          76,  78,  79,  80,  81,  82,  83,  85,  86,  87,  88,  89,  90,\n",
       "          91,  92,  93,  94,  95,  97,  99, 100, 102, 104, 105, 106, 110,\n",
       "         112, 114, 115, 116, 117, 119, 120, 122, 123, 124, 126, 127, 129,\n",
       "         130, 131, 132, 134, 135, 137, 138, 141, 142, 143, 144, 145, 146,\n",
       "         147, 148, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n",
       "         163, 164, 165, 166, 167]),\n",
       "  array([  0,   2,   3,   4,  10,  12,  18,  25,  33,  44,  50,  77,  84,\n",
       "          96,  98, 101, 103, 107, 108, 109, 111, 113, 118, 121, 125, 128,\n",
       "         133, 136, 139, 140, 149, 150, 162]))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 5 # Number of K-fold Splits\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(trn_X_pd, trn_y_pd))\n",
    "splits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "24\n",
      "10\n",
      "24\n",
      "10\n",
      "24\n",
      "9\n",
      "24\n",
      "9\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "\n",
    "    print( (trn_y_pd[valid_idx.astype(int)] == 1.0).sum())\n",
    "    print( (trn_y_pd[valid_idx.astype(int)] == 0.0).sum())\n",
    "    \n",
    "    #print( (trn_y_pd[train_idx.astype(int)] == 1.0).sum())\n",
    "    #print( (trn_y_pd[train_idx.astype(int)] == 0.0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dictionary batch\n",
    "class Dataset(data_utils.Dataset):\n",
    "   \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        return {'X': self.X[idx], 'y': self.y[idx]}\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_seq_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN_seq_1, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear((trn_X_pd.shape[1]), 50, bias=True),\n",
    "            torch.nn.BatchNorm1d(50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "\n",
    "            torch.nn.Linear(50, 150, bias=True),\n",
    "            torch.nn.BatchNorm1d(150),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            \n",
    "\n",
    "            #torch.nn.Dropout(0.3)\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            torch.nn.Linear(150,1, bias=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        x_out = self.classifier(x)\n",
    "        x_out = self.classifier2(x_out)\n",
    "        x_out = self.output_layer(x_out)\n",
    "        #x_out = F.dropout(x_out, p=0.5, training=self.training)\n",
    "        return torch.sigmoid(x_out)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[150,200,100,50]\n",
    "#[150,200,100,50,20]\n",
    "\n",
    "class DNN_seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN_seq, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear((trn_X_pd.shape[1]), 200, bias=True),\n",
    "            torch.nn.BatchNorm1d(200),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        self.classifier2 = nn.Sequential(    \n",
    "            torch.nn.Linear(200, 300, bias=True),\n",
    "            torch.nn.BatchNorm1d(300),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.classifier3 = nn.Sequential(  \n",
    "            torch.nn.Linear(300, 300, bias=True),\n",
    "            torch.nn.BatchNorm1d(300),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            torch.nn.Linear(300, 300, bias=True),\n",
    "            torch.nn.BatchNorm1d(300),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            \n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            torch.nn.Linear(300, 1, bias=True),\n",
    "            #nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        x_out = self.classifier(x)\n",
    "        x_out = self.classifier2(x_out)\n",
    "        x_out = self.classifier3(x_out)\n",
    "        x_out = self.output_layer(x_out)\n",
    "        #x_out = F.dropout(x_out, p=0.5, training=self.training)\n",
    "        return torch.sigmoid(x_out)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[150,200,100,50]\n",
    "#[150,200,100,50,20]\n",
    "\n",
    "class DNN_seq_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN_seq_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear((trn_X_pd.shape[1]), 150, bias=True),\n",
    "            torch.nn.BatchNorm1d(150),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.classifier2 = nn.Sequential(    \n",
    "            torch.nn.Linear(150, 100, bias=True),\n",
    "            torch.nn.BatchNorm1d(100),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.classifier3 = nn.Sequential(  \n",
    "            torch.nn.Linear(100, 150, bias=True),\n",
    "            torch.nn.BatchNorm1d(150),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            torch.nn.Linear(150, 250, bias=True),\n",
    "            torch.nn.BatchNorm1d(250),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            \n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            torch.nn.Linear(250, 1, bias=True),\n",
    "            torch.nn.BatchNorm1d(1)\n",
    "            #nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        x_out = self.classifier(x)\n",
    "        x_out = self.classifier2(x_out)\n",
    "        x_out = self.classifier3(x_out)\n",
    "        x_out = self.output_layer(x_out)\n",
    "        #x_out = F.dropout(x_out, p=0.5, training=self.training)\n",
    "        return torch.sigmoid(x_out)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN_seq_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "import time # ??\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    }
   ],
   "source": [
    "print(trn_X_pd.shape[0])\n",
    "          \n",
    "train_preds = np.zeros(trn_X_pd.shape[0])\n",
    "\n",
    "## Addiction\n",
    "train_y_sort = np.zeros(trn_X_pd.shape[0])\n",
    "\n",
    "test_preds = np.zeros(val_X_pd.shape[0])\n",
    "\n",
    "\n",
    "train_target = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Fold 1\n",
      "Epoch 1/50 \t loss=0.6270 \t val_loss=0.6313 \t time=0.13s\n",
      "Epoch 2/50 \t loss=0.5605 \t val_loss=0.5957 \t time=0.10s\n",
      "Epoch 3/50 \t loss=0.5530 \t val_loss=0.5671 \t time=0.09s\n",
      "Epoch 4/50 \t loss=0.5520 \t val_loss=0.5545 \t time=0.08s\n",
      "Epoch 5/50 \t loss=0.5468 \t val_loss=0.5319 \t time=0.11s\n",
      "Epoch 6/50 \t loss=0.5232 \t val_loss=0.5100 \t time=0.08s\n",
      "Epoch 7/50 \t loss=0.4973 \t val_loss=0.4909 \t time=0.08s\n",
      "Epoch 8/50 \t loss=0.4986 \t val_loss=0.4906 \t time=0.12s\n",
      "Epoch 9/50 \t loss=0.5061 \t val_loss=0.4942 \t time=0.11s\n",
      "Epoch 10/50 \t loss=0.5144 \t val_loss=0.5283 \t time=0.09s\n",
      "Epoch 11/50 \t loss=0.4587 \t val_loss=0.5283 \t time=0.13s\n",
      "Epoch 12/50 \t loss=0.4839 \t val_loss=0.5354 \t time=0.18s\n",
      "Epoch 13/50 \t loss=0.4943 \t val_loss=0.5300 \t time=0.15s\n",
      "Epoch 14/50 \t loss=0.4680 \t val_loss=0.5455 \t time=0.12s\n",
      "Epoch 15/50 \t loss=0.4661 \t val_loss=0.5570 \t time=0.08s\n",
      "Epoch 16/50 \t loss=0.4920 \t val_loss=0.5430 \t time=0.11s\n",
      "Epoch 17/50 \t loss=0.4455 \t val_loss=0.5386 \t time=0.09s\n",
      "Epoch 18/50 \t loss=0.4761 \t val_loss=0.5825 \t time=0.10s\n",
      "Epoch 19/50 \t loss=0.4893 \t val_loss=0.5610 \t time=0.09s\n",
      "Epoch 20/50 \t loss=0.3660 \t val_loss=0.5574 \t time=0.09s\n",
      "Epoch 21/50 \t loss=0.4700 \t val_loss=0.5491 \t time=0.08s\n",
      "Epoch 22/50 \t loss=0.4871 \t val_loss=0.5228 \t time=0.10s\n",
      "Epoch 23/50 \t loss=0.4890 \t val_loss=0.5429 \t time=0.10s\n",
      "Epoch 24/50 \t loss=0.4239 \t val_loss=0.5613 \t time=0.10s\n",
      "Epoch 25/50 \t loss=0.4184 \t val_loss=0.6077 \t time=0.10s\n",
      "Epoch 26/50 \t loss=0.4388 \t val_loss=0.5493 \t time=0.11s\n",
      "Epoch 27/50 \t loss=0.4420 \t val_loss=0.5695 \t time=0.10s\n",
      "Epoch 28/50 \t loss=0.4391 \t val_loss=0.5911 \t time=0.08s\n",
      "Epoch 29/50 \t loss=0.5014 \t val_loss=0.5746 \t time=0.10s\n",
      "Epoch 30/50 \t loss=0.4286 \t val_loss=0.5785 \t time=0.09s\n",
      "Epoch 31/50 \t loss=0.4351 \t val_loss=0.6146 \t time=0.08s\n",
      "Epoch 32/50 \t loss=0.4493 \t val_loss=0.6479 \t time=0.10s\n",
      "Epoch 33/50 \t loss=0.4377 \t val_loss=0.5925 \t time=0.09s\n",
      "Epoch 34/50 \t loss=0.4567 \t val_loss=0.6489 \t time=0.09s\n",
      "Epoch 35/50 \t loss=0.4104 \t val_loss=0.5795 \t time=0.09s\n",
      "Epoch 36/50 \t loss=0.4640 \t val_loss=0.5897 \t time=0.09s\n",
      "Epoch 37/50 \t loss=0.3822 \t val_loss=0.6170 \t time=0.08s\n",
      "Epoch 38/50 \t loss=0.4153 \t val_loss=0.6048 \t time=0.11s\n",
      "Epoch 39/50 \t loss=0.5179 \t val_loss=0.5548 \t time=0.10s\n",
      "Epoch 40/50 \t loss=0.4353 \t val_loss=0.5814 \t time=0.09s\n",
      "Epoch 41/50 \t loss=0.4208 \t val_loss=0.6607 \t time=0.11s\n",
      "Epoch 42/50 \t loss=0.4287 \t val_loss=0.6281 \t time=0.10s\n",
      "Epoch 43/50 \t loss=0.3753 \t val_loss=0.6006 \t time=0.10s\n",
      "Epoch 44/50 \t loss=0.3619 \t val_loss=0.6078 \t time=0.10s\n",
      "Epoch 45/50 \t loss=0.4735 \t val_loss=0.6561 \t time=0.09s\n",
      "Epoch 46/50 \t loss=0.4150 \t val_loss=0.6481 \t time=0.09s\n",
      "Epoch 47/50 \t loss=0.4271 \t val_loss=0.6627 \t time=0.09s\n",
      "Epoch 48/50 \t loss=0.4501 \t val_loss=0.6623 \t time=0.09s\n",
      "Epoch 49/50 \t loss=0.4050 \t val_loss=0.6431 \t time=0.10s\n",
      "Epoch 50/50 \t loss=0.3448 \t val_loss=0.7046 \t time=0.09s\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "1\n",
      "Fold 2\n",
      "Epoch 1/50 \t loss=0.6987 \t val_loss=0.6169 \t time=0.09s\n",
      "Epoch 2/50 \t loss=0.5511 \t val_loss=0.5280 \t time=0.09s\n",
      "Epoch 3/50 \t loss=0.5772 \t val_loss=0.4712 \t time=0.09s\n",
      "Epoch 4/50 \t loss=0.5523 \t val_loss=0.4466 \t time=0.11s\n",
      "Epoch 5/50 \t loss=0.5229 \t val_loss=0.4389 \t time=0.11s\n",
      "Epoch 6/50 \t loss=0.5301 \t val_loss=0.4277 \t time=0.12s\n",
      "Epoch 7/50 \t loss=0.5562 \t val_loss=0.4028 \t time=0.11s\n",
      "Epoch 8/50 \t loss=0.5171 \t val_loss=0.4128 \t time=0.10s\n",
      "Epoch 9/50 \t loss=0.5104 \t val_loss=0.3736 \t time=0.09s\n",
      "Epoch 10/50 \t loss=0.5035 \t val_loss=0.4134 \t time=0.11s\n",
      "Epoch 11/50 \t loss=0.5666 \t val_loss=0.4005 \t time=0.09s\n",
      "Epoch 12/50 \t loss=0.4811 \t val_loss=0.3846 \t time=0.09s\n",
      "Epoch 13/50 \t loss=0.4980 \t val_loss=0.3979 \t time=0.10s\n",
      "Epoch 14/50 \t loss=0.5176 \t val_loss=0.4355 \t time=0.09s\n",
      "Epoch 15/50 \t loss=0.5131 \t val_loss=0.4093 \t time=0.09s\n",
      "Epoch 16/50 \t loss=0.4843 \t val_loss=0.3807 \t time=0.10s\n",
      "Epoch 17/50 \t loss=0.4860 \t val_loss=0.3583 \t time=0.09s\n",
      "Epoch 18/50 \t loss=0.4656 \t val_loss=0.3765 \t time=0.09s\n",
      "Epoch 19/50 \t loss=0.4647 \t val_loss=0.3817 \t time=0.11s\n",
      "Epoch 20/50 \t loss=0.5156 \t val_loss=0.3756 \t time=0.10s\n",
      "Epoch 21/50 \t loss=0.4539 \t val_loss=0.3841 \t time=0.10s\n",
      "Epoch 22/50 \t loss=0.4095 \t val_loss=0.3861 \t time=0.12s\n",
      "Epoch 23/50 \t loss=0.4989 \t val_loss=0.4913 \t time=0.11s\n",
      "Epoch 24/50 \t loss=0.4913 \t val_loss=0.3873 \t time=0.09s\n",
      "Epoch 25/50 \t loss=0.4281 \t val_loss=0.4219 \t time=0.09s\n",
      "Epoch 26/50 \t loss=0.4686 \t val_loss=0.3923 \t time=0.10s\n",
      "Epoch 27/50 \t loss=0.4054 \t val_loss=0.3899 \t time=0.09s\n",
      "Epoch 28/50 \t loss=0.4523 \t val_loss=0.4312 \t time=0.09s\n",
      "Epoch 29/50 \t loss=0.4452 \t val_loss=0.4162 \t time=0.11s\n",
      "Epoch 30/50 \t loss=0.5395 \t val_loss=0.4218 \t time=0.09s\n",
      "Epoch 31/50 \t loss=0.4845 \t val_loss=0.4160 \t time=0.09s\n",
      "Epoch 32/50 \t loss=0.4390 \t val_loss=0.4012 \t time=0.09s\n",
      "Epoch 33/50 \t loss=0.4589 \t val_loss=0.4420 \t time=0.08s\n",
      "Epoch 34/50 \t loss=0.3985 \t val_loss=0.4105 \t time=0.08s\n",
      "Epoch 35/50 \t loss=0.4492 \t val_loss=0.4455 \t time=0.11s\n",
      "Epoch 36/50 \t loss=0.4145 \t val_loss=0.4196 \t time=0.10s\n",
      "Epoch 37/50 \t loss=0.4112 \t val_loss=0.4230 \t time=0.10s\n",
      "Epoch 38/50 \t loss=0.4916 \t val_loss=0.4544 \t time=0.10s\n",
      "Epoch 39/50 \t loss=0.4011 \t val_loss=0.4118 \t time=0.10s\n",
      "Epoch 40/50 \t loss=0.4276 \t val_loss=0.3762 \t time=0.09s\n",
      "Epoch 41/50 \t loss=0.4368 \t val_loss=0.3872 \t time=0.09s\n",
      "Epoch 42/50 \t loss=0.3870 \t val_loss=0.4107 \t time=0.09s\n",
      "Epoch 43/50 \t loss=0.4717 \t val_loss=0.4130 \t time=0.09s\n",
      "Epoch 44/50 \t loss=0.4185 \t val_loss=0.4364 \t time=0.10s\n",
      "Epoch 45/50 \t loss=0.4761 \t val_loss=0.4082 \t time=0.09s\n",
      "Epoch 46/50 \t loss=0.3851 \t val_loss=0.3956 \t time=0.09s\n",
      "Epoch 47/50 \t loss=0.4397 \t val_loss=0.4346 \t time=0.09s\n",
      "Epoch 48/50 \t loss=0.4860 \t val_loss=0.4297 \t time=0.09s\n",
      "Epoch 49/50 \t loss=0.4823 \t val_loss=0.4521 \t time=0.09s\n",
      "Epoch 50/50 \t loss=0.4196 \t val_loss=0.4601 \t time=0.11s\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "2\n",
      "Fold 3\n",
      "Epoch 1/50 \t loss=0.6696 \t val_loss=0.6597 \t time=0.11s\n",
      "Epoch 2/50 \t loss=0.5495 \t val_loss=0.6071 \t time=0.10s\n",
      "Epoch 3/50 \t loss=0.5292 \t val_loss=0.6395 \t time=0.11s\n",
      "Epoch 4/50 \t loss=0.4796 \t val_loss=0.6440 \t time=0.12s\n",
      "Epoch 5/50 \t loss=0.4815 \t val_loss=0.6602 \t time=0.10s\n",
      "Epoch 6/50 \t loss=0.5151 \t val_loss=0.6358 \t time=0.09s\n",
      "Epoch 7/50 \t loss=0.4419 \t val_loss=0.6408 \t time=0.11s\n",
      "Epoch 8/50 \t loss=0.4597 \t val_loss=0.6668 \t time=0.09s\n",
      "Epoch 9/50 \t loss=0.4100 \t val_loss=0.6772 \t time=0.08s\n",
      "Epoch 10/50 \t loss=0.4204 \t val_loss=0.6632 \t time=0.11s\n",
      "Epoch 11/50 \t loss=0.4464 \t val_loss=0.6715 \t time=0.10s\n",
      "Epoch 12/50 \t loss=0.3962 \t val_loss=0.6701 \t time=0.09s\n",
      "Epoch 13/50 \t loss=0.4300 \t val_loss=0.7269 \t time=0.10s\n",
      "Epoch 14/50 \t loss=0.5130 \t val_loss=0.6534 \t time=0.09s\n",
      "Epoch 15/50 \t loss=0.4208 \t val_loss=0.6317 \t time=0.10s\n",
      "Epoch 16/50 \t loss=0.3779 \t val_loss=0.6519 \t time=0.11s\n",
      "Epoch 17/50 \t loss=0.3973 \t val_loss=0.6870 \t time=0.10s\n",
      "Epoch 18/50 \t loss=0.3408 \t val_loss=0.6420 \t time=0.09s\n",
      "Epoch 19/50 \t loss=0.3807 \t val_loss=0.6857 \t time=0.11s\n",
      "Epoch 20/50 \t loss=0.4720 \t val_loss=0.6227 \t time=0.10s\n",
      "Epoch 21/50 \t loss=0.4297 \t val_loss=0.6722 \t time=0.09s\n",
      "Epoch 22/50 \t loss=0.4958 \t val_loss=0.6524 \t time=0.10s\n",
      "Epoch 23/50 \t loss=0.3811 \t val_loss=0.6301 \t time=0.08s\n",
      "Epoch 24/50 \t loss=0.3669 \t val_loss=0.6702 \t time=0.09s\n",
      "Epoch 25/50 \t loss=0.3887 \t val_loss=0.7199 \t time=0.10s\n",
      "Epoch 26/50 \t loss=0.3752 \t val_loss=0.6471 \t time=0.09s\n",
      "Epoch 27/50 \t loss=0.3955 \t val_loss=0.6345 \t time=0.09s\n",
      "Epoch 28/50 \t loss=0.4734 \t val_loss=0.6354 \t time=0.11s\n",
      "Epoch 29/50 \t loss=0.3875 \t val_loss=0.7337 \t time=0.08s\n",
      "Epoch 30/50 \t loss=0.4079 \t val_loss=0.6681 \t time=0.09s\n",
      "Epoch 31/50 \t loss=0.3846 \t val_loss=0.7517 \t time=0.10s\n",
      "Epoch 32/50 \t loss=0.3892 \t val_loss=0.7062 \t time=0.10s\n",
      "Epoch 33/50 \t loss=0.4339 \t val_loss=0.7009 \t time=0.10s\n",
      "Epoch 34/50 \t loss=0.4605 \t val_loss=0.7320 \t time=0.10s\n",
      "Epoch 35/50 \t loss=0.3986 \t val_loss=0.7565 \t time=0.10s\n",
      "Epoch 36/50 \t loss=0.3704 \t val_loss=0.6856 \t time=0.11s\n",
      "Epoch 37/50 \t loss=0.3781 \t val_loss=0.7419 \t time=0.08s\n",
      "Epoch 38/50 \t loss=0.4187 \t val_loss=0.7513 \t time=0.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 \t loss=0.3913 \t val_loss=0.6761 \t time=0.10s\n",
      "Epoch 40/50 \t loss=0.4219 \t val_loss=0.6139 \t time=0.09s\n",
      "Epoch 41/50 \t loss=0.3914 \t val_loss=0.7047 \t time=0.08s\n",
      "Epoch 42/50 \t loss=0.4225 \t val_loss=0.6847 \t time=0.09s\n",
      "Epoch 43/50 \t loss=0.3558 \t val_loss=0.6972 \t time=0.09s\n",
      "Epoch 44/50 \t loss=0.3295 \t val_loss=0.7246 \t time=0.09s\n",
      "Epoch 45/50 \t loss=0.4583 \t val_loss=0.6038 \t time=0.08s\n",
      "Epoch 46/50 \t loss=0.4023 \t val_loss=0.5514 \t time=0.09s\n",
      "Epoch 47/50 \t loss=0.3837 \t val_loss=0.7223 \t time=0.10s\n",
      "Epoch 48/50 \t loss=0.3727 \t val_loss=0.6101 \t time=0.11s\n",
      "Epoch 49/50 \t loss=0.4034 \t val_loss=0.5957 \t time=0.10s\n",
      "Epoch 50/50 \t loss=0.2889 \t val_loss=0.5924 \t time=0.09s\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      "3\n",
      "Fold 4\n",
      "Epoch 1/50 \t loss=0.6970 \t val_loss=0.6257 \t time=0.12s\n",
      "Epoch 2/50 \t loss=0.5828 \t val_loss=0.5338 \t time=0.09s\n",
      "Epoch 3/50 \t loss=0.5688 \t val_loss=0.4849 \t time=0.09s\n",
      "Epoch 4/50 \t loss=0.5237 \t val_loss=0.4761 \t time=0.09s\n",
      "Epoch 5/50 \t loss=0.4936 \t val_loss=0.4778 \t time=0.09s\n",
      "Epoch 6/50 \t loss=0.4756 \t val_loss=0.4658 \t time=0.09s\n",
      "Epoch 7/50 \t loss=0.5234 \t val_loss=0.4637 \t time=0.09s\n",
      "Epoch 8/50 \t loss=0.5075 \t val_loss=0.4879 \t time=0.08s\n",
      "Epoch 9/50 \t loss=0.5391 \t val_loss=0.4825 \t time=0.09s\n",
      "Epoch 10/50 \t loss=0.4913 \t val_loss=0.4691 \t time=0.10s\n",
      "Epoch 11/50 \t loss=0.4904 \t val_loss=0.4600 \t time=0.09s\n",
      "Epoch 12/50 \t loss=0.4254 \t val_loss=0.4669 \t time=0.09s\n",
      "Epoch 13/50 \t loss=0.4090 \t val_loss=0.4953 \t time=0.11s\n",
      "Epoch 14/50 \t loss=0.3977 \t val_loss=0.5151 \t time=0.10s\n",
      "Epoch 15/50 \t loss=0.4805 \t val_loss=0.5251 \t time=0.11s\n",
      "Epoch 16/50 \t loss=0.4213 \t val_loss=0.5198 \t time=0.12s\n",
      "Epoch 17/50 \t loss=0.4166 \t val_loss=0.5651 \t time=0.14s\n",
      "Epoch 18/50 \t loss=0.4467 \t val_loss=0.5156 \t time=0.08s\n",
      "Epoch 19/50 \t loss=0.4061 \t val_loss=0.5145 \t time=0.08s\n",
      "Epoch 20/50 \t loss=0.4546 \t val_loss=0.5814 \t time=0.09s\n",
      "Epoch 21/50 \t loss=0.4085 \t val_loss=0.5688 \t time=0.08s\n",
      "Epoch 22/50 \t loss=0.4306 \t val_loss=0.5490 \t time=0.08s\n",
      "Epoch 23/50 \t loss=0.4145 \t val_loss=0.5845 \t time=0.09s\n",
      "Epoch 24/50 \t loss=0.5108 \t val_loss=0.6999 \t time=0.08s\n",
      "Epoch 25/50 \t loss=0.4401 \t val_loss=0.5705 \t time=0.08s\n",
      "Epoch 26/50 \t loss=0.4782 \t val_loss=0.5798 \t time=0.10s\n",
      "Epoch 27/50 \t loss=0.4251 \t val_loss=0.6150 \t time=0.09s\n",
      "Epoch 28/50 \t loss=0.4540 \t val_loss=0.6230 \t time=0.08s\n",
      "Epoch 29/50 \t loss=0.3318 \t val_loss=0.6392 \t time=0.11s\n",
      "Epoch 30/50 \t loss=0.3601 \t val_loss=0.6577 \t time=0.10s\n",
      "Epoch 31/50 \t loss=0.4053 \t val_loss=0.5642 \t time=0.09s\n",
      "Epoch 32/50 \t loss=0.3876 \t val_loss=0.5836 \t time=0.10s\n",
      "Epoch 33/50 \t loss=0.3882 \t val_loss=0.6172 \t time=0.14s\n",
      "Epoch 34/50 \t loss=0.3898 \t val_loss=0.6168 \t time=0.11s\n",
      "Epoch 35/50 \t loss=0.4275 \t val_loss=0.5620 \t time=0.10s\n",
      "Epoch 36/50 \t loss=0.3757 \t val_loss=0.5368 \t time=0.09s\n",
      "Epoch 37/50 \t loss=0.4768 \t val_loss=0.5898 \t time=0.10s\n",
      "Epoch 38/50 \t loss=0.4593 \t val_loss=0.5248 \t time=0.08s\n",
      "Epoch 39/50 \t loss=0.4393 \t val_loss=0.5323 \t time=0.09s\n",
      "Epoch 40/50 \t loss=0.3706 \t val_loss=0.4879 \t time=0.12s\n",
      "Epoch 41/50 \t loss=0.4076 \t val_loss=0.5658 \t time=0.09s\n",
      "Epoch 42/50 \t loss=0.3897 \t val_loss=0.5559 \t time=0.09s\n",
      "Epoch 43/50 \t loss=0.4177 \t val_loss=0.5753 \t time=0.10s\n",
      "Epoch 44/50 \t loss=0.4197 \t val_loss=0.5440 \t time=0.10s\n",
      "Epoch 45/50 \t loss=0.3991 \t val_loss=0.5692 \t time=0.10s\n",
      "Epoch 46/50 \t loss=0.3988 \t val_loss=0.6210 \t time=0.11s\n",
      "Epoch 47/50 \t loss=0.3851 \t val_loss=0.6154 \t time=0.10s\n",
      "Epoch 48/50 \t loss=0.3210 \t val_loss=0.6278 \t time=0.10s\n",
      "Epoch 49/50 \t loss=0.3920 \t val_loss=0.7006 \t time=0.10s\n",
      "Epoch 50/50 \t loss=0.4209 \t val_loss=0.7224 \t time=0.10s\n",
      "[0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
      "4\n",
      "Fold 5\n",
      "Epoch 1/50 \t loss=0.6981 \t val_loss=0.6445 \t time=0.10s\n",
      "Epoch 2/50 \t loss=0.6109 \t val_loss=0.5603 \t time=0.09s\n",
      "Epoch 3/50 \t loss=0.5799 \t val_loss=0.5372 \t time=0.10s\n",
      "Epoch 4/50 \t loss=0.5313 \t val_loss=0.5159 \t time=0.10s\n",
      "Epoch 5/50 \t loss=0.5795 \t val_loss=0.5198 \t time=0.10s\n",
      "Epoch 6/50 \t loss=0.5251 \t val_loss=0.5056 \t time=0.10s\n",
      "Epoch 7/50 \t loss=0.5019 \t val_loss=0.5161 \t time=0.10s\n",
      "Epoch 8/50 \t loss=0.4750 \t val_loss=0.5130 \t time=0.09s\n",
      "Epoch 9/50 \t loss=0.4451 \t val_loss=0.5066 \t time=0.10s\n",
      "Epoch 10/50 \t loss=0.4814 \t val_loss=0.4933 \t time=0.11s\n",
      "Epoch 11/50 \t loss=0.4696 \t val_loss=0.5043 \t time=0.11s\n",
      "Epoch 12/50 \t loss=0.4864 \t val_loss=0.5250 \t time=0.10s\n",
      "Epoch 13/50 \t loss=0.4707 \t val_loss=0.5187 \t time=0.11s\n",
      "Epoch 14/50 \t loss=0.5582 \t val_loss=0.5112 \t time=0.09s\n",
      "Epoch 15/50 \t loss=0.4153 \t val_loss=0.5249 \t time=0.10s\n",
      "Epoch 16/50 \t loss=0.4239 \t val_loss=0.5048 \t time=0.09s\n",
      "Epoch 17/50 \t loss=0.4683 \t val_loss=0.5334 \t time=0.10s\n",
      "Epoch 18/50 \t loss=0.4188 \t val_loss=0.5051 \t time=0.10s\n",
      "Epoch 19/50 \t loss=0.4233 \t val_loss=0.5400 \t time=0.09s\n",
      "Epoch 20/50 \t loss=0.4269 \t val_loss=0.5074 \t time=0.10s\n",
      "Epoch 21/50 \t loss=0.5037 \t val_loss=0.5121 \t time=0.10s\n",
      "Epoch 22/50 \t loss=0.4141 \t val_loss=0.5019 \t time=0.09s\n",
      "Epoch 23/50 \t loss=0.4060 \t val_loss=0.5147 \t time=0.11s\n",
      "Epoch 24/50 \t loss=0.3605 \t val_loss=0.5180 \t time=0.11s\n",
      "Epoch 25/50 \t loss=0.4575 \t val_loss=0.4936 \t time=0.12s\n",
      "Epoch 26/50 \t loss=0.4409 \t val_loss=0.4760 \t time=0.11s\n",
      "Epoch 27/50 \t loss=0.4280 \t val_loss=0.4802 \t time=0.10s\n",
      "Epoch 28/50 \t loss=0.5021 \t val_loss=0.4906 \t time=0.10s\n",
      "Epoch 29/50 \t loss=0.3967 \t val_loss=0.4974 \t time=0.10s\n",
      "Epoch 30/50 \t loss=0.4360 \t val_loss=0.5057 \t time=0.10s\n",
      "Epoch 31/50 \t loss=0.4330 \t val_loss=0.5190 \t time=0.10s\n",
      "Epoch 32/50 \t loss=0.4373 \t val_loss=0.5103 \t time=0.10s\n",
      "Epoch 33/50 \t loss=0.4249 \t val_loss=0.4897 \t time=0.11s\n",
      "Epoch 34/50 \t loss=0.4704 \t val_loss=0.5106 \t time=0.09s\n",
      "Epoch 35/50 \t loss=0.4579 \t val_loss=0.5148 \t time=0.09s\n",
      "Epoch 36/50 \t loss=0.4162 \t val_loss=0.5295 \t time=0.11s\n",
      "Epoch 37/50 \t loss=0.4598 \t val_loss=0.4947 \t time=0.10s\n",
      "Epoch 38/50 \t loss=0.4556 \t val_loss=0.5154 \t time=0.10s\n",
      "Epoch 39/50 \t loss=0.3593 \t val_loss=0.4801 \t time=0.12s\n",
      "Epoch 40/50 \t loss=0.3832 \t val_loss=0.4905 \t time=0.11s\n",
      "Epoch 41/50 \t loss=0.4539 \t val_loss=0.5104 \t time=0.11s\n",
      "Epoch 42/50 \t loss=0.4621 \t val_loss=0.5125 \t time=0.11s\n",
      "Epoch 43/50 \t loss=0.4233 \t val_loss=0.5212 \t time=0.10s\n",
      "Epoch 44/50 \t loss=0.4406 \t val_loss=0.5149 \t time=0.09s\n",
      "Epoch 45/50 \t loss=0.4058 \t val_loss=0.5097 \t time=0.09s\n",
      "Epoch 46/50 \t loss=0.3410 \t val_loss=0.5357 \t time=0.10s\n",
      "Epoch 47/50 \t loss=0.3542 \t val_loss=0.5100 \t time=0.09s\n",
      "Epoch 48/50 \t loss=0.4251 \t val_loss=0.5175 \t time=0.09s\n",
      "Epoch 49/50 \t loss=0.4274 \t val_loss=0.5663 \t time=0.10s\n",
      "Epoch 50/50 \t loss=0.3583 \t val_loss=0.5508 \t time=0.10s\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[[24 24]\n",
      " [21 99]]\n",
      "Accuracy :  0.7321428571428571\n",
      "Sensitivity :  0.5\n",
      "Specificity :  0.825\n",
      "AUC:  0.6625\n",
      "\n",
      "\n",
      "All \t loss=0.3665 \t val_loss=0.6061 \t auc=0.6625\n"
     ]
    }
   ],
   "source": [
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "    \n",
    "    ## ???\n",
    "    x_train_fold = torch.tensor(trn_X_pd[train_idx.astype(int)], dtype=torch.float)   # use_cuse?\n",
    "    y_train_fold = torch.tensor(trn_y_pd[train_idx.astype(int), np.newaxis], dtype=torch.float)    \n",
    "\n",
    "    x_val_fold = torch.tensor(trn_X_pd[valid_idx.astype(int)], dtype=torch.float)\n",
    "    y_val_fold = torch.tensor(trn_y_pd[valid_idx.astype(int), np.newaxis], dtype=torch.float)  \n",
    "    \n",
    "################################################################################    \n",
    "    trn_X = torch.from_numpy(trn_X_pd[train_idx.astype(int)].astype(float))\n",
    "    trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\n",
    "    \n",
    "    # Train\n",
    "    trn = Dataset(trn_X, trn_y)\n",
    "    trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True, drop_last = True)  # True or False\n",
    "    trn_test_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=False, drop_last = False)  # True or False\n",
    "    \n",
    "    # Valid\n",
    "    valid_X = torch.from_numpy(trn_X_pd[valid_idx.astype(int)].astype(float))\n",
    "    valid_y = torch.from_numpy(trn_y_pd[valid_idx.astype(int)].astype(float))\n",
    "    \n",
    "    valid = Dataset(valid_X, valid_y)\n",
    "    valid_loader = data_utils.DataLoader(valid, batch_size=batch_size, shuffle=False, drop_last = False) # True or False\n",
    "    \n",
    "    # Test\n",
    "    val_X = torch.from_numpy(val_X_pd.astype(float))\n",
    "    val_y = torch.from_numpy(val_y_pd.astype(float))\n",
    "    val = Dataset(val_X, val_y)\n",
    "    test_loader = data_utils.DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "   ################################################################################    \n",
    "    print(i)\n",
    "    \n",
    "    ## Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    ## After just second, will modify optimizer(weight_decay,step_size, base_lr, max_lr,scheduler)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-5)\n",
    "    \n",
    "    model = DNN_seq_1()\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    num_epochs = 50\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    ##############################################################################\n",
    "    step_size = 2000\n",
    "    base_lr, max_lr = 0.001, 0.01  \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode='exp_range', gamma=0.99994)    \n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    print(f'Fold {i+1}')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        #correct = 0.   # Accuracy\n",
    "        \n",
    "        for batch_idx, trn in enumerate(trn_loader):\n",
    "            trn_X, trn_y = trn['X'], trn['y']\n",
    "            if use_cuda:\n",
    "                trn_X, trn_y = trn_X.cuda(), trn_y.cuda()\n",
    "            trn_X, trn_y = Variable(trn_X).float(), Variable(trn_y).float()\n",
    "            optimizer.zero_grad()\n",
    "            trn_pred = model(trn_X)\n",
    "\n",
    "            \n",
    "            if scheduler:\n",
    "                #print('cycle_LR')\n",
    "                scheduler.batch_step()\n",
    "            #print(trn_pred.squeeze())\n",
    "            #print(trn_y)\n",
    "            trn_loss = criterion(trn_pred.squeeze(), trn_y)\n",
    "            trn_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            avg_loss += trn_loss.item()/len(trn_loader)\n",
    "        \n",
    "            #predicted = torch.max(trn_pred.data, 1)[1] \n",
    "            #correct += (predicted == trn_y).sum()\n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds_fold = np.zeros((valid_X.size(0)))\n",
    "        test_preds_fold = np.zeros(val_X_pd.shape[0])  # Test\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, val in enumerate(valid_loader):\n",
    "                val_X, val_y = val['X'], val['y']\n",
    "                if use_cuda:\n",
    "                    val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "                val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "                optimizer.zero_grad()\n",
    "                val_pred = model(val_X).detach()\n",
    "\n",
    "            \n",
    "                val_loss = criterion(val_pred.squeeze(), val_y)\n",
    "        \n",
    "                avg_val_loss += val_loss.item()/len(valid_loader)\n",
    "            \n",
    "            \n",
    "                #val_pred = torch.max(val_pred, 1)[1]\n",
    "                val_pred = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "                #print(val_pred.cpu().numpy())\n",
    "                #print(val_pred.cpu().numpy()[:,0])\n",
    "            \n",
    "                valid_preds_fold[batch_idx * batch_size:(batch_idx+1) * batch_size] = (val_pred.cpu().numpy())    # modified [:,0]\n",
    "            \n",
    "            #valid_preds_fold_2 = \n",
    "                # Loss function chage -> plus Sigmoid\n",
    "            elapsed_time = time.time() - start_time \n",
    "            print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(epoch + 1, num_epochs, avg_loss, avg_val_loss, elapsed_time))            \n",
    "\n",
    "# Test        \n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss) \n",
    "    \n",
    "    for batch_idx, test in enumerate(test_loader):\n",
    "        test_X, test_y = test['X'], test['y']\n",
    "        if use_cuda:\n",
    "            test_X, test_y = test_X.cuda(), test_y.cuda()\n",
    "        test_X, test_y = Variable(test_X).float(), Variable(test_y).float()        \n",
    "        test_pred = model(test_X).detach()\n",
    "        \n",
    "        \n",
    "        test_pred = (test_pred > 0.5).flatten().type(torch.ByteTensor)\n",
    "        \n",
    "        #print(test_pred)\n",
    "        #print(test_pred.cpu().numpy())\n",
    "        test_preds_fold[batch_idx * batch_size:(batch_idx+1) * batch_size] = (test_pred.cpu().numpy())   # modified [:,0]\n",
    "        \n",
    "        \n",
    "    train_preds[valid_idx.astype(int)] = valid_preds_fold\n",
    "    print(valid_preds_fold)\n",
    "    print(trn_y_pd[valid_idx.astype(int)])\n",
    "    train_y_sort[valid_idx.astype(int)] = trn_y_pd[valid_idx.astype(int)]\n",
    "    test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "#predict_ = pd.DataFrame(predict)\n",
    "\n",
    "#predict_.iloc[:,0]\n",
    "\n",
    "#label = pd.DataFrame(val_y_pd)\n",
    "\n",
    "#test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)    \n",
    "    \n",
    "print(trn_y_pd)\n",
    "print(train_preds)\n",
    "\n",
    "auc  =  round(roc_auc_score(train_y_sort,train_preds.astype(int)),4)\n",
    "\n",
    "\n",
    "cnf = confusion_matrix(train_y_sort, train_preds, labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "print('AUC: ', auc)\n",
    "\n",
    "\n",
    "#fpr, trp, _  =  roc_auc_score(train_y_sort,train_preds.astype(int))\n",
    "\n",
    "#fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "#print(type(fpr))\n",
    "#print(fpr)\n",
    "\n",
    "#auc = auc(fpr, trp)\n",
    "#print('AUC: ', auc(fpr, trp))\n",
    "print('\\n')\n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 1., 1., 0., 0.])\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 0., 0., 0.])\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 1., 1., 0., 0.])\n",
      "tensor([1, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 1., 0., 0.])\n",
      "tensor([0, 0, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 0.])\n",
      "******************** Test ********************\n",
      "Loss: 0.30895769596099854, Accuracy: 0.7142857313156128 %\n",
      "**********************************************\n",
      "Val accuracy:0.714\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(valid_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        #print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        print(val_y)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(valid_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(valid_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6.080831e-15</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.896444e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.800919e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.123364e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.831288e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.254827e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.801564e-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.484800e-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.731060e-06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.216990e-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.220439e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.540676e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.588997e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.607370e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.403692e-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.574064e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.575955e-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.838596e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.311082e-08</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.511023e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.193784e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.485059e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.813606e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.126032e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.784010e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.798709e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.135942e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.926586e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.488538e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.791064e-09</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.446221e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.317819e-13</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.984026e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0                \n",
       "6.080831e-15  0.0\n",
       "3.896444e-01  0.0\n",
       "1.800919e-01  0.0\n",
       "1.123364e-01  0.0\n",
       "4.831288e-01  0.0\n",
       "2.254827e-01  1.0\n",
       "2.801564e-02  1.0\n",
       "7.484800e-02  1.0\n",
       "3.731060e-06  0.0\n",
       "5.216990e-03  0.0\n",
       "2.220439e-01  1.0\n",
       "5.540676e-05  0.0\n",
       "6.588997e-05  0.0\n",
       "8.607370e-01  0.0\n",
       "1.403692e-03  0.0\n",
       "7.574064e-02  0.0\n",
       "1.575955e-03  0.0\n",
       "8.838596e-01  0.0\n",
       "1.311082e-08  0.0\n",
       "1.511023e-05  0.0\n",
       "8.193784e-01  1.0\n",
       "5.485059e-01  1.0\n",
       "9.813606e-01  1.0\n",
       "6.126032e-07  0.0\n",
       "1.784010e-02  0.0\n",
       "9.798709e-01  1.0\n",
       "9.135942e-01  0.0\n",
       "7.926586e-01  1.0\n",
       "2.488538e-01  0.0\n",
       "4.791064e-09  0.0\n",
       "1.446221e-02  0.0\n",
       "3.317819e-13  0.0\n",
       "5.984026e-01  0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "pd.DataFrame(trn_y_pd[valid_idx.astype(int)], predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  4]\n",
      " [ 4 20]]\n",
      "Accuracy :  0.7575757575757576\n",
      "Sensitivity :  0.5555555555555556\n",
      "Specificity :  0.8333333333333334\n",
      "AUC:  0.6944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(trn_y_pd[valid_idx.astype(int)])\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "#fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "auc  =  round(roc_auc_score(test_p['label'],test_p['predicted_prob']),4)\n",
    "#print(fpr)\n",
    "#print(trp)\n",
    "#print(fpr)\n",
    "print('AUC: ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN_seq_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN_seq_1(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=150, bias=True)\n",
       "    (1): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (classifier2): Sequential(\n",
       "    (0): Linear(in_features=150, out_features=250, bias=True)\n",
       "    (1): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (output_layer): Sequential(\n",
       "    (0): Linear(in_features=250, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = TheModelClass(*args, **kwargs)                                                                                                     \n",
    "#optimizer = TheOptimizerClass(*args, **kwargs)                                                                                             \n",
    "checkpoint = torch.load(\"./platin_model_save/platin_model_300_100_100_model_1_150_250.pth\")                                                                                  \n",
    "model.load_state_dict(checkpoint['model_state_dict'])                                                                                         \n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])                                                                               \n",
    "#num_epochs = checkpoint['epoch']                                                                                                           \n",
    "loss = checkpoint['loss']                                                                                                                   \n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 1., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 1., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 1.])\n",
      "tensor([0, 0], dtype=torch.uint8)\n",
      "tensor([1., 1.])\n",
      "******************** Test ********************\n",
      "Loss: 0.7104251384735107, Accuracy: 0.6222222447395325 %\n",
      "**********************************************\n",
      "Val accuracy:0.622\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        #print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        print(val_y)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.493109</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.493315</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.490894</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.493182</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492600</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491756</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492351</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491634</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491395</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.488604</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491862</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491660</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.489704</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491839</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491603</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491365</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.493294</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492448</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492529</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492047</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491866</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492224</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491159</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.489942</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.489399</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.490230</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.488804</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.493695</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.490675</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.490661</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492573</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.493133</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.487380</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.490474</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492627</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.495118</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.489241</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.487941</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.489881</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.490638</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492586</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.490287</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0            \n",
       "0.493109  0.0\n",
       "0.493315  1.0\n",
       "0.490894  0.0\n",
       "0.493182  0.0\n",
       "0.492600  1.0\n",
       "0.491756  0.0\n",
       "0.492351  1.0\n",
       "0.491634  0.0\n",
       "0.491395  0.0\n",
       "0.488604  0.0\n",
       "0.491862  0.0\n",
       "0.491660  0.0\n",
       "0.489704  1.0\n",
       "0.491839  0.0\n",
       "0.491603  0.0\n",
       "0.491365  1.0\n",
       "0.493294  0.0\n",
       "0.492448  1.0\n",
       "0.492529  0.0\n",
       "0.492047  0.0\n",
       "0.491866  0.0\n",
       "0.492224  1.0\n",
       "0.491159  0.0\n",
       "0.489942  1.0\n",
       "0.489399  0.0\n",
       "0.490230  0.0\n",
       "0.488804  0.0\n",
       "0.493695  0.0\n",
       "0.490675  0.0\n",
       "0.490661  1.0\n",
       "0.492573  0.0\n",
       "0.493133  1.0\n",
       "0.487380  0.0\n",
       "0.490474  0.0\n",
       "0.492627  0.0\n",
       "0.495118  0.0\n",
       "0.489241  0.0\n",
       "0.487941  1.0\n",
       "0.489881  0.0\n",
       "0.490638  1.0\n",
       "0.492586  1.0\n",
       "0.490287  1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "pd.DataFrame(val_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 14]\n",
      " [ 0 28]]\n",
      "Accuracy :  0.6666666666666666\n",
      "Sensitivity :  0.0\n",
      "Specificity :  1.0\n",
      "<class 'numpy.ndarray'>\n",
      "AUC:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(val_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print(type(fpr))\n",
    "#print(fpr)\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "#'epoch': EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': trn_loss\n",
    "    }, \"./platin_model_save/platin_model_300_100_100_model_1_150_250___.pth\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test (Except Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4910],\n",
      "        [0.4930],\n",
      "        [0.4933],\n",
      "        [0.4881],\n",
      "        [0.4925]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4888],\n",
      "        [0.4965],\n",
      "        [0.4882],\n",
      "        [0.4894],\n",
      "        [0.4916]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4920],\n",
      "        [0.4901],\n",
      "        [0.4933],\n",
      "        [0.4892],\n",
      "        [0.4921]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4914],\n",
      "        [0.4929],\n",
      "        [0.4918],\n",
      "        [0.4873],\n",
      "        [0.4906]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4916],\n",
      "        [0.4917],\n",
      "        [0.4909],\n",
      "        [0.4888],\n",
      "        [0.4924]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4891],\n",
      "        [0.4934],\n",
      "        [0.4927],\n",
      "        [0.4933],\n",
      "        [0.4929]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4911],\n",
      "        [0.4915],\n",
      "        [0.4918],\n",
      "        [0.4935],\n",
      "        [0.4904]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4920],\n",
      "        [0.4916],\n",
      "        [0.4919],\n",
      "        [0.4920],\n",
      "        [0.4924]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4898],\n",
      "        [0.4904],\n",
      "        [0.4917],\n",
      "        [0.4928],\n",
      "        [0.4923]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4922],\n",
      "        [0.4912],\n",
      "        [0.4913],\n",
      "        [0.4892],\n",
      "        [0.4899]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4926],\n",
      "        [0.4911],\n",
      "        [0.4921],\n",
      "        [0.4926],\n",
      "        [0.4889]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4937],\n",
      "        [0.4902],\n",
      "        [0.4904],\n",
      "        [0.4921],\n",
      "        [0.4926]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4931],\n",
      "        [0.4885],\n",
      "        [0.4913],\n",
      "        [0.4939],\n",
      "        [0.4919]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4900],\n",
      "        [0.4895],\n",
      "        [0.4916],\n",
      "        [0.4903],\n",
      "        [0.4918]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4923],\n",
      "        [0.4915],\n",
      "        [0.4911],\n",
      "        [0.4915],\n",
      "        [0.4882]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4926],\n",
      "        [0.4909],\n",
      "        [0.4915],\n",
      "        [0.4896],\n",
      "        [0.4892]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4926],\n",
      "        [0.4905],\n",
      "        [0.4916],\n",
      "        [0.4929],\n",
      "        [0.4893]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4906],\n",
      "        [0.4926],\n",
      "        [0.4918],\n",
      "        [0.4934],\n",
      "        [0.4926]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4928],\n",
      "        [0.4921],\n",
      "        [0.4905],\n",
      "        [0.4903],\n",
      "        [0.4933]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4930],\n",
      "        [0.4894],\n",
      "        [0.4923],\n",
      "        [0.4890],\n",
      "        [0.4926]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4947],\n",
      "        [0.4921],\n",
      "        [0.4912],\n",
      "        [0.4926],\n",
      "        [0.4924]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4936],\n",
      "        [0.4900],\n",
      "        [0.4923],\n",
      "        [0.4919],\n",
      "        [0.4913]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4926],\n",
      "        [0.4929],\n",
      "        [0.4922],\n",
      "        [0.4891],\n",
      "        [0.4901]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4902],\n",
      "        [0.4897],\n",
      "        [0.4911],\n",
      "        [0.4904],\n",
      "        [0.4902]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4914],\n",
      "        [0.4900],\n",
      "        [0.4900],\n",
      "        [0.4907],\n",
      "        [0.4917]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4910],\n",
      "        [0.4916],\n",
      "        [0.4932],\n",
      "        [0.4948],\n",
      "        [0.4910]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4921],\n",
      "        [0.4925],\n",
      "        [0.4919],\n",
      "        [0.4917],\n",
      "        [0.4921]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "******************** Test ********************\n",
      "Loss: 0.6837326288223267, Accuracy: 0.7185184955596924 %\n",
      "**********************************************\n",
      "Val accuracy:0.719\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(trn_test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(trn_test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(trn_test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.490963</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492982</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.493317</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.488129</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492504</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492123</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492540</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491913</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491727</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.492095</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0            \n",
       "0.490963  1.0\n",
       "0.492982  0.0\n",
       "0.493317  0.0\n",
       "0.488129  0.0\n",
       "0.492504  0.0\n",
       "...       ...\n",
       "0.492123  1.0\n",
       "0.492540  0.0\n",
       "0.491913  0.0\n",
       "0.491727  0.0\n",
       "0.492095  0.0\n",
       "\n",
       "[135 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "#trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\n",
    "pd.DataFrame(trn_y_pd[train_idx.astype(int)], predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38  0]\n",
      " [97  0]]\n",
      "Accuracy :  0.2814814814814815\n",
      "Sensitivity :  1.0\n",
      "Specificity :  0.0\n",
      "AUC:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(trn_y_pd[train_idx.astype(int)])\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ........................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Train stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ........................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (Total Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X = torch.from_numpy(trn_X_pd.astype(float))\n",
    "trn_y = torch.from_numpy(trn_y_pd.astype(float))\n",
    "\n",
    "val_X = torch.from_numpy(val_X_pd.astype(float))\n",
    "val_y = torch.from_numpy(val_y_pd.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = Dataset(trn_X, trn_y)\n",
    "trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "trn_test_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "val = Dataset(val_X, val_y)\n",
    "val_loader = data_utils.DataLoader(val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 1e-3\n",
    "    \n",
    "## After just second, will modify optimizer(weight_decay,step_size, base_lr, max_lr,scheduler)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-5)\n",
    "    \n",
    "model = DNN_seq_2()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "num_epochs = 90\n",
    "    \n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "##############################################################################\n",
    "step_size = 2000\n",
    "base_lr, max_lr = 0.001, 0.01  \n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode='exp_range', gamma=0.99994)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 1.0370371341705322, Accuracy: 0.49696969985961914 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.4805886149406433, Accuracy: 0.539393961429596 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.7496358156204224, Accuracy: 0.6121212244033813 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.8888704180717468, Accuracy: 0.5696969628334045 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.4830182194709778, Accuracy: 0.6545454263687134 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 1.032348394393921, Accuracy: 0.6242424249649048 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.5486370325088501, Accuracy: 0.6727272868156433 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.46035242080688477, Accuracy: 0.6727272868156433 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.7438788414001465, Accuracy: 0.6727272868156433 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.6092754602432251, Accuracy: 0.6969696879386902 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.5012811422348022, Accuracy: 0.6909090876579285 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.3995819687843323, Accuracy: 0.7818182110786438 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.8750421404838562, Accuracy: 0.7333333492279053 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.4373825192451477, Accuracy: 0.7454545497894287 % \t time=0.36s\n",
      "******************** Train ********************\n",
      "Loss: 0.6508690118789673, Accuracy: 0.7515151500701904 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.4219430387020111, Accuracy: 0.7515151500701904 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.36383986473083496, Accuracy: 0.7454545497894287 % \t time=0.35s\n",
      "******************** Train ********************\n",
      "Loss: 0.3578457832336426, Accuracy: 0.7696969509124756 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.3863140940666199, Accuracy: 0.739393949508667 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.6617411375045776, Accuracy: 0.7696969509124756 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.38849326968193054, Accuracy: 0.7272727489471436 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.5601524710655212, Accuracy: 0.7696969509124756 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.2773747742176056, Accuracy: 0.7575757503509521 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.42588502168655396, Accuracy: 0.7696969509124756 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.40322479605674744, Accuracy: 0.7272727489471436 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.4128304123878479, Accuracy: 0.7636363506317139 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.6717929244041443, Accuracy: 0.7878788113594055 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.2722536325454712, Accuracy: 0.7636363506317139 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.39806389808654785, Accuracy: 0.6909090876579285 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.39339834451675415, Accuracy: 0.7030302882194519 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.3872677683830261, Accuracy: 0.739393949508667 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.3984304666519165, Accuracy: 0.7636363506317139 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.9852256774902344, Accuracy: 0.7878788113594055 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.4908868670463562, Accuracy: 0.7575757503509521 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.4120384752750397, Accuracy: 0.7757575511932373 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.371670663356781, Accuracy: 0.7454545497894287 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.3971256911754608, Accuracy: 0.7818182110786438 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.22568464279174805, Accuracy: 0.739393949508667 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.4451514184474945, Accuracy: 0.7878788113594055 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.25718721747398376, Accuracy: 0.7818182110786438 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.23656022548675537, Accuracy: 0.7636363506317139 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.42449814081192017, Accuracy: 0.7696969509124756 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.4009465277194977, Accuracy: 0.8121212124824524 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.23613576591014862, Accuracy: 0.800000011920929 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.30044227838516235, Accuracy: 0.7454545497894287 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.2707666754722595, Accuracy: 0.7757575511932373 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.34146925806999207, Accuracy: 0.8181818127632141 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.2843658924102783, Accuracy: 0.7636363506317139 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.22939610481262207, Accuracy: 0.7757575511932373 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.6624618768692017, Accuracy: 0.8060606122016907 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.1885232776403427, Accuracy: 0.7818182110786438 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.7053525447845459, Accuracy: 0.7636363506317139 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.20454922318458557, Accuracy: 0.8060606122016907 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.43391507863998413, Accuracy: 0.7818182110786438 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.17569522559642792, Accuracy: 0.7454545497894287 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.7638249397277832, Accuracy: 0.7575757503509521 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.40020951628685, Accuracy: 0.7939394116401672 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 1.0548434257507324, Accuracy: 0.7636363506317139 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.5061674118041992, Accuracy: 0.7878788113594055 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.5741032361984253, Accuracy: 0.7878788113594055 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.331132709980011, Accuracy: 0.8121212124824524 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.4184151589870453, Accuracy: 0.7575757503509521 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.18485507369041443, Accuracy: 0.8181818127632141 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.39683249592781067, Accuracy: 0.7636363506317139 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.34929656982421875, Accuracy: 0.7454545497894287 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.32119911909103394, Accuracy: 0.7333333492279053 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.2385237216949463, Accuracy: 0.8242424130439758 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.6356026530265808, Accuracy: 0.7696969509124756 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.19815996289253235, Accuracy: 0.8060606122016907 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.3106878697872162, Accuracy: 0.7878788113594055 % \t time=0.29s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 0.4314555525779724, Accuracy: 0.7636363506317139 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.6116787791252136, Accuracy: 0.7272727489471436 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.21853002905845642, Accuracy: 0.7515151500701904 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.43295758962631226, Accuracy: 0.8060606122016907 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.2347404509782791, Accuracy: 0.8363636136054993 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.18524721264839172, Accuracy: 0.8181818127632141 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.3621549606323242, Accuracy: 0.8121212124824524 % \t time=0.35s\n",
      "******************** Train ********************\n",
      "Loss: 0.2180204689502716, Accuracy: 0.7696969509124756 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.7717263698577881, Accuracy: 0.842424213886261 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.6814755201339722, Accuracy: 0.8121212124824524 % \t time=0.33s\n",
      "******************** Train ********************\n",
      "Loss: 0.320480078458786, Accuracy: 0.800000011920929 % \t time=0.36s\n",
      "******************** Train ********************\n",
      "Loss: 0.44986486434936523, Accuracy: 0.8242424130439758 % \t time=0.36s\n",
      "******************** Train ********************\n",
      "Loss: 0.9849674105644226, Accuracy: 0.7939394116401672 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.2842985987663269, Accuracy: 0.7878788113594055 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.2755371034145355, Accuracy: 0.8363636136054993 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.13689765334129333, Accuracy: 0.8303030133247375 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.4957852363586426, Accuracy: 0.8242424130439758 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.47853702306747437, Accuracy: 0.7757575511932373 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.12325544655323029, Accuracy: 0.7878788113594055 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.4983125627040863, Accuracy: 0.7757575511932373 % \t time=0.29s\n",
      "**********************************************\n",
      "Trn accuracy:0.776 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    #correct = 0.   # Accuracy\n",
    "    correct_val = 0.\n",
    "        \n",
    "    for batch_idx, trn in enumerate(trn_loader):\n",
    "        trn_X, trn_y = trn['X'], trn['y']\n",
    "        if use_cuda:\n",
    "            trn_X, trn_y = trn_X.cuda(), trn_y.cuda()\n",
    "        trn_X, trn_y = Variable(trn_X).float(), Variable(trn_y).float()\n",
    "        optimizer.zero_grad()\n",
    "        trn_pred = model(trn_X)\n",
    "        for i in trn_pred:\n",
    "            predict.append(i.detach().numpy())\n",
    "            \n",
    "        if scheduler:\n",
    "            #print('cycle_LR')\n",
    "            scheduler.batch_step()\n",
    "        #print(trn_pred.squeeze())\n",
    "        #print(trn_y)\n",
    "        trn_loss = criterion(trn_pred.squeeze(), trn_y)\n",
    "        trn_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += trn_loss.item()/len(trn_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        predicted = (trn_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        correct_val += (predicted == trn_y).sum()\n",
    "    print('*'*20, 'Train', '*'*20)\n",
    "    print('Loss: {}, Accuracy: {} % \\t time={:.2f}s'.format(trn_loss.item(), correct_val/(len(trn_loader)*batch_size), elapsed_time))\n",
    "print('*'*46)\n",
    "print(\"Trn accuracy:{:.3f} \".format(float(correct_val) / (len(trn_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3974],\n",
      "        [0.1855],\n",
      "        [0.2668],\n",
      "        [0.0728],\n",
      "        [0.2247]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.0610],\n",
      "        [0.4087],\n",
      "        [0.4424],\n",
      "        [0.1793],\n",
      "        [0.2813]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.1072],\n",
      "        [0.0815],\n",
      "        [0.4435],\n",
      "        [0.1815],\n",
      "        [0.1593]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.0377],\n",
      "        [0.0361],\n",
      "        [0.0371],\n",
      "        [0.3779],\n",
      "        [0.1261]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.5739],\n",
      "        [0.1788],\n",
      "        [0.1742],\n",
      "        [0.2784],\n",
      "        [0.1174]])\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.2394],\n",
      "        [0.5477],\n",
      "        [0.2490],\n",
      "        [0.0290],\n",
      "        [0.1413]])\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.2047],\n",
      "        [0.2901],\n",
      "        [0.1648],\n",
      "        [0.2002],\n",
      "        [0.4745]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.2104],\n",
      "        [0.2634],\n",
      "        [0.1128],\n",
      "        [0.1908],\n",
      "        [0.0549]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.1074],\n",
      "        [0.0919],\n",
      "        [0.4994],\n",
      "        [0.1457],\n",
      "        [0.4507]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.5549],\n",
      "        [0.6421],\n",
      "        [0.1049],\n",
      "        [0.1469],\n",
      "        [0.1236]])\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4193],\n",
      "        [0.0827],\n",
      "        [0.0497],\n",
      "        [0.2247],\n",
      "        [0.1747]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.3545],\n",
      "        [0.5426],\n",
      "        [0.1279],\n",
      "        [0.2431],\n",
      "        [0.5309]])\n",
      "tensor([0, 1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([[0.2912],\n",
      "        [0.2431],\n",
      "        [0.1903],\n",
      "        [0.3563],\n",
      "        [0.0596]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.5201],\n",
      "        [0.4270],\n",
      "        [0.3103],\n",
      "        [0.3620],\n",
      "        [0.4490]])\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.0560],\n",
      "        [0.1726],\n",
      "        [0.1099],\n",
      "        [0.5543],\n",
      "        [0.0828]])\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([[0.1014],\n",
      "        [0.1799],\n",
      "        [0.1042],\n",
      "        [0.4687],\n",
      "        [0.1923]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4433],\n",
      "        [0.1310],\n",
      "        [0.4037],\n",
      "        [0.0246],\n",
      "        [0.1178]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.3066],\n",
      "        [0.3879],\n",
      "        [0.0978],\n",
      "        [0.0380],\n",
      "        [0.5204]])\n",
      "tensor([0, 0, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([[0.0205],\n",
      "        [0.0476],\n",
      "        [0.0678],\n",
      "        [0.5464],\n",
      "        [0.1770]])\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([[0.1147],\n",
      "        [0.2248],\n",
      "        [0.1712],\n",
      "        [0.5270],\n",
      "        [0.2557]])\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([[0.4306],\n",
      "        [0.2273],\n",
      "        [0.0707],\n",
      "        [0.1742],\n",
      "        [0.3940]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.1019],\n",
      "        [0.7168],\n",
      "        [0.3308],\n",
      "        [0.1835],\n",
      "        [0.1229]])\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4398],\n",
      "        [0.1373],\n",
      "        [0.4803],\n",
      "        [0.1773],\n",
      "        [0.0294]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.2761],\n",
      "        [0.0362],\n",
      "        [0.0182],\n",
      "        [0.0453],\n",
      "        [0.0359]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.6044],\n",
      "        [0.5629],\n",
      "        [0.2214],\n",
      "        [0.0800],\n",
      "        [0.2946]])\n",
      "tensor([1, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.2197],\n",
      "        [0.1387],\n",
      "        [0.5324],\n",
      "        [0.0183],\n",
      "        [0.0390]])\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4928],\n",
      "        [0.3473],\n",
      "        [0.0370],\n",
      "        [0.3821],\n",
      "        [0.4670]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.5697],\n",
      "        [0.0506],\n",
      "        [0.5856],\n",
      "        [0.2754],\n",
      "        [0.1785]])\n",
      "tensor([1, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.0236],\n",
      "        [0.1484],\n",
      "        [0.0593],\n",
      "        [0.2044],\n",
      "        [0.3375]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.1695],\n",
      "        [0.1957],\n",
      "        [0.0642],\n",
      "        [0.5327],\n",
      "        [0.1740]])\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([[0.2686],\n",
      "        [0.2164],\n",
      "        [0.2199],\n",
      "        [0.4184],\n",
      "        [0.4773]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.0678],\n",
      "        [0.1953],\n",
      "        [0.2537],\n",
      "        [0.2133],\n",
      "        [0.4726]])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.2631],\n",
      "        [0.7523],\n",
      "        [0.1764],\n",
      "        [0.1748],\n",
      "        [0.0682]])\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([[0.4136],\n",
      "        [0.1877],\n",
      "        [0.1817]])\n",
      "tensor([0, 0, 0], dtype=torch.uint8)\n",
      "******************** Test ********************\n",
      "Loss: 0.4303619861602783, Accuracy: 0.8176470398902893 %\n",
      "**********************************************\n",
      "Val accuracy:0.818\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(trn_test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(trn_test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(trn_test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.397393</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.185460</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.266760</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.072809</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.224691</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.174780</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.068231</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.413631</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.187654</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.181660</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0            \n",
       "0.397393  1.0\n",
       "0.185460  0.0\n",
       "0.266760  1.0\n",
       "0.072809  0.0\n",
       "0.224691  0.0\n",
       "...       ...\n",
       "0.174780  0.0\n",
       "0.068231  0.0\n",
       "0.413631  1.0\n",
       "0.187654  0.0\n",
       "0.181660  0.0\n",
       "\n",
       "[168 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "#trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\n",
    "pd.DataFrame(trn_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19  29]\n",
      " [  0 120]]\n",
      "Accuracy :  0.8273809523809523\n",
      "Sensitivity :  0.3958333333333333\n",
      "Specificity :  1.0\n",
      "AUC:  0.6979166666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(trn_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 0., 0., 1.])\n",
      "tensor([0, 1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 1., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 0.])\n",
      "tensor([1, 0], dtype=torch.uint8)\n",
      "tensor([1., 0.])\n",
      "******************** Test ********************\n",
      "Loss: 0.3967731297016144, Accuracy: 0.644444465637207 %\n",
      "**********************************************\n",
      "Val accuracy:0.644\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        #print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        print(val_y)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.449398</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.173810</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.525002</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.017654</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.318993</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.408329</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.186568</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.449737</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.174035</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.217674</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.272487</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.169433</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.142663</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.386720</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.221057</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.063693</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.114062</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.397681</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.435515</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.433785</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.373202</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.257206</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.351418</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.288652</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.121296</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.060992</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.056814</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.035018</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.176432</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.201785</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.114430</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.604473</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.254487</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.229397</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.690109</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001029</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.060753</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.161144</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.325101</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.118374</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.550728</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.178836</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0            \n",
       "0.449398  0.0\n",
       "0.173810  0.0\n",
       "0.525002  0.0\n",
       "0.017654  0.0\n",
       "0.318993  1.0\n",
       "0.408329  0.0\n",
       "0.186568  0.0\n",
       "0.449737  0.0\n",
       "0.174035  0.0\n",
       "0.217674  1.0\n",
       "0.272487  0.0\n",
       "0.169433  0.0\n",
       "0.142663  1.0\n",
       "0.386720  0.0\n",
       "0.221057  1.0\n",
       "0.063693  1.0\n",
       "0.114062  0.0\n",
       "0.397681  0.0\n",
       "0.435515  0.0\n",
       "0.433785  1.0\n",
       "0.373202  0.0\n",
       "0.257206  0.0\n",
       "0.351418  0.0\n",
       "0.288652  0.0\n",
       "0.121296  0.0\n",
       "0.060992  1.0\n",
       "0.056814  0.0\n",
       "0.035018  0.0\n",
       "0.176432  0.0\n",
       "0.201785  1.0\n",
       "0.114430  0.0\n",
       "0.604473  0.0\n",
       "0.254487  1.0\n",
       "0.229397  1.0\n",
       "0.690109  1.0\n",
       "0.001029  0.0\n",
       "0.060753  0.0\n",
       "0.161144  1.0\n",
       "0.325101  0.0\n",
       "0.118374  0.0\n",
       "0.550728  1.0\n",
       "0.178836  0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "pd.DataFrame(val_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 11]\n",
      " [ 2 27]]\n",
      "Accuracy :  0.6904761904761905\n",
      "Sensitivity :  0.15384615384615385\n",
      "Specificity :  0.9310344827586207\n",
      "<class 'numpy.ndarray'>\n",
      "AUC:  0.5424403183023871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(val_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print(type(fpr))\n",
    "#print(fpr)\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# require to check~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "#'epoch': EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': trn_loss\n",
    "    }, \"./platin_model_save/platin_model_200_100_100_model_1_100_250__1.pth\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용합니다.\n",
    "실질적 조언\n",
    "새로 훈련할 데이터가 적지만 original 데이터와 유사할 경우\n",
    "\n",
    "데이터의 양이 적어 fine-tune (전체 모델에 대해서 backpropagation을 진행하는 것) 은 over-fitting의 위험이 있기에 하지 않습니다.\n",
    "새로 학습할 데이터는 original 데이터와 유사하기 때문에 이 경우 최종 linear classfier 레이어만 학습을 합니다.\n",
    "새로 훈련할 데이터가 매우 많으며 original 데이터와 유사할 경우\n",
    "\n",
    "새로 학습할 데이터의 양이 많다는 것은 over-fitting의 위험이 낮다는 뜻이므로, 전체 레이어에 대해서 fine-tune을 합니다.\n",
    "새로 훈련할 데이터가 적으며 original 데이터와 다른 경우\n",
    "\n",
    "데이터의 양이 적기 때문에 최종 단계의 linear classifier 레이어를 학습하는 것이 좋을 것입니다. 반면서 데이터가 서로 다르기 때문에 거의 마지막부분 (the top of the network)만 학습하는 것은 좋지 않습니다. 서로 상충이 되는데.. 이 경우에는 네트워크 초기 부분 어딘가 activation 이후에 특정 레이어를 학습시키는게 좋습니다.\n",
    "새로 훈련할 데이터가 많지만 original 데이터와와 다른 경우\n",
    "\n",
    "데이터가 많기 때문에 아예 새로운 ConvNet을 만들수도 있지만, 실적적으로 transfer learning이 더 효율이 좋습니다. 전체 네트워크에 대해서 fine-tune을 해도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=100, out_features=150, bias=True)\n",
       "  (1): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): Linear(in_features=150, out_features=250, bias=True)\n",
       "  (5): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[150,200,100,50]\n",
    "#[150,200,100,50,20]\n",
    "\n",
    "class DNN_seq_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN_seq_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear((trn_X_pd.shape[1]), 150, bias=True),\n",
    "            torch.nn.BatchNorm1d(150),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.classifier2 = nn.Sequential(    \n",
    "            torch.nn.Linear(150, 100, bias=True),\n",
    "            torch.nn.BatchNorm1d(100),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.classifier3 = nn.Sequential(  \n",
    "            torch.nn.Linear(100, 150, bias=True),\n",
    "            torch.nn.BatchNorm1d(150),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            torch.nn.Linear(150, 250, bias=True),\n",
    "            torch.nn.BatchNorm1d(250),\n",
    "            torch.nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            \n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            torch.nn.Linear(250, 1, bias=True),\n",
    "            torch.nn.BatchNorm1d(1)\n",
    "            #nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        x_out = self.classifier(x)\n",
    "        x_out = self.classifier2(x_out)\n",
    "        x_out = self.classifier3(x_out)\n",
    "        x_out = self.output_layer(x_out)\n",
    "        #x_out = F.dropout(x_out, p=0.5, training=self.training)\n",
    "        return torch.sigmoid(x_out)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = DNN_seq_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.classifier = model_2.classifier    \n",
    "model.classifier2 = model_2.classifier2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('./Platin_Data/Semi_Final/PC_tpm_200_minmax.csv', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_mod = df_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_mi = pd.read_csv('./Platin_Data/Semi_Final/MIR_rpm_100_minmax.csv', sep = '\\t', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_mi = df_data_mi.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_label,df_data_mod,df_data_mi], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLATINUM_STATUS</th>\n",
       "      <th>label</th>\n",
       "      <th>ENSG00000131096</th>\n",
       "      <th>ENSG00000187581</th>\n",
       "      <th>ENSG00000047936</th>\n",
       "      <th>ENSG00000186198</th>\n",
       "      <th>ENSG00000179914</th>\n",
       "      <th>ENSG00000186897</th>\n",
       "      <th>ENSG00000138136</th>\n",
       "      <th>ENSG00000139219</th>\n",
       "      <th>...</th>\n",
       "      <th>hsa-mir-514a-1</th>\n",
       "      <th>hsa-mir-101-2</th>\n",
       "      <th>hsa-mir-6892</th>\n",
       "      <th>hsa-mir-514a-2</th>\n",
       "      <th>hsa-mir-331</th>\n",
       "      <th>hsa-mir-3651</th>\n",
       "      <th>hsa-mir-550a-3</th>\n",
       "      <th>hsa-mir-6884</th>\n",
       "      <th>hsa-mir-1224</th>\n",
       "      <th>hsa-mir-3620</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1331</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.074279</td>\n",
       "      <td>0.149561</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.008411</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1332</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.056473</td>\n",
       "      <td>0.046285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.793124</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>0.007008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017185</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.054898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020890</td>\n",
       "      <td>0.014762</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.016416</td>\n",
       "      <td>0.066030</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.004774</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1364</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.020709</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.015363</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>0.067332</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.000942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2098</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.154442</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.233778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>0.003453</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.112554</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.027494</td>\n",
       "      <td>0.258720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.063795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.012297</td>\n",
       "      <td>0.005054</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.014861</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023144</td>\n",
       "      <td>0.096016</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.008756</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2113</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.138558</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.003812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLATINUM_STATUS  label  ENSG00000131096  ENSG00000187581  \\\n",
       "PATIENT_ID                                                              \n",
       "TCGA-04-1331       Sensitive    0.0         0.001124         0.074279   \n",
       "TCGA-04-1332       Sensitive    0.0         0.000566         0.001385   \n",
       "TCGA-04-1347       Sensitive    0.0         0.000096         0.002826   \n",
       "TCGA-04-1362       Resistant    1.0         0.002925         0.016522   \n",
       "TCGA-04-1364       Resistant    1.0         0.001883         1.000000   \n",
       "...                      ...    ...              ...              ...   \n",
       "TCGA-61-2098       Sensitive    0.0         0.000309         0.154442   \n",
       "TCGA-61-2109       Sensitive    0.0         0.001277         0.112554   \n",
       "TCGA-61-2110       Resistant    1.0         0.000163         0.006289   \n",
       "TCGA-61-2111       Sensitive    0.0         0.023144         0.096016   \n",
       "TCGA-61-2113       Sensitive    0.0         0.000137         0.010557   \n",
       "\n",
       "              ENSG00000047936  ENSG00000186198  ENSG00000179914  \\\n",
       "PATIENT_ID                                                        \n",
       "TCGA-04-1331         0.149561         0.020867         0.008411   \n",
       "TCGA-04-1332         0.002034         0.004467         0.056473   \n",
       "TCGA-04-1347         0.000060         0.004142         0.001344   \n",
       "TCGA-04-1362         0.002256         0.005812         0.001347   \n",
       "TCGA-04-1364         0.000016         0.020709         0.000372   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-61-2098         0.000215         0.001345         0.001390   \n",
       "TCGA-61-2109         0.005764         0.027494         0.258720   \n",
       "TCGA-61-2110         0.000069         0.000263         0.030643   \n",
       "TCGA-61-2111         0.000012         0.001709         0.001957   \n",
       "TCGA-61-2113         0.000103         0.002652         0.000410   \n",
       "\n",
       "              ENSG00000186897  ENSG00000138136  ENSG00000139219  ...  \\\n",
       "PATIENT_ID                                                       ...   \n",
       "TCGA-04-1331         0.372549         0.000000         0.074310  ...   \n",
       "TCGA-04-1332         0.046285         0.000000         0.005921  ...   \n",
       "TCGA-04-1347         0.793124         0.000461         0.001680  ...   \n",
       "TCGA-04-1362         0.015423         0.003234         0.054898  ...   \n",
       "TCGA-04-1364         0.245882         0.000000         0.010063  ...   \n",
       "...                       ...              ...              ...  ...   \n",
       "TCGA-61-2098         0.016812         0.000855         0.233778  ...   \n",
       "TCGA-61-2109         1.000000         0.003060         0.063795  ...   \n",
       "TCGA-61-2110         0.001324         0.000000         0.015528  ...   \n",
       "TCGA-61-2111         0.002696         0.021030         0.000892  ...   \n",
       "TCGA-61-2113         0.138558         0.004919         0.003812  ...   \n",
       "\n",
       "              hsa-mir-514a-1  hsa-mir-101-2  hsa-mir-6892  hsa-mir-514a-2  \\\n",
       "PATIENT_ID                                                                  \n",
       "TCGA-04-1331        0.000000       0.013312      0.006381        0.000037   \n",
       "TCGA-04-1332        0.000000       0.004477      0.002428        0.000000   \n",
       "TCGA-04-1347        0.000000       0.007213      0.007008        0.000000   \n",
       "TCGA-04-1362        0.020890       0.014762      0.002817        0.016416   \n",
       "TCGA-04-1364        0.003523       0.006381      0.015363        0.004760   \n",
       "...                      ...            ...           ...             ...   \n",
       "TCGA-61-2098        0.009848       0.003453      0.000070        0.005062   \n",
       "TCGA-61-2109        0.000188       0.012297      0.005054        0.000223   \n",
       "TCGA-61-2110        0.001060       0.014861      0.001302        0.000619   \n",
       "TCGA-61-2111        0.000004       0.008756      0.001384        0.000003   \n",
       "TCGA-61-2113        0.000033       0.016473      0.006167        0.000000   \n",
       "\n",
       "              hsa-mir-331  hsa-mir-3651  hsa-mir-550a-3  hsa-mir-6884  \\\n",
       "PATIENT_ID                                                              \n",
       "TCGA-04-1331     0.008103      0.000593        0.000886      0.000332   \n",
       "TCGA-04-1332     0.007846      0.002270        0.005067      0.000000   \n",
       "TCGA-04-1347     0.017185      0.001530        0.001314      0.000216   \n",
       "TCGA-04-1362     0.066030      0.001828        0.002836      0.000479   \n",
       "TCGA-04-1364     0.067332      0.000710        0.000300      0.000252   \n",
       "...                   ...           ...             ...           ...   \n",
       "TCGA-61-2098     0.005220      0.000150        0.000351      0.000014   \n",
       "TCGA-61-2109     0.002850      0.000077        0.000069      0.000009   \n",
       "TCGA-61-2110     0.004408      0.000117        0.000069      0.000000   \n",
       "TCGA-61-2111     0.003422      0.000052        0.000140      0.000000   \n",
       "TCGA-61-2113     0.005581      0.000342        0.001664      0.000039   \n",
       "\n",
       "              hsa-mir-1224  hsa-mir-3620  \n",
       "PATIENT_ID                                \n",
       "TCGA-04-1331      0.001581      0.000252  \n",
       "TCGA-04-1332      0.001556      0.000147  \n",
       "TCGA-04-1347      0.000000      0.000357  \n",
       "TCGA-04-1362      0.004774      0.003333  \n",
       "TCGA-04-1364      0.001199      0.000942  \n",
       "...                    ...           ...  \n",
       "TCGA-61-2098      0.000179      0.000072  \n",
       "TCGA-61-2109      0.000158      0.000365  \n",
       "TCGA-61-2110      0.000098      0.000089  \n",
       "TCGA-61-2111      0.001440      0.000129  \n",
       "TCGA-61-2113      0.000302      0.000320  \n",
       "\n",
       "[210 rows x 302 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TCGA-25-1635', 'TCGA-29-1691', 'TCGA-23-1030', 'TCGA-24-1565',\n",
       "       'TCGA-24-2280', 'TCGA-10-0931', 'TCGA-24-2024', 'TCGA-10-0927',\n",
       "       'TCGA-13-1497', 'TCGA-23-2078',\n",
       "       ...\n",
       "       'TCGA-24-2036', 'TCGA-04-1362', 'TCGA-13-0727', 'TCGA-25-2409',\n",
       "       'TCGA-36-1570', 'TCGA-13-1487', 'TCGA-61-1736', 'TCGA-23-1023',\n",
       "       'TCGA-24-1105', 'TCGA-04-1365'],\n",
       "      dtype='object', name='PATIENT_ID', length=168)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of Training Data\n",
      "Samples : 168\n",
      "Features : 300\n"
     ]
    }
   ],
   "source": [
    "trn_X_pd = train.drop([\"PLATINUM_STATUS\",\"label\"],axis=1).values\n",
    "trn_y_pd = train.label.values\n",
    "\n",
    "print(\"Information of Training Data\")\n",
    "print(\"Samples : {}\".format(trn_X_pd.shape[0]))\n",
    "print(\"Features : {}\".format(trn_X_pd.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of Training Data\n",
      "Samples : 42\n",
      "Features : 300\n"
     ]
    }
   ],
   "source": [
    "val_X_pd = val.drop([\"PLATINUM_STATUS\",\"label\"],axis=1).values\n",
    "val_y_pd = val.label.values\n",
    "\n",
    "print(\"Information of Training Data\")\n",
    "print(\"Samples : {}\".format(val_X_pd.shape[0]))\n",
    "print(\"Features : {}\".format(val_X_pd.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLATINUM_STATUS</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1331</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1332</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1362</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1364</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2098</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2109</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2110</th>\n",
       "      <td>Resistant</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2111</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-61-2113</th>\n",
       "      <td>Sensitive</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PLATINUM_STATUS  label\n",
       "PATIENT_ID                         \n",
       "TCGA-04-1331       Sensitive    0.0\n",
       "TCGA-04-1332       Sensitive    0.0\n",
       "TCGA-04-1347       Sensitive    0.0\n",
       "TCGA-04-1362       Resistant    1.0\n",
       "TCGA-04-1364       Resistant    1.0\n",
       "...                      ...    ...\n",
       "TCGA-61-2098       Sensitive    0.0\n",
       "TCGA-61-2109       Sensitive    0.0\n",
       "TCGA-61-2110       Resistant    1.0\n",
       "TCGA-61-2111       Sensitive    0.0\n",
       "TCGA-61-2113       Sensitive    0.0\n",
       "\n",
       "[210 rows x 2 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X = torch.from_numpy(trn_X_pd.astype(float))\n",
    "trn_y = torch.from_numpy(trn_y_pd.astype(float))\n",
    "\n",
    "val_X = torch.from_numpy(val_X_pd.astype(float))\n",
    "val_y = torch.from_numpy(val_y_pd.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = Dataset(trn_X, trn_y)\n",
    "trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "trn_test_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "val = Dataset(val_X, val_y)\n",
    "test_loader = data_utils.DataLoader(val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 1e-3\n",
    "    \n",
    "## After just second, will modify optimizer(weight_decay,step_size, base_lr, max_lr,scheduler)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 1e-5)\n",
    "    \n",
    "#model = DNN_seq_2()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "num_epochs = 90\n",
    "    \n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "##############################################################################\n",
    "step_size = 2000\n",
    "base_lr, max_lr = 0.001, 0.01  \n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode='exp_range', gamma=0.99994)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 1.0071132183074951, Accuracy: 0.581818163394928 % \t time=0.33s\n",
      "******************** Train ********************\n",
      "Loss: 0.41691604256629944, Accuracy: 0.7090908885002136 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.7919425964355469, Accuracy: 0.6969696879386902 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.545437216758728, Accuracy: 0.739393949508667 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.34149715304374695, Accuracy: 0.7212121486663818 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 1.0561163425445557, Accuracy: 0.7818182110786438 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.39828401803970337, Accuracy: 0.7636363506317139 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.3370503783226013, Accuracy: 0.7818182110786438 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 1.1350923776626587, Accuracy: 0.7454545497894287 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.3489335775375366, Accuracy: 0.7939394116401672 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.243299201130867, Accuracy: 0.842424213886261 % \t time=0.35s\n",
      "******************** Train ********************\n",
      "Loss: 0.4145277142524719, Accuracy: 0.7757575511932373 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.4217621684074402, Accuracy: 0.8121212124824524 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.4467671513557434, Accuracy: 0.8242424130439758 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.22897815704345703, Accuracy: 0.8303030133247375 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.592108428478241, Accuracy: 0.7757575511932373 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.2600855827331543, Accuracy: 0.8060606122016907 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.2349676638841629, Accuracy: 0.8121212124824524 % \t time=0.35s\n",
      "******************** Train ********************\n",
      "Loss: 0.1950533092021942, Accuracy: 0.842424213886261 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.24666407704353333, Accuracy: 0.8121212124824524 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.12552350759506226, Accuracy: 0.8363636136054993 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.16354086995124817, Accuracy: 0.8484848737716675 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.38372278213500977, Accuracy: 0.8666666746139526 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.3552098870277405, Accuracy: 0.8303030133247375 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.1931595802307129, Accuracy: 0.9151515364646912 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.45551198720932007, Accuracy: 0.8484848737716675 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.2864830493927002, Accuracy: 0.8545454740524292 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.3077026307582855, Accuracy: 0.800000011920929 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.1552678644657135, Accuracy: 0.842424213886261 % \t time=0.35s\n",
      "******************** Train ********************\n",
      "Loss: 0.35684555768966675, Accuracy: 0.8545454740524292 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.2296331375837326, Accuracy: 0.8121212124824524 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.10972452163696289, Accuracy: 0.8060606122016907 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.21808508038520813, Accuracy: 0.8060606122016907 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.33641317486763, Accuracy: 0.8848484754562378 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.43110227584838867, Accuracy: 0.7818182110786438 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.44195351004600525, Accuracy: 0.8060606122016907 % \t time=0.33s\n",
      "******************** Train ********************\n",
      "Loss: 0.4300507605075836, Accuracy: 0.8727272748947144 % \t time=0.36s\n",
      "******************** Train ********************\n",
      "Loss: 0.15807805955410004, Accuracy: 0.8545454740524292 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.1862991899251938, Accuracy: 0.842424213886261 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.5104192495346069, Accuracy: 0.8181818127632141 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.5306110382080078, Accuracy: 0.8484848737716675 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.14819779992103577, Accuracy: 0.8242424130439758 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.22071869671344757, Accuracy: 0.8363636136054993 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.22690685093402863, Accuracy: 0.8606060743331909 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.3186710476875305, Accuracy: 0.8242424130439758 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.17944225668907166, Accuracy: 0.8727272748947144 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.27672427892684937, Accuracy: 0.8545454740524292 % \t time=0.35s\n",
      "******************** Train ********************\n",
      "Loss: 0.22246447205543518, Accuracy: 0.8303030133247375 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.12160345166921616, Accuracy: 0.8606060743331909 % \t time=0.38s\n",
      "******************** Train ********************\n",
      "Loss: 0.513762354850769, Accuracy: 0.8060606122016907 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.3127741813659668, Accuracy: 0.8909090757369995 % \t time=0.37s\n",
      "******************** Train ********************\n",
      "Loss: 0.3958839774131775, Accuracy: 0.842424213886261 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.45369380712509155, Accuracy: 0.8242424130439758 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.4383088946342468, Accuracy: 0.7878788113594055 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.24743685126304626, Accuracy: 0.842424213886261 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.19467492401599884, Accuracy: 0.8121212124824524 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.2576668858528137, Accuracy: 0.8666666746139526 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.07395871728658676, Accuracy: 0.903030276298523 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.1783505380153656, Accuracy: 0.842424213886261 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.14993906021118164, Accuracy: 0.8363636136054993 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.38001230359077454, Accuracy: 0.8727272748947144 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5091192722320557, Accuracy: 0.8060606122016907 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5040379762649536, Accuracy: 0.8484848737716675 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.23296496272087097, Accuracy: 0.8787878751754761 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.16682398319244385, Accuracy: 0.842424213886261 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.0695703849196434, Accuracy: 0.8363636136054993 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5844326019287109, Accuracy: 0.9212121367454529 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.1761879026889801, Accuracy: 0.8606060743331909 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.24580149352550507, Accuracy: 0.8727272748947144 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5946789383888245, Accuracy: 0.8606060743331909 % \t time=0.25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 0.319019079208374, Accuracy: 0.8363636136054993 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.2018052637577057, Accuracy: 0.8484848737716675 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.11483678966760635, Accuracy: 0.8606060743331909 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.15633618831634521, Accuracy: 0.8363636136054993 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.3994195759296417, Accuracy: 0.8909090757369995 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5272725224494934, Accuracy: 0.8787878751754761 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.281921923160553, Accuracy: 0.8606060743331909 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.5282734632492065, Accuracy: 0.842424213886261 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.3003048896789551, Accuracy: 0.8787878751754761 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.08649857342243195, Accuracy: 0.8666666746139526 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.39222922921180725, Accuracy: 0.8787878751754761 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.12450303882360458, Accuracy: 0.8666666746139526 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.23866720497608185, Accuracy: 0.8969696760177612 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.13278929889202118, Accuracy: 0.8787878751754761 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.16504162549972534, Accuracy: 0.9272727370262146 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.48638004064559937, Accuracy: 0.8727272748947144 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.49838289618492126, Accuracy: 0.8787878751754761 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.21263650059700012, Accuracy: 0.8545454740524292 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.4492422044277191, Accuracy: 0.8969696760177612 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.34440502524375916, Accuracy: 0.8484848737716675 % \t time=0.25s\n",
      "**********************************************\n",
      "Trn accuracy:0.848 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    #correct = 0.   # Accuracy\n",
    "    correct_val = 0.\n",
    "        \n",
    "    for batch_idx, trn in enumerate(trn_loader):\n",
    "        trn_X, trn_y = trn['X'], trn['y']\n",
    "        if use_cuda:\n",
    "            trn_X, trn_y = trn_X.cuda(), trn_y.cuda()\n",
    "        trn_X, trn_y = Variable(trn_X).float(), Variable(trn_y).float()\n",
    "        optimizer.zero_grad()\n",
    "        trn_pred = model(trn_X)\n",
    "        for i in trn_pred:\n",
    "            predict.append(i.detach().numpy())\n",
    "            \n",
    "        if scheduler:\n",
    "            #print('cycle_LR')\n",
    "            scheduler.batch_step()\n",
    "        #print(trn_pred.squeeze())\n",
    "        #print(trn_y)\n",
    "        trn_loss = criterion(trn_pred.squeeze(), trn_y)\n",
    "        trn_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += trn_loss.item()/len(trn_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        predicted = (trn_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        correct_val += (predicted == trn_y).sum()\n",
    "    print('*'*20, 'Train', '*'*20)\n",
    "    print('Loss: {}, Accuracy: {} % \\t time={:.2f}s'.format(trn_loss.item(), correct_val/(len(trn_loader)*batch_size), elapsed_time))\n",
    "print('*'*46)\n",
    "print(\"Trn accuracy:{:.3f} \".format(float(correct_val) / (len(trn_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2319],\n",
      "        [0.6075],\n",
      "        [0.0560],\n",
      "        [0.0439],\n",
      "        [0.0374]])\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "******************** Test ********************\n",
      "Loss: 0.18055102229118347, Accuracy: 1.0 %\n",
      "**********************************************\n",
      "Val accuracy:1.000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(trn_test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(trn_test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(trn_test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (168, 1), indices imply (5, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     raise ValueError(\n\u001b[0;32m-> 1719\u001b[0;31m         \u001b[0;34m\"Shape of passed values is {0}, indices imply {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (168, 1), indices imply (5, 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-714a3c8a24fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredict_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_y_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1717\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m     raise ValueError(\n\u001b[0;32m-> 1719\u001b[0;31m         \u001b[0;34m\"Shape of passed values is {0}, indices imply {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m     )\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (168, 1), indices imply (5, 1)"
     ]
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "#trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\n",
    "pd.DataFrame(trn_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-fc29f22a4108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#print(test_p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicted_prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'f'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'continuous'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(trn_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 0., 0., 0.])\n",
      "tensor([1, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 0., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 1., 0.])\n",
      "tensor([0, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 1., 0., 0.])\n",
      "tensor([0, 1, 0, 0, 1], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 1.])\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 0.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 0., 0., 0.])\n",
      "tensor([0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0.])\n",
      "******************** Test ********************\n",
      "Loss: 0.08248241990804672, Accuracy: 0.9111111164093018 %\n",
      "**********************************************\n",
      "Val accuracy:0.911\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        #print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        print(val_y)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.579014</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.035489</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.040849</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.022445</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.491972</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.537468</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.032577</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050847</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.061795</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.242136</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.051903</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.230955</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.017047</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.096018</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.866469</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.053779</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.023639</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.133525</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.683034</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.054034</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.044093</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.681121</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.630900</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.054865</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.063908</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.034430</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.770462</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.060978</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.047941</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.752675</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.019921</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.040976</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.722392</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050058</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.430601</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.172124</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.037420</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.009098</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.016886</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.037097</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.057579</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100271</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0            \n",
       "0.579014  1.0\n",
       "0.035489  0.0\n",
       "0.040849  0.0\n",
       "0.022445  0.0\n",
       "0.491972  0.0\n",
       "0.537468  1.0\n",
       "0.032577  0.0\n",
       "0.050847  0.0\n",
       "0.061795  0.0\n",
       "0.242136  0.0\n",
       "0.051903  0.0\n",
       "0.230955  0.0\n",
       "0.017047  0.0\n",
       "0.096018  0.0\n",
       "0.866469  1.0\n",
       "0.053779  0.0\n",
       "0.023639  0.0\n",
       "0.133525  0.0\n",
       "0.683034  1.0\n",
       "0.054034  0.0\n",
       "0.044093  0.0\n",
       "0.681121  1.0\n",
       "0.630900  1.0\n",
       "0.054865  0.0\n",
       "0.063908  0.0\n",
       "0.034430  0.0\n",
       "0.770462  1.0\n",
       "0.060978  0.0\n",
       "0.047941  0.0\n",
       "0.752675  1.0\n",
       "0.019921  0.0\n",
       "0.040976  0.0\n",
       "0.722392  1.0\n",
       "0.050058  0.0\n",
       "0.430601  0.0\n",
       "0.172124  1.0\n",
       "0.037420  0.0\n",
       "0.009098  0.0\n",
       "0.016886  0.0\n",
       "0.037097  0.0\n",
       "0.057579  0.0\n",
       "0.100271  0.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "pd.DataFrame(val_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  1]\n",
      " [ 0 32]]\n",
      "Accuracy :  0.9761904761904762\n",
      "Sensitivity :  0.9\n",
      "Specificity :  1.0\n",
      "<class 'numpy.ndarray'>\n",
      "AUC:  0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(val_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print(type(fpr))\n",
    "#print(fpr)\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "#'epoch': EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': trn_loss\n",
    "    }, \"./platin_model_save/platin_model_200_|_100_model_2_lnc_transfer.pth\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/171 (0%)]\tLoss: 0.746858\t Accuracy:50.000%\n",
      "Epoch : 0 [32/171 (18%)]\tLoss: 0.662493\t Accuracy:60.417%\n",
      "Epoch : 0 [64/171 (36%)]\tLoss: 0.704260\t Accuracy:60.000%\n",
      "Epoch : 0 [96/171 (55%)]\tLoss: 0.674394\t Accuracy:66.071%\n",
      "Epoch : 0 [128/171 (73%)]\tLoss: 0.722818\t Accuracy:67.361%\n",
      "Epoch : 0 [110/171 (91%)]\tLoss: 0.599336\t Accuracy:65.909%\n",
      "Epoch : 1 [0/171 (0%)]\tLoss: 0.575366\t Accuracy:87.500%\n",
      "Epoch : 1 [32/171 (18%)]\tLoss: 0.667310\t Accuracy:70.833%\n",
      "Epoch : 1 [64/171 (36%)]\tLoss: 0.673750\t Accuracy:68.750%\n",
      "Epoch : 1 [96/171 (55%)]\tLoss: 0.751240\t Accuracy:67.857%\n",
      "Epoch : 1 [128/171 (73%)]\tLoss: 0.740712\t Accuracy:67.361%\n",
      "Epoch : 1 [110/171 (91%)]\tLoss: 0.710288\t Accuracy:65.909%\n",
      "Epoch : 2 [0/171 (0%)]\tLoss: 0.641428\t Accuracy:62.500%\n",
      "Epoch : 2 [32/171 (18%)]\tLoss: 0.618767\t Accuracy:68.750%\n",
      "Epoch : 2 [64/171 (36%)]\tLoss: 0.619703\t Accuracy:65.000%\n",
      "Epoch : 2 [96/171 (55%)]\tLoss: 0.579154\t Accuracy:65.179%\n",
      "Epoch : 2 [128/171 (73%)]\tLoss: 0.667362\t Accuracy:66.667%\n",
      "Epoch : 2 [110/171 (91%)]\tLoss: 0.572492\t Accuracy:65.909%\n",
      "Epoch : 3 [0/171 (0%)]\tLoss: 0.709065\t Accuracy:75.000%\n",
      "Epoch : 3 [32/171 (18%)]\tLoss: 0.596966\t Accuracy:70.833%\n",
      "Epoch : 3 [64/171 (36%)]\tLoss: 0.615786\t Accuracy:72.500%\n",
      "Epoch : 3 [96/171 (55%)]\tLoss: 0.585047\t Accuracy:68.750%\n",
      "Epoch : 3 [128/171 (73%)]\tLoss: 0.710601\t Accuracy:70.139%\n",
      "Epoch : 3 [110/171 (91%)]\tLoss: 0.893020\t Accuracy:65.909%\n",
      "Epoch : 4 [0/171 (0%)]\tLoss: 0.663931\t Accuracy:68.750%\n",
      "Epoch : 4 [32/171 (18%)]\tLoss: 0.728086\t Accuracy:60.417%\n",
      "Epoch : 4 [64/171 (36%)]\tLoss: 0.655770\t Accuracy:62.500%\n",
      "Epoch : 4 [96/171 (55%)]\tLoss: 0.606156\t Accuracy:65.179%\n",
      "Epoch : 4 [128/171 (73%)]\tLoss: 0.622089\t Accuracy:66.667%\n",
      "Epoch : 4 [110/171 (91%)]\tLoss: 0.579138\t Accuracy:65.909%\n",
      "Epoch : 5 [0/171 (0%)]\tLoss: 0.675125\t Accuracy:68.750%\n",
      "Epoch : 5 [32/171 (18%)]\tLoss: 0.662640\t Accuracy:64.583%\n",
      "Epoch : 5 [64/171 (36%)]\tLoss: 0.550371\t Accuracy:71.250%\n",
      "Epoch : 5 [96/171 (55%)]\tLoss: 0.596231\t Accuracy:69.643%\n",
      "Epoch : 5 [128/171 (73%)]\tLoss: 0.657763\t Accuracy:69.444%\n",
      "Epoch : 5 [110/171 (91%)]\tLoss: 0.628837\t Accuracy:65.909%\n",
      "Epoch : 6 [0/171 (0%)]\tLoss: 0.709476\t Accuracy:62.500%\n",
      "Epoch : 6 [32/171 (18%)]\tLoss: 0.679773\t Accuracy:72.917%\n",
      "Epoch : 6 [64/171 (36%)]\tLoss: 0.579446\t Accuracy:67.500%\n",
      "Epoch : 6 [96/171 (55%)]\tLoss: 0.582037\t Accuracy:68.750%\n",
      "Epoch : 6 [128/171 (73%)]\tLoss: 0.705310\t Accuracy:67.361%\n",
      "Epoch : 6 [110/171 (91%)]\tLoss: 0.729313\t Accuracy:65.909%\n",
      "Epoch : 7 [0/171 (0%)]\tLoss: 0.688270\t Accuracy:62.500%\n",
      "Epoch : 7 [32/171 (18%)]\tLoss: 0.753611\t Accuracy:68.750%\n",
      "Epoch : 7 [64/171 (36%)]\tLoss: 0.588925\t Accuracy:62.500%\n",
      "Epoch : 7 [96/171 (55%)]\tLoss: 0.562787\t Accuracy:65.179%\n",
      "Epoch : 7 [128/171 (73%)]\tLoss: 0.592404\t Accuracy:66.667%\n",
      "Epoch : 7 [110/171 (91%)]\tLoss: 0.673879\t Accuracy:65.909%\n",
      "Epoch : 8 [0/171 (0%)]\tLoss: 0.641790\t Accuracy:81.250%\n",
      "Epoch : 8 [32/171 (18%)]\tLoss: 0.659684\t Accuracy:68.750%\n",
      "Epoch : 8 [64/171 (36%)]\tLoss: 0.469823\t Accuracy:75.000%\n",
      "Epoch : 8 [96/171 (55%)]\tLoss: 0.649886\t Accuracy:68.750%\n",
      "Epoch : 8 [128/171 (73%)]\tLoss: 0.584355\t Accuracy:68.750%\n",
      "Epoch : 8 [110/171 (91%)]\tLoss: 0.712025\t Accuracy:65.909%\n",
      "Epoch : 9 [0/171 (0%)]\tLoss: 0.726692\t Accuracy:56.250%\n",
      "Epoch : 9 [32/171 (18%)]\tLoss: 0.564370\t Accuracy:62.500%\n",
      "Epoch : 9 [64/171 (36%)]\tLoss: 0.594889\t Accuracy:58.750%\n",
      "Epoch : 9 [96/171 (55%)]\tLoss: 0.599691\t Accuracy:63.393%\n",
      "Epoch : 9 [128/171 (73%)]\tLoss: 0.662696\t Accuracy:65.972%\n",
      "Epoch : 9 [110/171 (91%)]\tLoss: 0.608411\t Accuracy:65.909%\n"
     ]
    }
   ],
   "source": [
    "#model = DNN_seq()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "fit(model, trn_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Test ********************\n",
      "Loss: 0.5934396982192993, Accuracy: 0.7291666865348816 %\n",
      "**********************************************\n",
      "Val accuracy:0.729\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(val_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        val_loss = criterion(val_pred, val_y)\n",
    "        val_loss_summary += val_loss\n",
    "        predicted = torch.max(val_pred.data, 1)[1] \n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(val_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(val_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifier\n",
    "# VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# ensemble 할 model 정의\n",
    "models = [\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('bc', BaggingClassifier()),\n",
    "    ('etc',ExtraTreesClassifier()),\n",
    "    ('gbc', GradientBoostingClassifier()),\n",
    "    ('rfc', RandomForestClassifier()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('svc', SVC(probability=True)),\n",
    "    ('xgb', XGBClassifier()),\n",
    "    ('lgbm', LGBMClassifier()),\n",
    "    ('dtc', DecisionTreeClassifier()),\n",
    "    ('lr', LogisticRegressionCV()),\n",
    "    ('ridge', RidgeClassifier()),\n",
    "]\n",
    "\n",
    "# hard vote\n",
    "hard_vote  = VotingClassifier(models, voting='hard')\n",
    "hard_vote_cv = cross_validate(hard_vote, x_train, y_train, cv=k_fold)\n",
    "hard_vote.fit(x_train, y_train)\n",
    "\n",
    "# soft vote\n",
    "soft_vote  = VotingClassifier(models, voting='soft')\n",
    "soft_vote_cv = cross_validate(soft_vote, x_train, y_train, cv=k_fold)\n",
    "soft_vote.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X = torch.from_numpy(trn_X_pd.astype(float))\n",
    "trn_y = torch.from_numpy(trn_y_pd.astype(float))\n",
    "\n",
    "val_X = torch.from_numpy(val_X_pd.astype(float))\n",
    "val_y = torch.from_numpy(val_y_pd.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = Dataset(trn_X, trn_y)\n",
    "trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "trn_test_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "val = Dataset(val_X, val_y)\n",
    "test_loader = data_utils.DataLoader(val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 (Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 1e-3\n",
    "model_1 = DNN_seq_1()   \n",
    "## After just second, will modify optimizer(weight_decay,step_size, base_lr, max_lr,scheduler)\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=0.001, weight_decay = 1e-5)\n",
    "    \n",
    "model_1 = DNN_seq_1()\n",
    "\n",
    "\n",
    "##\n",
    "use_cuda = torch.cuda.is_available()\n",
    "num_epochs = 90\n",
    "    \n",
    "if use_cuda:\n",
    "    model_1 = model_1.cuda()\n",
    "    \n",
    "##############################################################################\n",
    "step_size = 2000\n",
    "base_lr, max_lr = 0.001, 0.01  \n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_1.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode='exp_range', gamma=0.99994)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 0.62250155210495, Accuracy: 0.6727272868156433 % \t time=0.16s\n",
      "******************** Train ********************\n",
      "Loss: 0.28187888860702515, Accuracy: 0.7757575511932373 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.4212983250617981, Accuracy: 0.7818182110786438 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.3047163784503937, Accuracy: 0.8242424130439758 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.18727238476276398, Accuracy: 0.8727272748947144 % \t time=0.19s\n",
      "******************** Train ********************\n",
      "Loss: 0.15723516047000885, Accuracy: 0.8727272748947144 % \t time=0.17s\n",
      "******************** Train ********************\n",
      "Loss: 0.08779895305633545, Accuracy: 0.8666666746139526 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 1.234900951385498, Accuracy: 0.8606060743331909 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.5220003724098206, Accuracy: 0.8969696760177612 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.2840283215045929, Accuracy: 0.9333333373069763 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.2387736290693283, Accuracy: 0.9151515364646912 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.18635377287864685, Accuracy: 0.9515151381492615 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.07692518085241318, Accuracy: 0.9151515364646912 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.05599735304713249, Accuracy: 0.8727272748947144 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.534781277179718, Accuracy: 0.8606060743331909 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.057902783155441284, Accuracy: 0.9090909361839294 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.008008360862731934, Accuracy: 0.9333333373069763 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.02095702290534973, Accuracy: 0.903030276298523 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.04419896751642227, Accuracy: 0.9151515364646912 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.4942611753940582, Accuracy: 0.9575757384300232 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.2955169975757599, Accuracy: 0.9090909361839294 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.6337161064147949, Accuracy: 0.8727272748947144 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.04954375699162483, Accuracy: 0.9272727370262146 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.07095135748386383, Accuracy: 0.9757575988769531 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.28759220242500305, Accuracy: 0.9333333373069763 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.010455144569277763, Accuracy: 0.9636363387107849 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.02844441495835781, Accuracy: 0.8909090757369995 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.006581947207450867, Accuracy: 0.8666666746139526 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.028744200244545937, Accuracy: 0.939393937587738 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.10535931587219238, Accuracy: 0.903030276298523 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.04219571873545647, Accuracy: 0.9515151381492615 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.18546509742736816, Accuracy: 0.9696969985961914 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.18141062557697296, Accuracy: 0.9636363387107849 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.632726788520813, Accuracy: 0.9454545378684998 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.3828902244567871, Accuracy: 0.939393937587738 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.006130811758339405, Accuracy: 0.9454545378684998 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.1307135373353958, Accuracy: 0.903030276298523 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.05619405582547188, Accuracy: 0.9212121367454529 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.3888086676597595, Accuracy: 0.9454545378684998 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.25399938225746155, Accuracy: 0.9151515364646912 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.14394767582416534, Accuracy: 0.9515151381492615 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.013043539598584175, Accuracy: 0.9090909361839294 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.04059789702296257, Accuracy: 0.939393937587738 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.4711894392967224, Accuracy: 0.9090909361839294 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.27365201711654663, Accuracy: 0.9212121367454529 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 1.1884006261825562, Accuracy: 0.9515151381492615 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.18024671077728271, Accuracy: 0.8969696760177612 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.47524842619895935, Accuracy: 0.9272727370262146 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.4806756377220154, Accuracy: 0.9272727370262146 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.05919594317674637, Accuracy: 0.9454545378684998 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.022758426144719124, Accuracy: 0.939393937587738 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.026986276730895042, Accuracy: 0.9636363387107849 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.05205497890710831, Accuracy: 0.9575757384300232 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.01322900503873825, Accuracy: 0.9151515364646912 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.11537815630435944, Accuracy: 0.9575757384300232 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.04799365624785423, Accuracy: 0.9696969985961914 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.019096996635198593, Accuracy: 0.9272727370262146 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.036252010613679886, Accuracy: 0.903030276298523 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.35007181763648987, Accuracy: 0.9151515364646912 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.3869224488735199, Accuracy: 0.8969696760177612 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.14708928763866425, Accuracy: 0.9333333373069763 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.6135136485099792, Accuracy: 0.939393937587738 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.5050512552261353, Accuracy: 0.9333333373069763 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.4565650522708893, Accuracy: 0.939393937587738 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.6415605545043945, Accuracy: 0.9333333373069763 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.052321888506412506, Accuracy: 0.9636363387107849 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.13653358817100525, Accuracy: 0.9636363387107849 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.002113726455718279, Accuracy: 0.939393937587738 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.19608402252197266, Accuracy: 0.9818181991577148 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.16916313767433167, Accuracy: 0.939393937587738 % \t time=0.14s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 0.025340717285871506, Accuracy: 0.939393937587738 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.020182045176625252, Accuracy: 0.939393937587738 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.00676519563421607, Accuracy: 0.9636363387107849 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.06858143955469131, Accuracy: 0.9515151381492615 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.03390727937221527, Accuracy: 0.9575757384300232 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.00573237519711256, Accuracy: 0.9454545378684998 % \t time=0.15s\n",
      "******************** Train ********************\n",
      "Loss: 0.009802227839827538, Accuracy: 0.9696969985961914 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.006746111903339624, Accuracy: 0.9818181991577148 % \t time=0.13s\n",
      "******************** Train ********************\n",
      "Loss: 0.0028420479502528906, Accuracy: 0.9575757384300232 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.08800500631332397, Accuracy: 0.9878787994384766 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.0037551014684140682, Accuracy: 0.9575757384300232 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.07235263288021088, Accuracy: 0.9878787994384766 % \t time=0.13s\n",
      "******************** Train ********************\n",
      "Loss: 0.0049733491614460945, Accuracy: 0.9818181991577148 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.029510319232940674, Accuracy: 0.9757575988769531 % \t time=0.13s\n",
      "******************** Train ********************\n",
      "Loss: 0.16043011844158173, Accuracy: 0.9878787994384766 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.022079041227698326, Accuracy: 0.9515151381492615 % \t time=0.13s\n",
      "******************** Train ********************\n",
      "Loss: 0.46233201026916504, Accuracy: 0.9696969985961914 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.002653642324730754, Accuracy: 0.9696969985961914 % \t time=0.13s\n",
      "******************** Train ********************\n",
      "Loss: 0.01609763689339161, Accuracy: 0.9515151381492615 % \t time=0.14s\n",
      "******************** Train ********************\n",
      "Loss: 0.0257441196590662, Accuracy: 0.9575757384300232 % \t time=0.14s\n",
      "**********************************************\n",
      "Trn accuracy:0.958 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    #correct = 0.   # Accuracy\n",
    "    correct_val = 0.\n",
    "        \n",
    "    for batch_idx, trn in enumerate(trn_loader):\n",
    "        trn_X, trn_y = trn['X'], trn['y']\n",
    "        if use_cuda:\n",
    "            trn_X, trn_y = trn_X.cuda(), trn_y.cuda()\n",
    "        trn_X, trn_y = Variable(trn_X).float(), Variable(trn_y).float()\n",
    "        optimizer.zero_grad()\n",
    "        trn_pred = model_1(trn_X)\n",
    "        for i in trn_pred:\n",
    "            predict.append(i.detach().numpy())\n",
    "            \n",
    "        if scheduler:\n",
    "            #print('cycle_LR')\n",
    "            scheduler.batch_step()\n",
    "        #print(trn_pred.squeeze())\n",
    "        #print(trn_y)\n",
    "        trn_loss = criterion(trn_pred.squeeze(), trn_y)\n",
    "        trn_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += trn_loss.item()/len(trn_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        predicted = (trn_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        correct_val += (predicted == trn_y).sum()\n",
    "    print('*'*20, 'Train', '*'*20)\n",
    "    print('Loss: {}, Accuracy: {} % \\t time={:.2f}s'.format(trn_loss.item(), correct_val/(len(trn_loader)*batch_size), elapsed_time))\n",
    "print('*'*46)\n",
    "print(\"Trn accuracy:{:.3f} \".format(float(correct_val) / (len(trn_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 (Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 1e-3\n",
    "model_2 = DNN_seq_2()   \n",
    "## After just second, will modify optimizer(weight_decay,step_size, base_lr, max_lr,scheduler)\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001, weight_decay = 1e-5)\n",
    "    \n",
    "model_2 = DNN_seq_2()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "num_epochs = 90\n",
    "    \n",
    "if use_cuda:\n",
    "    model_2 = model_2.cuda()\n",
    "    \n",
    "##############################################################################\n",
    "step_size = 2000\n",
    "base_lr, max_lr = 0.001, 0.01  \n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_2.parameters()), \n",
    "                             lr=max_lr)\n",
    "    \n",
    "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr, step_size=step_size, mode='exp_range', gamma=0.99994)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 0.8399748802185059, Accuracy: 0.4727272689342499 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.7234965562820435, Accuracy: 0.5333333611488342 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.432243674993515, Accuracy: 0.4545454680919647 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.6998279094696045, Accuracy: 0.5878787636756897 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5822752118110657, Accuracy: 0.6000000238418579 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.6820012927055359, Accuracy: 0.6484848260879517 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.456350713968277, Accuracy: 0.6060606241226196 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.7419701814651489, Accuracy: 0.7272727489471436 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.692994236946106, Accuracy: 0.7212121486663818 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.5962520241737366, Accuracy: 0.7696969509124756 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.3935946822166443, Accuracy: 0.7818182110786438 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.5501577258110046, Accuracy: 0.8363636136054993 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.5006453394889832, Accuracy: 0.7939394116401672 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.4818669259548187, Accuracy: 0.842424213886261 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.6670717000961304, Accuracy: 0.8181818127632141 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.3047718405723572, Accuracy: 0.8060606122016907 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.2705218195915222, Accuracy: 0.8484848737716675 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.6247094869613647, Accuracy: 0.842424213886261 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.5249779224395752, Accuracy: 0.8060606122016907 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.22665588557720184, Accuracy: 0.842424213886261 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.2149306833744049, Accuracy: 0.8363636136054993 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.521223247051239, Accuracy: 0.8606060743331909 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.4156918525695801, Accuracy: 0.8363636136054993 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.27309316396713257, Accuracy: 0.9151515364646912 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.6101252436637878, Accuracy: 0.8727272748947144 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.5132195949554443, Accuracy: 0.8303030133247375 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.22462375462055206, Accuracy: 0.8484848737716675 % \t time=0.32s\n",
      "******************** Train ********************\n",
      "Loss: 0.4953829348087311, Accuracy: 0.8363636136054993 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.4799553453922272, Accuracy: 0.8303030133247375 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 1.0274922847747803, Accuracy: 0.8606060743331909 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.3033207952976227, Accuracy: 0.800000011920929 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.19556206464767456, Accuracy: 0.8545454740524292 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.5195893049240112, Accuracy: 0.8969696760177612 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.47913241386413574, Accuracy: 0.8484848737716675 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.3844112157821655, Accuracy: 0.842424213886261 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.13163313269615173, Accuracy: 0.8909090757369995 % \t time=0.30s\n",
      "******************** Train ********************\n",
      "Loss: 0.21070340275764465, Accuracy: 0.8787878751754761 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.2163941115140915, Accuracy: 0.8121212124824524 % \t time=0.24s\n",
      "******************** Train ********************\n",
      "Loss: 0.516783595085144, Accuracy: 0.8545454740524292 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.3344552516937256, Accuracy: 0.8303030133247375 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.14525513350963593, Accuracy: 0.8666666746139526 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.11363925784826279, Accuracy: 0.8727272748947144 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.1877330094575882, Accuracy: 0.8242424130439758 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.5322023630142212, Accuracy: 0.9090909361839294 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.1007605642080307, Accuracy: 0.8969696760177612 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.525795578956604, Accuracy: 0.8848484754562378 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.4238113760948181, Accuracy: 0.8848484754562378 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.2891852557659149, Accuracy: 0.8545454740524292 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.4364989399909973, Accuracy: 0.8060606122016907 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.2019343376159668, Accuracy: 0.9090909361839294 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.1662965714931488, Accuracy: 0.8969696760177612 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 1.4666862487792969, Accuracy: 0.8363636136054993 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.14182142913341522, Accuracy: 0.8909090757369995 % \t time=0.31s\n",
      "******************** Train ********************\n",
      "Loss: 0.9145947694778442, Accuracy: 0.903030276298523 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.07940193265676498, Accuracy: 0.8363636136054993 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.17036917805671692, Accuracy: 0.8666666746139526 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.16900086402893066, Accuracy: 0.9151515364646912 % \t time=0.33s\n",
      "******************** Train ********************\n",
      "Loss: 0.5095900297164917, Accuracy: 0.8787878751754761 % \t time=0.34s\n",
      "******************** Train ********************\n",
      "Loss: 0.15878844261169434, Accuracy: 0.8848484754562378 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.5019715428352356, Accuracy: 0.8545454740524292 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.40426912903785706, Accuracy: 0.8727272748947144 % \t time=0.29s\n",
      "******************** Train ********************\n",
      "Loss: 0.5407168865203857, Accuracy: 0.8727272748947144 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.24202880263328552, Accuracy: 0.8909090757369995 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.2628694176673889, Accuracy: 0.8727272748947144 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.36116042733192444, Accuracy: 0.8242424130439758 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.2258966863155365, Accuracy: 0.8848484754562378 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.24134019017219543, Accuracy: 0.8848484754562378 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.48857277631759644, Accuracy: 0.8303030133247375 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5607336759567261, Accuracy: 0.8909090757369995 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.48611316084861755, Accuracy: 0.8727272748947144 % \t time=0.26s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Train ********************\n",
      "Loss: 0.4875887930393219, Accuracy: 0.8606060743331909 % \t time=0.28s\n",
      "******************** Train ********************\n",
      "Loss: 0.6442869901657104, Accuracy: 0.9090909361839294 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.1508985310792923, Accuracy: 0.8969696760177612 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.517178475856781, Accuracy: 0.9090909361839294 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.39280542731285095, Accuracy: 0.842424213886261 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.6134015321731567, Accuracy: 0.8666666746139526 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.4274056851863861, Accuracy: 0.8666666746139526 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5373755693435669, Accuracy: 0.8787878751754761 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.45477932691574097, Accuracy: 0.8727272748947144 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.1773659586906433, Accuracy: 0.8969696760177612 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.5520422458648682, Accuracy: 0.9090909361839294 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.23437225818634033, Accuracy: 0.9454545378684998 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.16929179430007935, Accuracy: 0.903030276298523 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.518828809261322, Accuracy: 0.9090909361839294 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.04965993016958237, Accuracy: 0.842424213886261 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.520846962928772, Accuracy: 0.8909090757369995 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.14813964068889618, Accuracy: 0.8303030133247375 % \t time=0.27s\n",
      "******************** Train ********************\n",
      "Loss: 0.5396893620491028, Accuracy: 0.8181818127632141 % \t time=0.25s\n",
      "******************** Train ********************\n",
      "Loss: 0.08631910383701324, Accuracy: 0.8787878751754761 % \t time=0.26s\n",
      "******************** Train ********************\n",
      "Loss: 0.15601126849651337, Accuracy: 0.8545454740524292 % \t time=0.25s\n",
      "**********************************************\n",
      "Trn accuracy:0.855 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predict = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    #correct = 0.   # Accuracy\n",
    "    correct_val = 0.\n",
    "        \n",
    "    for batch_idx, trn in enumerate(trn_loader):\n",
    "        trn_X, trn_y = trn['X'], trn['y']\n",
    "        if use_cuda:\n",
    "            trn_X, trn_y = trn_X.cuda(), trn_y.cuda()\n",
    "        trn_X, trn_y = Variable(trn_X).float(), Variable(trn_y).float()\n",
    "        optimizer.zero_grad()\n",
    "        trn_pred = model_2(trn_X)\n",
    "        for i in trn_pred:\n",
    "            predict.append(i.detach().numpy())\n",
    "            \n",
    "        if scheduler:\n",
    "            #print('cycle_LR')\n",
    "            scheduler.batch_step()\n",
    "        #print(trn_pred.squeeze())\n",
    "        #print(trn_y)\n",
    "        trn_loss = criterion(trn_pred.squeeze(), trn_y)\n",
    "        trn_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += trn_loss.item()/len(trn_loader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        predicted = (trn_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        correct_val += (predicted == trn_y).sum()\n",
    "    print('*'*20, 'Train', '*'*20)\n",
    "    print('Loss: {}, Accuracy: {} % \\t time={:.2f}s'.format(trn_loss.item(), correct_val/(len(trn_loader)*batch_size), elapsed_time))\n",
    "print('*'*46)\n",
    "print(\"Trn accuracy:{:.3f} \".format(float(correct_val) / (len(trn_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 1.])\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 0.])\n",
      "tensor([1, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 0.])\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 1., 0., 0.])\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 1., 0.])\n",
      "tensor([0, 0, 0, 1, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 0.])\n",
      "tensor([0, 1, 1, 0, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 1.])\n",
      "tensor([1, 0], dtype=torch.uint8)\n",
      "tensor([1., 1.])\n",
      "******************** Test ********************\n",
      "Loss: 11.427840232849121, Accuracy: 0.6888889074325562 %\n",
      "**********************************************\n",
      "Val accuracy:0.689\n"
     ]
    }
   ],
   "source": [
    "model_1.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict_1 = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict_1.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        #print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        print(val_y)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9.805429e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.976170e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.016732e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.509615e-10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.157224e-12</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.218026e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.021635e-08</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.078228e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.676259e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.964975e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.062897e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.897543e-08</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.992230e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.175650e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.237735e-11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.417114e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.604965e-12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.923767e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.639702e-04</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.618500e-10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.593946e-19</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.990624e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.112814e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.152596e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.351660e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.560944e-13</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.077807e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.550341e-04</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.626122e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.934481e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.680470e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.705988e-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.486762e-06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.476430e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.714285e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.047422e-15</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.289396e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.977102e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.742782e-06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.884452e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.609789e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.793563e-10</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0                \n",
       "9.805429e-01  0.0\n",
       "9.976170e-01  1.0\n",
       "4.016732e-05  0.0\n",
       "1.509615e-10  0.0\n",
       "1.157224e-12  1.0\n",
       "2.218026e-07  0.0\n",
       "2.021635e-08  1.0\n",
       "9.078228e-01  0.0\n",
       "4.676259e-02  0.0\n",
       "3.964975e-07  0.0\n",
       "8.062897e-01  0.0\n",
       "1.897543e-08  0.0\n",
       "9.992230e-01  1.0\n",
       "1.175650e-05  0.0\n",
       "4.237735e-11  0.0\n",
       "2.417114e-01  1.0\n",
       "3.604965e-12  0.0\n",
       "9.923767e-01  1.0\n",
       "9.639702e-04  0.0\n",
       "4.618500e-10  0.0\n",
       "2.593946e-19  0.0\n",
       "9.990624e-01  1.0\n",
       "4.112814e-05  0.0\n",
       "1.152596e-01  1.0\n",
       "1.351660e-01  0.0\n",
       "5.560944e-13  0.0\n",
       "2.077807e-07  0.0\n",
       "1.550341e-04  0.0\n",
       "9.626122e-01  0.0\n",
       "9.934481e-01  1.0\n",
       "4.680470e-01  0.0\n",
       "1.705988e-02  1.0\n",
       "2.486762e-06  0.0\n",
       "3.476430e-07  0.0\n",
       "5.714285e-07  0.0\n",
       "3.047422e-15  0.0\n",
       "7.289396e-01  0.0\n",
       "9.977102e-01  1.0\n",
       "1.742782e-06  0.0\n",
       "8.884452e-01  1.0\n",
       "6.609789e-01  1.0\n",
       "1.793563e-10  1.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict_1)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "#trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\n",
    "pd.DataFrame(val_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  6]\n",
      " [ 5 23]]\n",
      "Accuracy :  0.7380952380952381\n",
      "Sensitivity :  0.5714285714285714\n",
      "Specificity :  0.8214285714285714\n",
      "AUC:  0.6964285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict_1)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(val_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 1, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 0.])\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 0.])\n",
      "tensor([0, 0, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([1., 0., 1., 0., 0.])\n",
      "tensor([0, 1, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 1., 0.])\n",
      "tensor([0, 0, 0, 1, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "tensor([0, 0, 0, 0, 0], dtype=torch.uint8)\n",
      "tensor([0., 1., 0., 0., 0.])\n",
      "tensor([0, 1, 1, 0, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 1.])\n",
      "tensor([0, 0], dtype=torch.uint8)\n",
      "tensor([1., 1.])\n",
      "******************** Test ********************\n",
      "Loss: 12.315314292907715, Accuracy: 0.6888889074325562 %\n",
      "**********************************************\n",
      "Val accuracy:0.689\n"
     ]
    }
   ],
   "source": [
    "model_2.eval()\n",
    "#predict = []\n",
    "#y_val = val_y\n",
    "predict_2 = []\n",
    "\n",
    "correct_val = 0.\n",
    "with torch.no_grad():\n",
    "    val_loss_summary = 0.0\n",
    "    for j, val in enumerate(test_loader):\n",
    "        val_X, val_y = val['X'], val['y']\n",
    "        if use_cuda:\n",
    "            val_X, val_y = val_X.cuda(), val_y.cuda()\n",
    "        val_X, val_y = Variable(val_X).float(), Variable(val_y).float()\n",
    "        val_pred = model(val_X)\n",
    "        for i in val_pred:\n",
    "            predict_2.append(i.numpy())\n",
    "        val_pred = val_pred.type_as(torch.FloatTensor())\n",
    "        #print(val_pred)\n",
    "        val_loss = criterion(val_pred, val_y.unsqueeze(1))\n",
    "        val_loss_summary += val_loss\n",
    "        #predicted = torch.max(val_pred.data, 1)[1]\n",
    "        predicted = (val_pred >= 0.5).flatten().type(torch.ByteTensor)\n",
    "        print(predicted)\n",
    "        print(val_y)\n",
    "        correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(test_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(test_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9.689116e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.968559e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.458268e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.186585e-14</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.359037e-08</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.534286e-06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.988177e-04</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.421953e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.593546e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.231794e-12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.986011e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.235494e-08</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.998287e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.350888e-04</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.620600e-12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.298606e-03</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.159246e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.989016e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.136225e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.071033e-14</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.562101e-12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.999052e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.216958e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.295523e-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.267374e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.084518e-15</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.182267e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.226810e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.507245e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.840326e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.621656e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.295801e-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.685233e-06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.085879e-09</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.036557e-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.561978e-15</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.330527e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.980589e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.118218e-10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.413763e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.670189e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.525099e-11</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0                \n",
       "9.689116e-01  0.0\n",
       "9.968559e-01  1.0\n",
       "7.458268e-05  0.0\n",
       "7.186585e-14  0.0\n",
       "2.359037e-08  1.0\n",
       "2.534286e-06  0.0\n",
       "1.988177e-04  1.0\n",
       "3.421953e-02  0.0\n",
       "9.593546e-01  0.0\n",
       "5.231794e-12  0.0\n",
       "4.986011e-01  0.0\n",
       "2.235494e-08  0.0\n",
       "9.998287e-01  1.0\n",
       "5.350888e-04  0.0\n",
       "6.620600e-12  0.0\n",
       "2.298606e-03  1.0\n",
       "4.159246e-05  0.0\n",
       "9.989016e-01  1.0\n",
       "4.136225e-02  0.0\n",
       "1.071033e-14  0.0\n",
       "7.562101e-12  0.0\n",
       "9.999052e-01  1.0\n",
       "2.216958e-07  0.0\n",
       "2.295523e-02  1.0\n",
       "2.267374e-02  0.0\n",
       "4.084518e-15  0.0\n",
       "2.182267e-07  0.0\n",
       "1.226810e-02  0.0\n",
       "9.507245e-01  0.0\n",
       "9.840326e-01  1.0\n",
       "2.621656e-02  0.0\n",
       "8.295801e-02  1.0\n",
       "7.685233e-06  0.0\n",
       "9.085879e-09  0.0\n",
       "4.036557e-03  0.0\n",
       "5.561978e-15  0.0\n",
       "9.330527e-01  0.0\n",
       "9.980589e-01  1.0\n",
       "9.118218e-10  0.0\n",
       "7.413763e-01  1.0\n",
       "2.670189e-01  1.0\n",
       "7.525099e-11  1.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict_2)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "#trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\n",
    "pd.DataFrame(val_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  7]\n",
      " [ 4 24]]\n",
      "Accuracy :  0.7380952380952381\n",
      "Sensitivity :  0.5\n",
      "Specificity :  0.8571428571428571\n",
      "AUC:  0.6785714285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict_2)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(val_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.9805429], dtype=float32),\n",
       " array([0.997617], dtype=float32),\n",
       " array([4.0167324e-05], dtype=float32),\n",
       " array([1.5096147e-10], dtype=float32),\n",
       " array([1.1572244e-12], dtype=float32),\n",
       " array([2.2180257e-07], dtype=float32),\n",
       " array([2.0216353e-08], dtype=float32),\n",
       " array([0.90782285], dtype=float32),\n",
       " array([0.04676259], dtype=float32),\n",
       " array([3.9649746e-07], dtype=float32),\n",
       " array([0.80628973], dtype=float32),\n",
       " array([1.897543e-08], dtype=float32),\n",
       " array([0.999223], dtype=float32),\n",
       " array([1.1756502e-05], dtype=float32),\n",
       " array([4.237735e-11], dtype=float32),\n",
       " array([0.24171144], dtype=float32),\n",
       " array([3.6049649e-12], dtype=float32),\n",
       " array([0.9923767], dtype=float32),\n",
       " array([0.00096397], dtype=float32),\n",
       " array([4.6184997e-10], dtype=float32),\n",
       " array([2.5939459e-19], dtype=float32),\n",
       " array([0.99906236], dtype=float32),\n",
       " array([4.1128143e-05], dtype=float32),\n",
       " array([0.1152596], dtype=float32),\n",
       " array([0.13516602], dtype=float32),\n",
       " array([5.560944e-13], dtype=float32),\n",
       " array([2.0778066e-07], dtype=float32),\n",
       " array([0.00015503], dtype=float32),\n",
       " array([0.9626122], dtype=float32),\n",
       " array([0.9934481], dtype=float32),\n",
       " array([0.46804696], dtype=float32),\n",
       " array([0.01705988], dtype=float32),\n",
       " array([2.4867618e-06], dtype=float32),\n",
       " array([3.47643e-07], dtype=float32),\n",
       " array([5.714285e-07], dtype=float32),\n",
       " array([3.0474219e-15], dtype=float32),\n",
       " array([0.7289396], dtype=float32),\n",
       " array([0.9977102], dtype=float32),\n",
       " array([1.7427823e-06], dtype=float32),\n",
       " array([0.8884452], dtype=float32),\n",
       " array([0.6609789], dtype=float32),\n",
       " array([1.7935634e-10], dtype=float32)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.9689116], dtype=float32),\n",
       " array([0.99685585], dtype=float32),\n",
       " array([7.4582684e-05], dtype=float32),\n",
       " array([7.186585e-14], dtype=float32),\n",
       " array([2.3590365e-08], dtype=float32),\n",
       " array([2.5342856e-06], dtype=float32),\n",
       " array([0.00019882], dtype=float32),\n",
       " array([0.03421953], dtype=float32),\n",
       " array([0.9593546], dtype=float32),\n",
       " array([5.231794e-12], dtype=float32),\n",
       " array([0.49860108], dtype=float32),\n",
       " array([2.235494e-08], dtype=float32),\n",
       " array([0.9998287], dtype=float32),\n",
       " array([0.00053509], dtype=float32),\n",
       " array([6.6206e-12], dtype=float32),\n",
       " array([0.00229861], dtype=float32),\n",
       " array([4.1592455e-05], dtype=float32),\n",
       " array([0.9989016], dtype=float32),\n",
       " array([0.04136225], dtype=float32),\n",
       " array([1.0710325e-14], dtype=float32),\n",
       " array([7.562101e-12], dtype=float32),\n",
       " array([0.9999052], dtype=float32),\n",
       " array([2.216958e-07], dtype=float32),\n",
       " array([0.02295523], dtype=float32),\n",
       " array([0.02267374], dtype=float32),\n",
       " array([4.084518e-15], dtype=float32),\n",
       " array([2.1822666e-07], dtype=float32),\n",
       " array([0.0122681], dtype=float32),\n",
       " array([0.9507245], dtype=float32),\n",
       " array([0.9840326], dtype=float32),\n",
       " array([0.02621656], dtype=float32),\n",
       " array([0.08295801], dtype=float32),\n",
       " array([7.685233e-06], dtype=float32),\n",
       " array([9.085879e-09], dtype=float32),\n",
       " array([0.00403656], dtype=float32),\n",
       " array([5.5619783e-15], dtype=float32),\n",
       " array([0.93305266], dtype=float32),\n",
       " array([0.99805886], dtype=float32),\n",
       " array([9.118218e-10], dtype=float32),\n",
       " array([0.7413763], dtype=float32),\n",
       " array([0.26701894], dtype=float32),\n",
       " array([7.5250986e-11], dtype=float32)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_3 = predict_1 + predict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'map' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-072387173aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'map' has no len()"
     ]
    }
   ],
   "source": [
    "len(predict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "predict_3 = map(operator.add, predict_1, predict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x7f2206e8ef60>\n"
     ]
    }
   ],
   "source": [
    "print(predict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9747273]\n",
      "[array([0.9747273], dtype=float32)]\n",
      "[0.99723643]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32)]\n",
      "[5.7375004e-05]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32)]\n",
      "[7.551666e-11]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32)]\n",
      "[1.1795761e-08]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32)]\n",
      "[1.3780441e-06]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32)]\n",
      "[9.9418954e-05]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32)]\n",
      "[0.47102118]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32)]\n",
      "[0.50305855]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32)]\n",
      "[1.9825134e-07]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32)]\n",
      "[0.65244544]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32)]\n",
      "[2.0665183e-08]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32)]\n",
      "[0.99952585]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32)]\n",
      "[0.00027342]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32)]\n",
      "[2.4498976e-11]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32)]\n",
      "[0.12200502]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32)]\n",
      "[2.079623e-05]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32)]\n",
      "[0.99563915]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32)]\n",
      "[0.02116311]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32)]\n",
      "[2.3093034e-10]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32)]\n",
      "[3.7810506e-12]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32)]\n",
      "[0.9994838]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32)]\n",
      "[2.067492e-05]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32)]\n",
      "[0.06910741]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32)]\n",
      "[0.07891988]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32)]\n",
      "[2.8008946e-13]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32)]\n",
      "[2.1300366e-07]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32)]\n",
      "[0.00621157]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32)]\n",
      "[0.9566684]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32)]\n",
      "[0.9887403]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32)]\n",
      "[0.24713176]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32)]\n",
      "[0.05000895]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32)]\n",
      "[5.085997e-06]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32)]\n",
      "[1.7836444e-07]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32)]\n",
      "[0.00201856]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32)]\n",
      "[4.3047002e-15]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32)]\n",
      "[0.83099616]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32), array([0.83099616], dtype=float32)]\n",
      "[0.9978845]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32), array([0.83099616], dtype=float32), array([0.9978845], dtype=float32)]\n",
      "[8.71847e-07]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32), array([0.83099616], dtype=float32), array([0.9978845], dtype=float32), array([8.71847e-07], dtype=float32)]\n",
      "[0.81491077]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32), array([0.83099616], dtype=float32), array([0.9978845], dtype=float32), array([8.71847e-07], dtype=float32), array([0.81491077], dtype=float32)]\n",
      "[0.4639989]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32), array([0.83099616], dtype=float32), array([0.9978845], dtype=float32), array([8.71847e-07], dtype=float32), array([0.81491077], dtype=float32), array([0.4639989], dtype=float32)]\n",
      "[1.2730367e-10]\n",
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32), array([0.83099616], dtype=float32), array([0.9978845], dtype=float32), array([8.71847e-07], dtype=float32), array([0.81491077], dtype=float32), array([0.4639989], dtype=float32), array([1.2730367e-10], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "predict = []\n",
    "\n",
    "for i in predict_3:\n",
    "    print (i/2)\n",
    "    predict.append(i/2)\n",
    "    print (predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.9747273], dtype=float32), array([0.99723643], dtype=float32), array([5.7375004e-05], dtype=float32), array([7.551666e-11], dtype=float32), array([1.1795761e-08], dtype=float32), array([1.3780441e-06], dtype=float32), array([9.9418954e-05], dtype=float32), array([0.47102118], dtype=float32), array([0.50305855], dtype=float32), array([1.9825134e-07], dtype=float32), array([0.65244544], dtype=float32), array([2.0665183e-08], dtype=float32), array([0.99952585], dtype=float32), array([0.00027342], dtype=float32), array([2.4498976e-11], dtype=float32), array([0.12200502], dtype=float32), array([2.079623e-05], dtype=float32), array([0.99563915], dtype=float32), array([0.02116311], dtype=float32), array([2.3093034e-10], dtype=float32), array([3.7810506e-12], dtype=float32), array([0.9994838], dtype=float32), array([2.067492e-05], dtype=float32), array([0.06910741], dtype=float32), array([0.07891988], dtype=float32), array([2.8008946e-13], dtype=float32), array([2.1300366e-07], dtype=float32), array([0.00621157], dtype=float32), array([0.9566684], dtype=float32), array([0.9887403], dtype=float32), array([0.24713176], dtype=float32), array([0.05000895], dtype=float32), array([5.085997e-06], dtype=float32), array([1.7836444e-07], dtype=float32), array([0.00201856], dtype=float32), array([4.3047002e-15], dtype=float32), array([0.83099616], dtype=float32), array([0.9978845], dtype=float32), array([8.71847e-07], dtype=float32), array([0.81491077], dtype=float32), array([0.4639989], dtype=float32), array([1.2730367e-10], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predict)\n",
    "type(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_y_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation (Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9.747273e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.972364e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.737500e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.551666e-11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.179576e-08</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.378044e-06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.941895e-05</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.710212e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.030586e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.982513e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.524454e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.066518e-08</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.995258e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.734226e-04</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.449898e-11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.220050e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.079623e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.956391e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.116311e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.309303e-10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.781051e-12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.994838e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.067492e-05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.910741e-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.891988e-02</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.800895e-13</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.130037e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.211566e-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.566684e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.887403e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.471318e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.000895e-02</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.085997e-06</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.783644e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.018564e-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.304700e-15</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.309962e-01</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.978845e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.718470e-07</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.149108e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.639989e-01</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.273037e-10</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0                \n",
       "9.747273e-01  0.0\n",
       "9.972364e-01  1.0\n",
       "5.737500e-05  0.0\n",
       "7.551666e-11  0.0\n",
       "1.179576e-08  1.0\n",
       "1.378044e-06  0.0\n",
       "9.941895e-05  1.0\n",
       "4.710212e-01  0.0\n",
       "5.030586e-01  0.0\n",
       "1.982513e-07  0.0\n",
       "6.524454e-01  0.0\n",
       "2.066518e-08  0.0\n",
       "9.995258e-01  1.0\n",
       "2.734226e-04  0.0\n",
       "2.449898e-11  0.0\n",
       "1.220050e-01  1.0\n",
       "2.079623e-05  0.0\n",
       "9.956391e-01  1.0\n",
       "2.116311e-02  0.0\n",
       "2.309303e-10  0.0\n",
       "3.781051e-12  0.0\n",
       "9.994838e-01  1.0\n",
       "2.067492e-05  0.0\n",
       "6.910741e-02  1.0\n",
       "7.891988e-02  0.0\n",
       "2.800895e-13  0.0\n",
       "2.130037e-07  0.0\n",
       "6.211566e-03  0.0\n",
       "9.566684e-01  0.0\n",
       "9.887403e-01  1.0\n",
       "2.471318e-01  0.0\n",
       "5.000895e-02  1.0\n",
       "5.085997e-06  0.0\n",
       "1.783644e-07  0.0\n",
       "2.018564e-03  0.0\n",
       "4.304700e-15  0.0\n",
       "8.309962e-01  0.0\n",
       "9.978845e-01  1.0\n",
       "8.718470e-07  0.0\n",
       "8.149108e-01  1.0\n",
       "4.639989e-01  1.0\n",
       "1.273037e-10  1.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "#trn_y = torch.from_numpy(trn_y_pd[train_idx.astype(int)].astype(float))\n",
    "pd.DataFrame(val_y_pd, predict_.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  7]\n",
      " [ 5 23]]\n",
      "Accuracy :  0.7142857142857143\n",
      "Sensitivity :  0.5\n",
      "Specificity :  0.8214285714285714\n",
      "AUC:  0.6607142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "predict_ = pd.DataFrame(predict)\n",
    "\n",
    "predict_.iloc[:,0]\n",
    "\n",
    "label = pd.DataFrame(val_y_pd)\n",
    "\n",
    "test_p = pd.concat([predict_.iloc[:,0], label], axis = 1)\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "test_p.columns = ['predict','label']\n",
    "\n",
    "\n",
    "test_p.loc[test_p['predict'] >= 0.5, 'predicted_prob'] = 1\n",
    "test_p.loc[test_p['predict'] < 0.5, 'predicted_prob'] = 0\n",
    "\n",
    "\n",
    "#print(test_p)\n",
    "\n",
    "cnf = confusion_matrix(test_p['label'], test_p['predicted_prob'], labels = [1,0])\n",
    "\n",
    "print(cnf)\n",
    "\n",
    "total1 = sum(sum(cnf))\n",
    "\n",
    "accuracy1=(cnf[0,0]+cnf[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cnf[0,0]/(cnf[0,0]+cnf[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cnf[1,1]/(cnf[1,0]+cnf[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "fpr, trp, _ = roc_curve(test_p['label'], test_p['predicted_prob'])\n",
    "\n",
    "print('AUC: ', auc(fpr, trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y_pd_ = val_y_pd.to_numpy()\n",
    "val_y_pd_ = torch.from_numpy(val_y_pd_)\n",
    "val_y_pd_ = val_y_pd_.type_as(torch.FloatTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = criterion(predict, val_y)\n",
    "val_loss_summary += val_loss\n",
    "predicted = torch.max(val_pred.data, 1)[1] \n",
    "correct_val += (predicted == val_y).sum()\n",
    "\n",
    "print('*'*20, 'Test', '*'*20)\n",
    "print('Loss: {}, Accuracy: {} %'.format(val_loss.item(), correct_val/(len(val_loader)*batch_size)))\n",
    "print('*'*46)\n",
    "print(\"Val accuracy:{:.3f}\".format(float(correct_val) / (len(val_loader)*batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     1.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     1.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    1.0\n",
       "14    0.0\n",
       "15    0.0\n",
       "16    0.0\n",
       "17    1.0\n",
       "18    0.0\n",
       "19    0.0\n",
       "20    0.0\n",
       "21    0.0\n",
       "22    0.0\n",
       "23    0.0\n",
       "24    0.0\n",
       "25    0.0\n",
       "26    1.0\n",
       "27    0.0\n",
       "28    0.0\n",
       "29    1.0\n",
       "30    0.0\n",
       "31    0.0\n",
       "32    0.0\n",
       "33    0.0\n",
       "34    0.0\n",
       "35    0.0\n",
       "36    1.0\n",
       "37    1.0\n",
       "38    0.0\n",
       "39    0.0\n",
       "40    0.0\n",
       "41    0.0\n",
       "42    0.0\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.weight tensor([[-0.0565, -0.1205,  0.0296,  ...,  0.2230, -0.0025,  0.0114],\n",
      "        [-0.0879, -0.0202, -0.0235,  ...,  0.0587,  0.0370, -0.0111],\n",
      "        [-0.0381, -0.0511, -0.0141,  ..., -0.0142,  0.0329,  0.0550],\n",
      "        ...,\n",
      "        [ 0.0349, -0.0490, -0.0410,  ..., -0.0243, -0.0495,  0.0048],\n",
      "        [ 0.0487,  0.1016, -0.0085,  ..., -0.1036,  0.0225, -0.0505],\n",
      "        [-0.0129,  0.1087,  0.0454,  ..., -0.1197,  0.0375,  0.0101]])\n",
      "layer1.0.bias tensor([-0.0296, -0.0339,  0.0066, -0.0166, -0.0347, -0.0489,  0.0233,  0.0351,\n",
      "         0.0348,  0.0116,  0.0404, -0.0197, -0.0154,  0.0126,  0.0236,  0.0361,\n",
      "        -0.0178, -0.0001, -0.0028, -0.0274, -0.0495, -0.0317,  0.0020,  0.0040,\n",
      "         0.0019,  0.0077,  0.0134, -0.0460,  0.0494,  0.0400, -0.0072, -0.0119,\n",
      "         0.0044, -0.0115,  0.0237, -0.0051, -0.0136, -0.0101,  0.0195,  0.0419,\n",
      "        -0.0333, -0.0286,  0.0231, -0.0157,  0.0131,  0.0034, -0.0038,  0.0113,\n",
      "        -0.0342,  0.0022, -0.0403,  0.0045,  0.0368, -0.0412, -0.0265,  0.0073,\n",
      "         0.0281,  0.0489, -0.0224, -0.0353, -0.0270,  0.0293, -0.0182,  0.0238,\n",
      "         0.0492,  0.0268,  0.0054, -0.0453, -0.0368, -0.0456, -0.0042, -0.0114,\n",
      "         0.0184, -0.0176,  0.0122, -0.0008,  0.0443, -0.0175, -0.0142,  0.0456,\n",
      "         0.0210, -0.0015, -0.0053,  0.0494, -0.0356, -0.0446,  0.0353, -0.0413,\n",
      "        -0.0138,  0.0264, -0.0138,  0.0239,  0.0239, -0.0406, -0.0066,  0.0318,\n",
      "        -0.0107,  0.0155, -0.0385, -0.0457, -0.0217, -0.0321,  0.0310, -0.0050,\n",
      "        -0.0381, -0.0156,  0.0157, -0.0302,  0.0393,  0.0008,  0.0229, -0.0228,\n",
      "        -0.0309,  0.0328,  0.0005,  0.0439,  0.0028,  0.0360, -0.0042,  0.0088,\n",
      "        -0.0314, -0.0466, -0.0220, -0.0228,  0.0237,  0.0239, -0.0307,  0.0453,\n",
      "         0.0329,  0.0349, -0.0221, -0.0098,  0.0226,  0.0481,  0.0167,  0.0323,\n",
      "        -0.0268,  0.0228,  0.0485, -0.0443,  0.0335,  0.0312,  0.0197, -0.0286,\n",
      "         0.0466, -0.0496,  0.0195, -0.0330,  0.0122, -0.0012,  0.0474, -0.0334,\n",
      "         0.0289,  0.0292,  0.0017,  0.0107,  0.0175, -0.0181, -0.0057,  0.0273,\n",
      "         0.0200,  0.0319, -0.0083,  0.0171,  0.0426,  0.0274, -0.0409,  0.0243,\n",
      "        -0.0078,  0.0461,  0.0331, -0.0404,  0.0284,  0.0241, -0.0382, -0.0323,\n",
      "        -0.0196, -0.0103, -0.0421,  0.0310, -0.0475, -0.0364, -0.0091, -0.0065,\n",
      "        -0.0075,  0.0139,  0.0052,  0.0198,  0.0273,  0.0429,  0.0285, -0.0189,\n",
      "         0.0003,  0.0293, -0.0074,  0.0034, -0.0308, -0.0453, -0.0052,  0.0491,\n",
      "        -0.0164, -0.0174, -0.0105,  0.0064,  0.0330, -0.0178, -0.0432, -0.0326,\n",
      "        -0.0172,  0.0131, -0.0105,  0.0036, -0.0133, -0.0358,  0.0205,  0.0050,\n",
      "         0.0117,  0.0358, -0.0050, -0.0465,  0.0437,  0.0382, -0.0406,  0.0342,\n",
      "        -0.0039, -0.0102, -0.0305,  0.0462, -0.0189,  0.0109,  0.0156,  0.0486,\n",
      "        -0.0352,  0.0466, -0.0362,  0.0174,  0.0249,  0.0121,  0.0045, -0.0139,\n",
      "        -0.0131, -0.0221,  0.0172, -0.0261, -0.0400,  0.0163, -0.0023, -0.0367,\n",
      "         0.0451, -0.0103,  0.0202, -0.0446, -0.0080,  0.0331, -0.0087, -0.0047])\n",
      "layer1.1.weight tensor([1.0543, 1.0026, 0.9529, 0.9930, 0.9804, 0.9381, 0.9887, 1.0181, 1.3591,\n",
      "        0.9947, 0.9440, 0.9238, 1.0188, 0.9941, 0.9750, 0.9654, 1.0311, 1.0246,\n",
      "        0.9704, 0.9845, 1.0102, 0.9843, 0.9939, 0.9649, 1.0927, 1.1654, 0.9758,\n",
      "        0.9636, 1.0126, 0.9536, 1.0806, 0.9525, 0.9579, 0.9856, 0.9407, 0.9693,\n",
      "        0.9402, 1.0998, 0.9559, 0.9616, 1.0285, 0.9483, 1.0569, 0.9598, 0.9758,\n",
      "        0.9606, 0.9784, 0.9570, 0.9713, 0.9745, 1.0181, 1.0555, 0.9997, 0.9530,\n",
      "        0.9576, 0.9488, 0.9400, 0.9601, 0.9809, 0.9516, 0.9940, 0.9563, 1.0968,\n",
      "        0.9425, 0.9743, 1.0133, 0.9653, 1.1313, 1.0063, 0.9513, 1.2053, 0.9754,\n",
      "        0.9650, 0.9459, 0.9965, 0.9641, 0.9772, 1.0243, 0.9510, 0.9320, 0.9907,\n",
      "        0.9788, 1.0122, 1.0071, 0.9641, 0.9579, 1.1537, 0.9829, 0.9348, 1.0536,\n",
      "        0.9752, 0.9476, 0.9320, 1.0961, 0.9682, 0.9551, 1.0997, 0.9738, 0.9502,\n",
      "        0.9723, 0.9532, 0.9421, 1.0103, 0.9597, 1.0276, 0.9726, 0.9488, 0.9493,\n",
      "        0.9718, 0.9464, 0.9359, 1.0518, 0.9688, 0.9973, 0.9829, 0.9552, 0.9926,\n",
      "        0.9318, 1.1189, 0.9734, 1.0136, 1.0211, 1.1543, 1.0242, 0.9966, 0.9609,\n",
      "        0.9532, 0.9632, 0.9638, 0.9995, 1.0706, 1.2656, 1.0072, 1.0146, 1.0994,\n",
      "        0.9740, 0.9531, 1.0132, 0.9401, 1.0987, 0.9823, 1.0166, 1.0076, 0.9821,\n",
      "        0.9629, 0.9700, 1.0205, 0.9578, 1.0034, 0.9603, 0.9256, 0.9741, 0.9436,\n",
      "        1.1408, 0.9683, 0.9890, 0.9576, 0.9858, 0.9450, 0.9762, 0.9716, 0.9857,\n",
      "        0.9722, 0.9662, 1.1388, 1.0447, 0.9543, 0.9899, 0.9534, 1.0293, 0.9659,\n",
      "        0.9652, 1.0391, 0.9915, 0.9734, 1.0190, 0.9628, 0.9587, 0.9835, 0.9557,\n",
      "        0.9662, 0.9700, 0.9721, 0.9751, 0.9466, 1.0051, 0.9608, 0.9418, 0.9616,\n",
      "        0.9540, 0.9386, 0.9793, 0.9455, 0.9494, 0.9596, 0.9577, 1.1870, 0.9387,\n",
      "        1.0832, 0.9497, 1.0929, 0.9469, 0.9594, 0.9605, 1.0027, 0.9616, 0.9803,\n",
      "        0.9703, 1.0022, 0.9896, 0.9997, 1.0361, 0.9539, 0.9467, 0.9919, 0.9524,\n",
      "        0.9626, 0.9502, 0.9668, 1.1326, 0.9875, 1.0050, 0.9409, 1.0002, 0.9510,\n",
      "        0.9608, 0.9515, 0.9672, 0.9675, 1.0145, 1.0126, 1.0117, 0.9585, 0.9675,\n",
      "        0.9777, 0.9710, 0.9834, 1.1106, 0.9546, 0.9559, 0.9482, 0.9359, 0.9625,\n",
      "        0.9448, 0.9875, 0.9419, 0.9797, 0.9734, 1.0407, 0.9971, 0.9410, 0.9882,\n",
      "        0.9700, 0.9617, 0.9896, 0.9808])\n",
      "layer1.1.bias tensor([ 0.0453,  0.0241, -0.0505, -0.0232, -0.0577, -0.0721,  0.0315, -0.0061,\n",
      "         0.0808,  0.0042, -0.0511, -0.0356,  0.0082,  0.0235, -0.0111,  0.0091,\n",
      "        -0.0469,  0.0067, -0.0420, -0.0269, -0.0116, -0.0002, -0.0145, -0.0298,\n",
      "         0.0468,  0.0831, -0.0057, -0.0144,  0.0346, -0.0472,  0.0046, -0.0278,\n",
      "        -0.0359, -0.0035, -0.0414, -0.0530, -0.0214,  0.0057, -0.0362, -0.0383,\n",
      "         0.0049, -0.0249,  0.0879, -0.0336,  0.0092, -0.0274, -0.0124, -0.0407,\n",
      "         0.0249, -0.0224,  0.0245, -0.0011, -0.0500, -0.0545, -0.0306, -0.0362,\n",
      "        -0.0262, -0.0363, -0.0222, -0.0485, -0.0457, -0.0340,  0.0623, -0.0656,\n",
      "        -0.0472, -0.0040,  0.0202,  0.0939,  0.0504, -0.0124,  0.1581, -0.0275,\n",
      "        -0.0311, -0.0313, -0.0377, -0.0105, -0.0219,  0.0396, -0.0215, -0.0541,\n",
      "         0.0313, -0.0159,  0.0051, -0.0322, -0.0650, -0.0638,  0.1269,  0.0325,\n",
      "        -0.0127,  0.0025, -0.0383, -0.0019, -0.0304,  0.0603, -0.0517, -0.0490,\n",
      "         0.0615, -0.0521, -0.0420, -0.0156, -0.0182, -0.0318, -0.0394, -0.0332,\n",
      "         0.0085, -0.0176, -0.0462, -0.0274,  0.0113, -0.0159, -0.0646,  0.0320,\n",
      "        -0.0337, -0.0111, -0.0506, -0.0191, -0.0149, -0.0349,  0.0042, -0.0751,\n",
      "         0.0237,  0.0079,  0.0560,  0.0476, -0.0440, -0.0315, -0.0526, -0.0280,\n",
      "        -0.0255, -0.0052,  0.0083,  0.1634, -0.0207,  0.0299,  0.0531,  0.0486,\n",
      "        -0.0296, -0.0142, -0.0345,  0.0608, -0.0170, -0.0334,  0.0234, -0.0233,\n",
      "        -0.0080, -0.0628, -0.0103, -0.0211, -0.0157, -0.0268, -0.0607, -0.0041,\n",
      "        -0.0442,  0.0705,  0.0504, -0.0203, -0.0289, -0.0262, -0.0366, -0.0557,\n",
      "        -0.0434,  0.0295, -0.0221, -0.0399,  0.1607,  0.0627, -0.0258,  0.0372,\n",
      "        -0.0028,  0.0355, -0.0461, -0.0278,  0.0041, -0.0111, -0.0156, -0.0463,\n",
      "        -0.0400, -0.0635, -0.0304, -0.0536, -0.0209, -0.0195, -0.0341, -0.0342,\n",
      "        -0.0113, -0.0032, -0.0328, -0.0393, -0.0565,  0.0103, -0.0485, -0.0362,\n",
      "         0.0448, -0.0292, -0.0124, -0.0273,  0.1589, -0.0610,  0.0671, -0.0247,\n",
      "         0.0505, -0.0319, -0.0130, -0.0454,  0.0191, -0.0244, -0.0111, -0.0171,\n",
      "        -0.0051, -0.0271, -0.0062,  0.0734, -0.0318, -0.0160, -0.0424, -0.0499,\n",
      "        -0.0272, -0.0263,  0.0187,  0.1053, -0.0019, -0.0242, -0.0582,  0.0288,\n",
      "        -0.0535, -0.0423, -0.0635, -0.0212, -0.0191,  0.0121,  0.0152, -0.0035,\n",
      "        -0.0330, -0.0343, -0.0404, -0.0321, -0.0270,  0.0617, -0.0283, -0.0453,\n",
      "        -0.0607, -0.0292, -0.0565, -0.0464, -0.0083, -0.0198, -0.0132, -0.0766,\n",
      "         0.0538, -0.0385, -0.0541, -0.0003, -0.0083, -0.0426, -0.0276, -0.0277])\n",
      "layer2.0.weight tensor([[-0.0163,  0.0263,  0.0218,  ..., -0.0967,  0.0156,  0.0105],\n",
      "        [-0.0522, -0.0449,  0.0215,  ..., -0.0324, -0.0184,  0.0182],\n",
      "        [ 0.0422,  0.0070,  0.0411,  ...,  0.0558,  0.0299,  0.0019],\n",
      "        ...,\n",
      "        [ 0.0493, -0.0322, -0.0337,  ...,  0.0173,  0.0147, -0.0171],\n",
      "        [-0.0296, -0.0210, -0.0163,  ...,  0.0227,  0.0137, -0.0562],\n",
      "        [ 0.0282,  0.0394,  0.0259,  ...,  0.0094,  0.0067,  0.0486]])\n",
      "layer2.0.bias tensor([-0.0604, -0.0613,  0.0348,  0.0436,  0.0475,  0.0515,  0.0059, -0.0128,\n",
      "         0.0378, -0.0207,  0.0542,  0.0261,  0.0211, -0.0375, -0.0479,  0.0064,\n",
      "         0.0352, -0.0484,  0.0122,  0.0089,  0.0133,  0.0003,  0.0068, -0.0451,\n",
      "         0.0169, -0.0488,  0.0247, -0.0605, -0.0118,  0.0411, -0.0161,  0.0565,\n",
      "         0.0277, -0.0389, -0.0478,  0.0508,  0.0215, -0.0234,  0.0101,  0.0033,\n",
      "        -0.0417, -0.0411,  0.0051,  0.0259,  0.0112,  0.0247,  0.0465, -0.0558,\n",
      "         0.0354, -0.0163, -0.0328, -0.0269, -0.0410, -0.0317,  0.0026, -0.0495,\n",
      "         0.0258, -0.0097,  0.0296,  0.0282,  0.0211,  0.0254,  0.0217, -0.0031,\n",
      "         0.0474, -0.0321, -0.0401, -0.0512,  0.0529,  0.0175, -0.0365,  0.0139,\n",
      "         0.0285, -0.0031,  0.0229,  0.0554, -0.0087, -0.0225,  0.0534,  0.0102,\n",
      "        -0.0117,  0.0120, -0.0551,  0.0136,  0.0420,  0.0340, -0.0566,  0.0161,\n",
      "        -0.0286,  0.0448,  0.0400,  0.0561,  0.0053, -0.0277, -0.0146,  0.0201,\n",
      "         0.0278,  0.0577,  0.0026,  0.0097, -0.0232, -0.0103,  0.0562, -0.0150,\n",
      "         0.0318,  0.0545, -0.0133,  0.0257,  0.0521, -0.0492, -0.0415,  0.0575,\n",
      "         0.0273, -0.0348,  0.0407,  0.0224, -0.0134,  0.0508,  0.0162, -0.0108,\n",
      "        -0.0385, -0.0458, -0.0352, -0.0646,  0.0572,  0.0406,  0.0331,  0.0130])\n",
      "layer2.1.weight tensor([0.9613, 1.0475, 1.0010, 0.9392, 1.0344, 0.9671, 1.0034, 1.1226, 0.9615,\n",
      "        1.0508, 0.9742, 1.0172, 0.9995, 0.9541, 0.9533, 0.9548, 0.9846, 1.1078,\n",
      "        1.1053, 1.0107, 0.9783, 0.9653, 1.0068, 1.0774, 0.9704, 1.0177, 0.9810,\n",
      "        0.9845, 0.9522, 1.0674, 0.9781, 0.9977, 0.9818, 1.0031, 0.9531, 1.0444,\n",
      "        1.0160, 1.0373, 1.0434, 0.9564, 1.0378, 0.9935, 1.0403, 0.9438, 0.9588,\n",
      "        0.9968, 0.9936, 1.0114, 1.0069, 0.9975, 1.0479, 1.0201, 0.9579, 1.0324,\n",
      "        1.0037, 0.9983, 0.9907, 1.0427, 0.9711, 0.9804, 1.0222, 0.9770, 1.0041,\n",
      "        0.9864, 0.9818, 0.9704, 0.9547, 0.9659, 1.0392, 1.0374, 0.9591, 0.9856,\n",
      "        1.0427, 0.9606, 0.9982, 0.9646, 1.0630, 0.9772, 0.9811, 0.9855, 0.9293,\n",
      "        1.0613, 1.0292, 1.0194, 1.0508, 0.9637, 1.1035, 0.9961, 1.0145, 1.0209,\n",
      "        0.9505, 1.0620, 0.9405, 0.9742, 0.9636, 1.0114, 1.0882, 1.0284, 1.0017,\n",
      "        1.0713, 0.9807, 1.0086, 0.9554, 1.0239, 1.0211, 0.9505, 1.0052, 1.0249,\n",
      "        0.9842, 1.0019, 0.9609, 1.0182, 0.9800, 1.0274, 1.0270, 0.9745, 1.0085,\n",
      "        0.9902, 0.9659, 0.9994, 1.0481, 0.9465, 0.9692, 1.0124, 1.0340, 0.9958,\n",
      "        0.9650, 0.9584])\n",
      "layer2.1.bias tensor([-3.3632e-02, -1.0531e-02, -3.9815e-02, -5.3454e-02,  3.4553e-03,\n",
      "        -4.2636e-02, -3.5241e-02,  6.7848e-02, -2.7361e-02,  2.4047e-02,\n",
      "        -1.6313e-02, -2.8308e-02, -3.9380e-02, -1.7013e-02, -1.2474e-02,\n",
      "        -1.6568e-02,  2.9685e-02,  5.9898e-02,  8.8914e-02,  8.4402e-03,\n",
      "        -3.1797e-02, -2.5944e-02,  1.3087e-02,  3.0514e-02, -2.9099e-02,\n",
      "        -3.4053e-02, -1.0561e-02, -3.3698e-02, -1.6890e-02, -2.2441e-02,\n",
      "        -2.8737e-02,  1.4771e-02,  1.2317e-03, -3.8425e-03, -6.0187e-03,\n",
      "        -2.0330e-02,  1.3631e-03,  1.1908e-03, -4.2749e-02, -1.9610e-02,\n",
      "        -5.1234e-03, -1.1312e-02,  5.1386e-02, -2.8814e-02, -5.9600e-02,\n",
      "        -6.9483e-03,  2.6689e-03, -1.9948e-02, -2.7281e-02,  1.8556e-02,\n",
      "        -2.7163e-02, -7.9423e-02, -5.0906e-02, -1.5404e-02, -3.7304e-02,\n",
      "        -4.5451e-03, -6.8602e-03, -3.7263e-02, -1.3598e-02,  8.3878e-03,\n",
      "         1.0625e-02, -1.2431e-02,  2.4046e-03,  8.8591e-03,  3.5623e-03,\n",
      "        -2.1075e-02, -5.0911e-02,  3.3220e-03,  7.3397e-03,  1.2624e-02,\n",
      "        -2.7113e-02, -9.7818e-03,  3.8295e-02, -3.5996e-02, -4.4027e-03,\n",
      "        -6.1831e-03,  7.9379e-05, -6.9035e-03, -3.7248e-03, -2.9304e-02,\n",
      "        -3.3609e-02, -4.9656e-02,  2.8775e-02,  2.0694e-02,  3.1142e-02,\n",
      "        -3.4996e-02, -5.2912e-02, -2.7226e-02, -8.7165e-03, -7.9831e-02,\n",
      "        -3.4923e-02,  1.6893e-02, -1.9217e-02, -1.2025e-04, -2.4480e-02,\n",
      "         8.3395e-03, -2.0882e-02,  7.1040e-02,  9.8776e-03,  8.4546e-02,\n",
      "        -4.6718e-03, -1.2601e-02, -3.4672e-02,  4.7202e-02, -7.7692e-03,\n",
      "        -3.0968e-02, -2.4136e-02,  4.0197e-02,  1.8163e-03, -8.8145e-03,\n",
      "        -4.8762e-02, -2.5213e-02, -1.8892e-02, -3.4188e-02,  1.4136e-02,\n",
      "        -9.4315e-03, -7.7451e-03,  7.3673e-03,  2.9620e-03, -8.7451e-03,\n",
      "         5.1112e-02, -2.6812e-02,  1.3600e-02,  1.1149e-02, -6.7495e-03,\n",
      "        -9.1601e-03, -4.1967e-02, -2.4559e-02])\n",
      "layer3.0.weight tensor([[-0.0475,  0.0049, -0.0305,  ..., -0.0959, -0.0193, -0.0498],\n",
      "        [ 0.0476,  0.0125, -0.0632,  ...,  0.0559, -0.0873, -0.0563],\n",
      "        [-0.0611,  0.0562,  0.0944,  ...,  0.0271, -0.0053,  0.0391],\n",
      "        ...,\n",
      "        [-0.0332,  0.1086, -0.0952,  ...,  0.0282, -0.0768, -0.0400],\n",
      "        [-0.0533,  0.0318,  0.0123,  ...,  0.0516, -0.0466,  0.0648],\n",
      "        [-0.0028, -0.0263, -0.0079,  ...,  0.0050, -0.0584, -0.0270]])\n",
      "layer3.0.bias tensor([-0.0864,  0.0066,  0.0272,  0.0414,  0.0265, -0.0724,  0.0081, -0.0026,\n",
      "         0.0090, -0.0017, -0.0632,  0.0474,  0.0010,  0.0513, -0.0701,  0.0224,\n",
      "         0.0710,  0.0793,  0.0045, -0.0487,  0.0904, -0.0718, -0.0645,  0.0604,\n",
      "         0.0616,  0.0063, -0.0180, -0.0622, -0.0654, -0.0706,  0.0485,  0.0171,\n",
      "         0.0422,  0.0428, -0.0145, -0.0604, -0.0029,  0.0437, -0.0610, -0.0750,\n",
      "        -0.0723,  0.0333,  0.0454,  0.0449, -0.0565, -0.0309,  0.0858, -0.0418,\n",
      "         0.0134, -0.0809,  0.0819, -0.0303, -0.0396,  0.0419, -0.0036, -0.0205,\n",
      "         0.0124,  0.0358,  0.0826,  0.0011, -0.0844,  0.0698,  0.0577,  0.0164])\n",
      "layer3.1.weight tensor([1.0830, 0.9978, 0.9785, 1.2189, 1.0479, 1.2179, 1.1423, 1.0221, 1.2209,\n",
      "        1.1106, 1.1230, 1.1783, 1.2140, 1.0997, 1.1747, 1.1660, 0.9775, 1.1345,\n",
      "        1.2364, 1.2392, 1.1082, 1.0455, 1.0523, 1.1933, 1.1867, 1.2126, 1.1887,\n",
      "        1.1772, 1.1870, 1.1890, 1.2267, 1.1849, 1.1846, 1.1480, 1.2154, 1.0959,\n",
      "        1.1951, 1.0215, 1.2021, 1.2693, 1.2102, 1.0501, 1.2439, 1.1928, 1.1395,\n",
      "        1.0154, 1.2521, 1.2279, 1.2349, 1.1956, 1.0306, 1.2408, 1.0883, 1.0951,\n",
      "        1.0982, 1.1461, 1.1568, 1.1664, 1.1719, 1.1948, 1.0316, 1.2075, 1.1764,\n",
      "        1.1313])\n",
      "layer3.1.bias tensor([ 0.0915, -0.0282, -0.0143,  0.2714,  0.0145,  0.2964,  0.2684,  0.0075,\n",
      "         0.2234,  0.1543,  0.1794,  0.1890,  0.2182,  0.0831,  0.2497,  0.1692,\n",
      "        -0.0144,  0.1800,  0.2716,  0.2873,  0.1167,  0.1014,  0.1085,  0.2057,\n",
      "         0.1829,  0.2182,  0.1975,  0.2033,  0.2191,  0.2082,  0.2647,  0.1941,\n",
      "         0.2544,  0.1868,  0.2324,  0.1946,  0.2205,  0.0303,  0.2813,  0.2636,\n",
      "         0.2980,  0.0259,  0.2997,  0.2090,  0.1250, -0.0026,  0.2421,  0.2573,\n",
      "         0.2519,  0.2493,  0.0357,  0.2008,  0.0534,  0.0487,  0.1670,  0.2083,\n",
      "         0.2069,  0.2459,  0.1904,  0.2156,  0.0678,  0.1993,  0.2226,  0.1382])\n",
      "layer4.0.weight tensor([[ 0.1443, -0.1018, -0.1090, -0.0516,  0.1599,  0.0069, -0.0503,  0.1055,\n",
      "         -0.0097, -0.0815, -0.1210, -0.1320,  0.0117,  0.1893,  0.0297, -0.0609,\n",
      "         -0.0963, -0.0788,  0.0810,  0.0670,  0.1692,  0.0684,  0.0730, -0.0411,\n",
      "          0.1759,  0.1943,  0.1316, -0.0388,  0.0772,  0.0503, -0.0200,  0.0197,\n",
      "         -0.0622, -0.0458, -0.0191,  0.0823,  0.1046,  0.1087, -0.0157,  0.0328,\n",
      "          0.0674, -0.1792,  0.0546, -0.0750,  0.2092,  0.0876,  0.1790, -0.0397,\n",
      "          0.1453,  0.0945,  0.0643,  0.1934,  0.0981,  0.1405,  0.0046, -0.1102,\n",
      "         -0.1080, -0.1053,  0.0836, -0.0286,  0.1281,  0.1765,  0.1460,  0.1653],\n",
      "        [ 0.0217,  0.0608,  0.0180,  0.1182, -0.0015,  0.2358,  0.0999, -0.0263,\n",
      "          0.2130,  0.0464,  0.1344,  0.1936,  0.1836,  0.0360,  0.2525,  0.0794,\n",
      "          0.0240,  0.2377,  0.0799,  0.0862,  0.0153,  0.0383,  0.0203,  0.2627,\n",
      "          0.0624,  0.1321,  0.2048,  0.1786,  0.1144,  0.1500,  0.1790,  0.2333,\n",
      "          0.1601,  0.2412,  0.1343,  0.0168,  0.2571, -0.0316,  0.1852,  0.1524,\n",
      "          0.1819,  0.0625,  0.1196,  0.2069,  0.0415, -0.0076,  0.1371,  0.1301,\n",
      "          0.1420,  0.1877,  0.0073,  0.0249, -0.0140, -0.0378,  0.0512,  0.2081,\n",
      "          0.1792,  0.1329,  0.2212,  0.1911,  0.0125,  0.0667,  0.0923,  0.0203],\n",
      "        [-0.2432, -0.0389, -0.0607, -0.2514, -0.1295, -0.0640, -0.2343, -0.1205,\n",
      "         -0.0664, -0.1287, -0.1257, -0.1201, -0.1826, -0.1296, -0.0727, -0.2364,\n",
      "         -0.0779, -0.0961, -0.2272, -0.1283, -0.1555, -0.1959,  0.0120, -0.0102,\n",
      "         -0.2502, -0.1883, -0.1268, -0.2563, -0.1923, -0.1318, -0.0212,  0.0008,\n",
      "         -0.2428, -0.1623, -0.0560, -0.2609, -0.1249, -0.1706, -0.0733, -0.0552,\n",
      "         -0.0518,  0.0199, -0.2394, -0.1281, -0.1758, -0.1558, -0.2302, -0.2144,\n",
      "         -0.1794, -0.1492, -0.1079, -0.1032, -0.1590, -0.1594, -0.1735, -0.0209,\n",
      "         -0.0855, -0.0201, -0.2071, -0.0804, -0.0605, -0.2281, -0.1177, -0.1835],\n",
      "        [-0.1199, -0.0040, -0.0171, -0.1126, -0.0389, -0.1439, -0.0756, -0.2350,\n",
      "         -0.2176, -0.1180, -0.2137, -0.1639, -0.2511, -0.1544, -0.1507, -0.2416,\n",
      "         -0.1334, -0.2039, -0.2148, -0.1279, -0.2169, -0.0966, -0.2507, -0.1037,\n",
      "         -0.0338, -0.2607, -0.0781, -0.0964, -0.0657, -0.1267, -0.2049, -0.1176,\n",
      "         -0.2286, -0.0985, -0.2631, -0.1282, -0.1944, -0.1586, -0.0471, -0.1824,\n",
      "         -0.1434, -0.1002, -0.1454, -0.1895, -0.2000, -0.0785, -0.0276, -0.0718,\n",
      "         -0.1264, -0.1293, -0.2575, -0.0849, -0.1897, -0.1408, -0.1570, -0.2139,\n",
      "         -0.1875, -0.2081, -0.0471, -0.0203, -0.2226, -0.1819, -0.1165, -0.1302],\n",
      "        [-0.1802, -0.0633, -0.0637, -0.2384, -0.0146, -0.0645, -0.0368, -0.2012,\n",
      "         -0.0591, -0.1984, -0.2079, -0.0333, -0.1293, -0.0126, -0.0276, -0.2091,\n",
      "         -0.0411, -0.1115, -0.1910, -0.1666, -0.2572, -0.1108, -0.0301, -0.0069,\n",
      "         -0.0419, -0.1471, -0.2321, -0.2270, -0.2011, -0.0748, -0.2039, -0.1080,\n",
      "         -0.1286, -0.0987, -0.0976, -0.1672, -0.1203, -0.1282, -0.0689, -0.0554,\n",
      "         -0.1317, -0.0058, -0.0384,  0.0061, -0.0375, -0.0419, -0.0470, -0.1376,\n",
      "         -0.1174, -0.2775, -0.0460, -0.2670, -0.0307, -0.2332, -0.2090, -0.1205,\n",
      "          0.0200, -0.2760, -0.0058, -0.1297, -0.1080, -0.1665, -0.2090, -0.2020],\n",
      "        [-0.1231, -0.0062, -0.0950, -0.1043, -0.2140,  0.0014, -0.0336, -0.0853,\n",
      "         -0.2615, -0.0879, -0.0022, -0.0751, -0.2024, -0.1064, -0.2189, -0.1478,\n",
      "         -0.0348, -0.0073, -0.2251, -0.0484, -0.2678, -0.2492, -0.2124, -0.1391,\n",
      "         -0.1951, -0.1726, -0.2075, -0.2298, -0.0884, -0.1018, -0.2566, -0.0414,\n",
      "          0.0069, -0.0555, -0.0833, -0.1223, -0.1051, -0.0751, -0.0328, -0.2043,\n",
      "         -0.1451, -0.0688, -0.1671, -0.2221, -0.1884, -0.1681, -0.2421, -0.1808,\n",
      "         -0.0997, -0.1051, -0.0809, -0.2211, -0.1743, -0.1355, -0.2134, -0.1541,\n",
      "         -0.1121, -0.0627, -0.2451, -0.0142, -0.1448, -0.0595, -0.0637, -0.2183],\n",
      "        [-0.1407, -0.0914, -0.0005, -0.1951, -0.0161, -0.0542, -0.1097, -0.0301,\n",
      "         -0.0765, -0.1972, -0.0196, -0.2088, -0.1064, -0.1356, -0.0803, -0.0849,\n",
      "          0.0189, -0.0439, -0.0596, -0.1871, -0.1199, -0.0753, -0.2501, -0.2427,\n",
      "         -0.1096, -0.1046, -0.0102, -0.1465, -0.1557, -0.1363, -0.1542, -0.0851,\n",
      "         -0.1840, -0.1841, -0.2426, -0.1027, -0.0040, -0.0706, -0.2566, -0.1660,\n",
      "         -0.1931, -0.1065, -0.1912, -0.1641, -0.2157, -0.0099, -0.2577, -0.0814,\n",
      "         -0.1111, -0.1409, -0.0185, -0.2288, -0.1723, -0.0902, -0.0752, -0.0525,\n",
      "         -0.0377, -0.0278, -0.0338, -0.1065, -0.1842, -0.1288, -0.2489, -0.2255],\n",
      "        [-0.2539, -0.1380, -0.1654, -0.0267, -0.1212, -0.1274, -0.2463, -0.0753,\n",
      "         -0.2290, -0.0399, -0.0709, -0.2486, -0.1969, -0.0137, -0.2001, -0.2162,\n",
      "         -0.0443, -0.2057, -0.1781, -0.2061, -0.1792, -0.2439, -0.1128, -0.0377,\n",
      "         -0.1224, -0.2723, -0.1398, -0.1423, -0.1756, -0.0747, -0.2065, -0.2079,\n",
      "         -0.0225, -0.0912, -0.2301, -0.0589, -0.1865, -0.2143, -0.0564, -0.1112,\n",
      "         -0.0200, -0.0838, -0.1945, -0.0198, -0.2092, -0.0651, -0.0361, -0.1678,\n",
      "         -0.1863, -0.1500, -0.2503, -0.2740, -0.1245, -0.0233, -0.0282, -0.1324,\n",
      "         -0.1591, -0.2734, -0.0080, -0.2291, -0.2594, -0.1609, -0.1763, -0.2036],\n",
      "        [-0.0516, -0.0618, -0.0515, -0.1324, -0.0758, -0.0121, -0.1110,  0.0086,\n",
      "         -0.0043, -0.2348, -0.0616, -0.0385, -0.1168, -0.0838, -0.0131, -0.1502,\n",
      "         -0.0839,  0.0026, -0.1464, -0.2231, -0.0360,  0.0006, -0.1124, -0.0058,\n",
      "         -0.1015, -0.0259, -0.2408, -0.1184, -0.2530, -0.2854, -0.0473, -0.2345,\n",
      "         -0.1317, -0.1904, -0.1330, -0.0268, -0.0028, -0.0418, -0.2312, -0.1631,\n",
      "         -0.1685, -0.0413, -0.1153, -0.2217, -0.2219, -0.0316, -0.1949, -0.1124,\n",
      "         -0.0946, -0.2711, -0.2002, -0.2090, -0.0699, -0.1576, -0.0864, -0.1298,\n",
      "         -0.0196, -0.0162, -0.0367, -0.1147, -0.1285, -0.0883, -0.0282, -0.1284],\n",
      "        [-0.2205, -0.1128, -0.2114, -0.1163, -0.0732, -0.0474, -0.0949, -0.1668,\n",
      "         -0.2371, -0.2560, -0.0541, -0.0704, -0.0962, -0.2574, -0.2749, -0.0283,\n",
      "         -0.0677, -0.2434, -0.1809, -0.0916, -0.2321, -0.1687, -0.0626, -0.1199,\n",
      "         -0.2002, -0.1164, -0.2182, -0.1223, -0.2658, -0.0749, -0.0277, -0.2505,\n",
      "         -0.1637, -0.1010, -0.2258, -0.1584, -0.0216, -0.0381, -0.0874, -0.1867,\n",
      "         -0.2840, -0.1811, -0.1091, -0.1530, -0.0389, -0.1132, -0.0818, -0.1548,\n",
      "         -0.0786, -0.2039, -0.0594, -0.2796, -0.2199, -0.0509, -0.0241, -0.0649,\n",
      "         -0.0618, -0.0783, -0.1577, -0.0774, -0.0268, -0.0769, -0.2652, -0.1091]])\n",
      "layer4.0.bias tensor([ 0.1278,  0.0571, -0.1977, -0.2206, -0.1804, -0.0446, -0.1893, -0.0693,\n",
      "        -0.1943, -0.1235])\n"
     ]
    }
   ],
   "source": [
    "#num_ftrs = model.forward.in_features\n",
    " \n",
    "for name, param in model.named_parameters():\n",
    "    print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-cd914773920e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.layer1 = Sequential(\n\u001b[0m\u001b[1;32m      2\u001b[0m                       Linear(4096, 2))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model.layer1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'out_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-d6ee396a1192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'out_features'"
     ]
    }
   ],
   "source": [
    "nn.Linear(num_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'parameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ea8074fb84b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'parameter'"
     ]
    }
   ],
   "source": [
    "list(model.children())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-45e4dc8164de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-76-3c96f51ab486>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLPRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mh3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = MLPRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-d5aee46402fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-558e02a0941d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrn_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_X' is not defined"
     ]
    }
   ],
   "source": [
    "trn = data_utils.TensorDataset(trn_X, trn_y)\n",
    "trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val = data_utils.TensorDataset(val_X, val_y)\n",
    "val_loader = data_utils.DataLoader(val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X_pd = df2_mod.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ID</th>\n",
       "      <th>ENSG00000069482</th>\n",
       "      <th>ENSG00000072657</th>\n",
       "      <th>ENSG00000078399</th>\n",
       "      <th>ENSG00000080572</th>\n",
       "      <th>ENSG00000100678</th>\n",
       "      <th>ENSG00000104435</th>\n",
       "      <th>ENSG00000104888</th>\n",
       "      <th>ENSG00000105146</th>\n",
       "      <th>ENSG00000109321</th>\n",
       "      <th>ENSG00000112499</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000274576</th>\n",
       "      <th>ENSG00000275756</th>\n",
       "      <th>ENSG00000276775</th>\n",
       "      <th>ENSG00000277247</th>\n",
       "      <th>ENSG00000278196</th>\n",
       "      <th>ENSG00000278698</th>\n",
       "      <th>ENSG00000279834</th>\n",
       "      <th>ENSG00000279970</th>\n",
       "      <th>ENSG00000280411</th>\n",
       "      <th>ENSG00000281880</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-61-1910</th>\n",
       "      <td>0.970585</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.044476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043732</td>\n",
       "      <td>2.308592</td>\n",
       "      <td>1.014494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.604301</td>\n",
       "      <td>0.299376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894232</td>\n",
       "      <td>1.612154</td>\n",
       "      <td>0.761720</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-0982</th>\n",
       "      <td>0.395276</td>\n",
       "      <td>0.005010</td>\n",
       "      <td>0.090384</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.082034</td>\n",
       "      <td>0.242661</td>\n",
       "      <td>0.691196</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>...</td>\n",
       "      <td>24.797152</td>\n",
       "      <td>0.259035</td>\n",
       "      <td>4.625172</td>\n",
       "      <td>0.188089</td>\n",
       "      <td>4.211878</td>\n",
       "      <td>0.270091</td>\n",
       "      <td>0.034947</td>\n",
       "      <td>0.311310</td>\n",
       "      <td>0.357217</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-36-1580</th>\n",
       "      <td>1.403594</td>\n",
       "      <td>0.008641</td>\n",
       "      <td>0.133845</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.041762</td>\n",
       "      <td>0.074384</td>\n",
       "      <td>0.984290</td>\n",
       "      <td>2.329158</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>...</td>\n",
       "      <td>4.017149</td>\n",
       "      <td>0.289083</td>\n",
       "      <td>0.893372</td>\n",
       "      <td>0.314861</td>\n",
       "      <td>4.817979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707253</td>\n",
       "      <td>15.996040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-25-1321</th>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.162223</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101272</td>\n",
       "      <td>0.240456</td>\n",
       "      <td>1.740308</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>...</td>\n",
       "      <td>2.576079</td>\n",
       "      <td>0.166843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121147</td>\n",
       "      <td>1.356422</td>\n",
       "      <td>0.521892</td>\n",
       "      <td>0.157563</td>\n",
       "      <td>2.033770</td>\n",
       "      <td>1.495528</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-30-1892</th>\n",
       "      <td>0.206074</td>\n",
       "      <td>0.133216</td>\n",
       "      <td>0.119614</td>\n",
       "      <td>0.056072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055381</td>\n",
       "      <td>0.280458</td>\n",
       "      <td>0.592872</td>\n",
       "      <td>0.914734</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>...</td>\n",
       "      <td>1.058604</td>\n",
       "      <td>0.114269</td>\n",
       "      <td>2.825069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.716018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294278</td>\n",
       "      <td>1.890974</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-04-1347</th>\n",
       "      <td>0.319288</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.011701</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>0.663423</td>\n",
       "      <td>0.094262</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.038608</td>\n",
       "      <td>0.567030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071847</td>\n",
       "      <td>4.616723</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-24-0968</th>\n",
       "      <td>0.237050</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>0.185222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307004</td>\n",
       "      <td>0.213121</td>\n",
       "      <td>1.291373</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>...</td>\n",
       "      <td>15.221570</td>\n",
       "      <td>0.525782</td>\n",
       "      <td>4.332951</td>\n",
       "      <td>2.720169</td>\n",
       "      <td>31.845646</td>\n",
       "      <td>0.205584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710875</td>\n",
       "      <td>6.117782</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-30-1891</th>\n",
       "      <td>0.772034</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.110194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.200678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098616</td>\n",
       "      <td>2.959271</td>\n",
       "      <td>0.077290</td>\n",
       "      <td>...</td>\n",
       "      <td>47.331214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.631492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.568652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.395413</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-09-0366</th>\n",
       "      <td>0.195435</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.631170</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>0.303848</td>\n",
       "      <td>0.703636</td>\n",
       "      <td>5.424154</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225023</td>\n",
       "      <td>0.534375</td>\n",
       "      <td>0.100086</td>\n",
       "      <td>1.481522</td>\n",
       "      <td>3.712525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058985</td>\n",
       "      <td>2.477118</td>\n",
       "      <td>0.602935</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-13-0884</th>\n",
       "      <td>0.784894</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.035967</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.023577</td>\n",
       "      <td>0.643764</td>\n",
       "      <td>0.967456</td>\n",
       "      <td>0.005017</td>\n",
       "      <td>...</td>\n",
       "      <td>2.586317</td>\n",
       "      <td>0.074447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162171</td>\n",
       "      <td>7.989302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060262</td>\n",
       "      <td>0.191723</td>\n",
       "      <td>3.849928</td>\n",
       "      <td>0.000402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ID            ENSG00000069482  ENSG00000072657  ENSG00000078399  \\\n",
       "TCGA-61-1910         0.970585         0.003561         0.044476   \n",
       "TCGA-24-0982         0.395276         0.005010         0.090384   \n",
       "TCGA-36-1580         1.403594         0.008641         0.133845   \n",
       "TCGA-25-1321         0.308600         0.003129         0.004478   \n",
       "TCGA-30-1892         0.206074         0.133216         0.119614   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-04-1347         0.319288         0.000491         0.033696   \n",
       "TCGA-24-0968         0.237050         0.009708         0.185222   \n",
       "TCGA-30-1891         0.772034         0.000481         0.110194   \n",
       "TCGA-09-0366         0.195435         0.000085         1.631170   \n",
       "TCGA-13-0884         0.784894         0.000393         0.035967   \n",
       "\n",
       "ID            ENSG00000080572  ENSG00000100678  ENSG00000104435  \\\n",
       "TCGA-61-1910         0.000000         0.000000         0.000000   \n",
       "TCGA-24-0982         0.007062         0.000000         0.006438   \n",
       "TCGA-36-1580         0.001970         0.002559         0.041762   \n",
       "TCGA-25-1321         0.162223         0.000788         0.000000   \n",
       "TCGA-30-1892         0.056072         0.000000         0.055381   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-04-1347         0.005704         0.000988         0.011701   \n",
       "TCGA-24-0968         0.000000         0.000000         0.000000   \n",
       "TCGA-30-1891         0.000000         0.000646         0.200678   \n",
       "TCGA-09-0366         0.026487         0.001376         0.006339   \n",
       "TCGA-13-0884         0.012177         0.001055         0.004163   \n",
       "\n",
       "ID            ENSG00000104888  ENSG00000105146  ENSG00000109321  \\\n",
       "TCGA-61-1910         0.043732         2.308592         1.014494   \n",
       "TCGA-24-0982         0.082034         0.242661         0.691196   \n",
       "TCGA-36-1580         0.074384         0.984290         2.329158   \n",
       "TCGA-25-1321         0.101272         0.240456         1.740308   \n",
       "TCGA-30-1892         0.280458         0.592872         0.914734   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-04-1347         0.060742         0.663423         0.094262   \n",
       "TCGA-24-0968         0.307004         0.213121         1.291373   \n",
       "TCGA-30-1891         0.000000         0.098616         2.959271   \n",
       "TCGA-09-0366         0.303848         0.703636         5.424154   \n",
       "TCGA-13-0884         0.023577         0.643764         0.967456   \n",
       "\n",
       "ID            ENSG00000112499  ...  ENSG00000274576  ENSG00000275756  \\\n",
       "TCGA-61-1910         0.000000  ...         0.000000         0.920597   \n",
       "TCGA-24-0982         0.004156  ...        24.797152         0.259035   \n",
       "TCGA-36-1580         0.001391  ...         4.017149         0.289083   \n",
       "TCGA-25-1321         0.004819  ...         2.576079         0.166843   \n",
       "TCGA-30-1892         0.004400  ...         1.058604         0.114269   \n",
       "...                       ...  ...              ...              ...   \n",
       "TCGA-04-1347         0.000671  ...         0.000000         0.000000   \n",
       "TCGA-24-0968         0.003796  ...        15.221570         0.525782   \n",
       "TCGA-30-1891         0.077290  ...        47.331214         0.000000   \n",
       "TCGA-09-0366         0.008418  ...         0.225023         0.534375   \n",
       "TCGA-13-0884         0.005017  ...         2.586317         0.074447   \n",
       "\n",
       "ID            ENSG00000276775  ENSG00000277247  ENSG00000278196  \\\n",
       "TCGA-61-1910         0.000000         1.604301         0.299376   \n",
       "TCGA-24-0982         4.625172         0.188089         4.211878   \n",
       "TCGA-36-1580         0.893372         0.314861         4.817979   \n",
       "TCGA-25-1321         0.000000         0.121147         1.356422   \n",
       "TCGA-30-1892         2.825069         0.000000         3.716018   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-04-1347         0.000000         3.038608         0.567030   \n",
       "TCGA-24-0968         4.332951         2.720169        31.845646   \n",
       "TCGA-30-1891         2.631492         0.000000        65.568652   \n",
       "TCGA-09-0366         0.100086         1.481522         3.712525   \n",
       "TCGA-13-0884         0.000000         0.162171         7.989302   \n",
       "\n",
       "ID            ENSG00000278698  ENSG00000279834  ENSG00000279970  \\\n",
       "TCGA-61-1910         0.000000         0.894232         1.612154   \n",
       "TCGA-24-0982         0.270091         0.034947         0.311310   \n",
       "TCGA-36-1580         0.000000         0.000000         0.707253   \n",
       "TCGA-25-1321         0.521892         0.157563         2.033770   \n",
       "TCGA-30-1892         0.000000         0.000000         0.294278   \n",
       "...                       ...              ...              ...   \n",
       "TCGA-04-1347         0.000000         0.000000         0.071847   \n",
       "TCGA-24-0968         0.205584         0.000000         0.710875   \n",
       "TCGA-30-1891         0.000000         0.000000         0.000000   \n",
       "TCGA-09-0366         0.000000         0.058985         2.477118   \n",
       "TCGA-13-0884         0.000000         0.060262         0.191723   \n",
       "\n",
       "ID            ENSG00000280411  ENSG00000281880  \n",
       "TCGA-61-1910         0.761720         0.000000  \n",
       "TCGA-24-0982         0.357217         0.000000  \n",
       "TCGA-36-1580        15.996040         0.000000  \n",
       "TCGA-25-1321         1.495528         0.000000  \n",
       "TCGA-30-1892         1.890974         0.000000  \n",
       "...                       ...              ...  \n",
       "TCGA-04-1347         4.616723         0.000000  \n",
       "TCGA-24-0968         6.117782         0.000000  \n",
       "TCGA-30-1891        80.395413         0.000000  \n",
       "TCGA-09-0366         0.602935         0.001573  \n",
       "TCGA-13-0884         3.849928         0.000402  \n",
       "\n",
       "[214 rows x 395 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_X_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_y_pd = df2_mod.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCGA-61-1910    0.007666\n",
       "TCGA-24-0982    0.007190\n",
       "TCGA-36-1580    0.027081\n",
       "TCGA-25-1321    0.002315\n",
       "TCGA-30-1892    0.057091\n",
       "                  ...   \n",
       "TCGA-04-1347    0.000000\n",
       "TCGA-24-0968    0.013682\n",
       "TCGA-30-1891    0.034186\n",
       "TCGA-09-0366    0.032361\n",
       "TCGA-13-0884    0.018597\n",
       "Name: ENSG00000048545, Length: 214, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X = torch.from_numpy(trn_X_pd.astype(float).values)\n",
    "trn_y = torch.from_numpy(trn_y_pd.astype(float).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = data_utils.TensorDataset(trn_X, trn_y)\n",
    "trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = next(iter(trn_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dictionary batch\n",
    "class Dataset(data_utils.Dataset):\n",
    "   \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        return {'X': self.X[idx], 'y': self.y[idx]}\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = Dataset(trn_X, trn_y)\n",
    "trn_loader = data_utils.DataLoader(trn, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
